{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from tqdm import tqdm  # Import tqdm for making progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_articles = pq.read_table('ebnerd_demo/articles.parquet')\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>last_modified_time</th>\n",
       "      <th>premium</th>\n",
       "      <th>body</th>\n",
       "      <th>published_time</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>article_type</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>entity_groups</th>\n",
       "      <th>topics</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>category_str</th>\n",
       "      <th>total_inviews</th>\n",
       "      <th>total_pageviews</th>\n",
       "      <th>total_read_time</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3037230</td>\n",
       "      <td>Ishockey-spiller: Jeg troede jeg skulle dø</td>\n",
       "      <td>ISHOCKEY: Ishockey-spilleren Sebastian Harts h...</td>\n",
       "      <td>2023-06-29 06:20:57</td>\n",
       "      <td>False</td>\n",
       "      <td>Ambitionerne om at komme til USA og spille ish...</td>\n",
       "      <td>2003-08-28 08:55:00</td>\n",
       "      <td>None</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/sport/anden_sport/isho...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kriminalitet, Kendt, Sport, Katastrofe, Mindr...</td>\n",
       "      <td>142</td>\n",
       "      <td>[327, 334]</td>\n",
       "      <td>sport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3044020</td>\n",
       "      <td>Prins Harry tvunget til dna-test</td>\n",
       "      <td>Hoffet tvang Prins Harry til at tage dna-test ...</td>\n",
       "      <td>2023-06-29 06:21:16</td>\n",
       "      <td>False</td>\n",
       "      <td>Den britiske tabloidavis The Sun fortsætter me...</td>\n",
       "      <td>2005-06-29 08:47:00</td>\n",
       "      <td>[3097307, 3097197, 3104927]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/underholdning/udlandke...</td>\n",
       "      <td>...</td>\n",
       "      <td>[PER, PER]</td>\n",
       "      <td>[Kriminalitet, Kendt, Underholdning, Personfar...</td>\n",
       "      <td>414</td>\n",
       "      <td>[432]</td>\n",
       "      <td>underholdning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7084</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3057622</td>\n",
       "      <td>Rådden kørsel på blå plader</td>\n",
       "      <td>Kan ikke straffes: Udenlandske diplomater i Da...</td>\n",
       "      <td>2023-06-29 06:21:24</td>\n",
       "      <td>False</td>\n",
       "      <td>Slingrende spritkørsel. Grove overtrædelser af...</td>\n",
       "      <td>2005-10-10 07:20:00</td>\n",
       "      <td>[3047102]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/nyheder/samfund/articl...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kriminalitet, Transportmiddel, Bil]</td>\n",
       "      <td>118</td>\n",
       "      <td>[133]</td>\n",
       "      <td>nyheder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3073151</td>\n",
       "      <td>Mærsk-arvinger i livsfare</td>\n",
       "      <td>FANGET I FLODBØLGEN: Skibsrederens oldebørn må...</td>\n",
       "      <td>2023-06-29 06:21:38</td>\n",
       "      <td>False</td>\n",
       "      <td>To oldebørn af skibsreder Mærsk McKinney Mølle...</td>\n",
       "      <td>2005-01-04 06:59:00</td>\n",
       "      <td>[3067474, 3067478, 3153705]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/nyheder/samfund/articl...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Erhverv, Privat virksomhed, Livsstil, Familie...</td>\n",
       "      <td>118</td>\n",
       "      <td>[133]</td>\n",
       "      <td>nyheder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3193383</td>\n",
       "      <td>Skød svigersøn gennem babydyne</td>\n",
       "      <td>44-årig kvinde tiltalt for drab på ekssvigersø...</td>\n",
       "      <td>2023-06-29 06:22:57</td>\n",
       "      <td>False</td>\n",
       "      <td>En 44-årig mormor blev i dag fremstillet i et ...</td>\n",
       "      <td>2003-09-15 15:30:00</td>\n",
       "      <td>None</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/krimi/article3193383.ece</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kriminalitet, Personfarlig kriminalitet]</td>\n",
       "      <td>140</td>\n",
       "      <td>[]</td>\n",
       "      <td>krimi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                       title  \\\n",
       "0     3037230  Ishockey-spiller: Jeg troede jeg skulle dø   \n",
       "1     3044020            Prins Harry tvunget til dna-test   \n",
       "2     3057622                 Rådden kørsel på blå plader   \n",
       "3     3073151                   Mærsk-arvinger i livsfare   \n",
       "4     3193383              Skød svigersøn gennem babydyne   \n",
       "\n",
       "                                            subtitle  last_modified_time  \\\n",
       "0  ISHOCKEY: Ishockey-spilleren Sebastian Harts h... 2023-06-29 06:20:57   \n",
       "1  Hoffet tvang Prins Harry til at tage dna-test ... 2023-06-29 06:21:16   \n",
       "2  Kan ikke straffes: Udenlandske diplomater i Da... 2023-06-29 06:21:24   \n",
       "3  FANGET I FLODBØLGEN: Skibsrederens oldebørn må... 2023-06-29 06:21:38   \n",
       "4  44-årig kvinde tiltalt for drab på ekssvigersø... 2023-06-29 06:22:57   \n",
       "\n",
       "   premium                                               body  \\\n",
       "0    False  Ambitionerne om at komme til USA og spille ish...   \n",
       "1    False  Den britiske tabloidavis The Sun fortsætter me...   \n",
       "2    False  Slingrende spritkørsel. Grove overtrædelser af...   \n",
       "3    False  To oldebørn af skibsreder Mærsk McKinney Mølle...   \n",
       "4    False  En 44-årig mormor blev i dag fremstillet i et ...   \n",
       "\n",
       "       published_time                    image_ids     article_type  \\\n",
       "0 2003-08-28 08:55:00                         None  article_default   \n",
       "1 2005-06-29 08:47:00  [3097307, 3097197, 3104927]  article_default   \n",
       "2 2005-10-10 07:20:00                    [3047102]  article_default   \n",
       "3 2005-01-04 06:59:00  [3067474, 3067478, 3153705]  article_default   \n",
       "4 2003-09-15 15:30:00                         None  article_default   \n",
       "\n",
       "                                                 url  ... entity_groups  \\\n",
       "0  https://ekstrabladet.dk/sport/anden_sport/isho...  ...            []   \n",
       "1  https://ekstrabladet.dk/underholdning/udlandke...  ...    [PER, PER]   \n",
       "2  https://ekstrabladet.dk/nyheder/samfund/articl...  ...            []   \n",
       "3  https://ekstrabladet.dk/nyheder/samfund/articl...  ...            []   \n",
       "4   https://ekstrabladet.dk/krimi/article3193383.ece  ...            []   \n",
       "\n",
       "                                              topics category  subcategory  \\\n",
       "0  [Kriminalitet, Kendt, Sport, Katastrofe, Mindr...      142   [327, 334]   \n",
       "1  [Kriminalitet, Kendt, Underholdning, Personfar...      414        [432]   \n",
       "2               [Kriminalitet, Transportmiddel, Bil]      118        [133]   \n",
       "3  [Erhverv, Privat virksomhed, Livsstil, Familie...      118        [133]   \n",
       "4          [Kriminalitet, Personfarlig kriminalitet]      140           []   \n",
       "\n",
       "    category_str total_inviews  total_pageviews  total_read_time  \\\n",
       "0          sport           NaN              NaN              NaN   \n",
       "1  underholdning           NaN              NaN              NaN   \n",
       "2        nyheder           NaN              NaN              NaN   \n",
       "3        nyheder           NaN              NaN              NaN   \n",
       "4          krimi           NaN              NaN              NaN   \n",
       "\n",
       "   sentiment_score  sentiment_label  \n",
       "0           0.9752         Negative  \n",
       "1           0.7084         Negative  \n",
       "2           0.9236         Negative  \n",
       "3           0.9945         Negative  \n",
       "4           0.9966         Negative  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_articles.size)\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Encoder - 1st Sublayer - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaWordEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the tokenizer and model from the specified pretrained XLM-RoBERTa model.\n",
    "        \"\"\"\n",
    "        super(XLMRobertaWordEmbedder, self).__init__()\n",
    "\n",
    "        # Initialize the tokenizer\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        # Set the model to evaluation mode to deactivate dropout layers\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, titles):\n",
    "        \"\"\"\n",
    "        Generates word embeddings for the provided input list of titles.\n",
    "\n",
    "        Args:\n",
    "            titles (List[str]): A list of input titles.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor containing word embeddings with shape (batch_size, seq_length, hidden_size).\n",
    "        \"\"\"\n",
    "        # Tokenize the input titles\n",
    "        encoded_input = self.tokenizer(\n",
    "            titles,                      # List of titles to encode\n",
    "            padding='max_length',        # Pad all sequences to the max_length\n",
    "            truncation=True,             # Truncate sentences longer than max_length\n",
    "            max_length=30,               # Define a fixed max_length\n",
    "            return_tensors='pt',         # Return PyTorch tensors\n",
    "            return_attention_mask=True,  # Return attention masks\n",
    "            return_token_type_ids=False  # XLM-RoBERTa doesn't use token type IDs\n",
    "        )\n",
    "\n",
    "        # Move tensors to the same device as the model\n",
    "        device = next(self.model.parameters()).device\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            outputs = self.model(**encoded_input)\n",
    "\n",
    "        # Extract the last hidden states (token embeddings)\n",
    "        token_embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n",
    "        attention_mask = encoded_input['attention_mask']  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        return token_embeddings, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the word embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings Shape: torch.Size([3, 30, 768])\n",
      "Attention Mask Shape: torch.Size([3, 30])\n",
      "First title's first token embedding: tensor([0.1011, 0.1113, 0.1128, 0.0316, 0.0820])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "embedder = XLMRobertaWordEmbedder()\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedder.to(device)\n",
    "\n",
    "# Example input: three titles\n",
    "titles = [\n",
    "    \"Learning to code is a valuable skill\",\n",
    "    \"Artificial intelligence is transforming industries\",\n",
    "    \"Exploring the universe is humanity's greatest adventure\"\n",
    "]\n",
    "\n",
    "# Generate word embeddings\n",
    "token_embeddings, attention_mask = embedder(titles)\n",
    "\n",
    "# Output shapes and data\n",
    "print(\"Token Embeddings Shape:\", token_embeddings.shape)  # (batch_size, seq_length, hidden_size)\n",
    "print(\"Attention Mask Shape:\", attention_mask.shape)  # (batch_size, seq_length)\n",
    "\n",
    "# Print a small part of the embeddings for inspection\n",
    "print(\"First title's first token embedding:\", token_embeddings[0, 0, :5])  # First 5 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Encoder - 2nd Sublayer - The Word-Level Multi-Head Self-Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLevelMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the 2nd layer with the Word-Level Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden embeddings (e.g., 768 for xlm-roberta-base).\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dropout (float): Dropout probability for attention weights.\n",
    "        \"\"\"\n",
    "        super(WordLevelMultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures input/output tensors are (batch, seq, feature)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head self-attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, hidden_size).\n",
    "            attention_mask (torch.Tensor, optional): Attention mask of shape (batch_size, seq_length),\n",
    "                                                     where elements with value `True` are masked.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after self-attention and residual connection,\n",
    "                          shape (batch_size, seq_length, hidden_size).\n",
    "            torch.Tensor: Attention weights of shape (batch_size, num_heads, seq_length, seq_length).\n",
    "        \"\"\"\n",
    "        # Apply multi-head self-attention\n",
    "        # Note: nn.MultiheadAttention expects inputs of shape (batch, seq, feature) with batch_first=True\n",
    "        attn_output, attn_weights = self.multihead_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=attention_mask  # Masks padded tokens if provided\n",
    "        )\n",
    "\n",
    "        # Add residual connections\n",
    "        x = x + attn_output\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Encoder - 3nd Sublayer - The Additive Word Attention Net-Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveWordAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dim):\n",
    "        \"\"\"\n",
    "        Initializes 3rd layer with the Additive Word Attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden embeddings (e.g., 768 for xlm-roberta-base).\n",
    "            attention_dim (int): The dimensionality of the attention space.\n",
    "        \"\"\"\n",
    "        super(AdditiveWordAttention, self).__init__()\n",
    "\n",
    "        # Projection layer Vw: projects hidden_size to attention_dim\n",
    "        self.Vw = nn.Linear(hidden_size, attention_dim, bias=True)\n",
    "\n",
    "        # Query vector qw: projects attention_dim to a scalar score\n",
    "        self.qw = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, h, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the additive word attention layer.\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Input tensor of shape (batch_size, seq_length, hidden_size).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, seq_length),\n",
    "                                          where elements with value `True` indicate valid tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: News representations with shape (batch_size, hidden_size).\n",
    "            torch.Tensor: Attention weights with shape (batch_size, seq_length).\n",
    "        \"\"\"\n",
    "        # Apply linear projection and activation\n",
    "        u = self.tanh(self.Vw(h))  # Shape: (batch_size, seq_length, attention_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        a = self.qw(u).squeeze(-1)  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        # Apply mask: set scores of padded tokens to -inf\n",
    "        if mask is not None:\n",
    "            a = a.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        alpha = F.softmax(a, dim=1)  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        # Compute the weighted sum of word embeddings\n",
    "        r = torch.sum(h * alpha.unsqueeze(-1), dim=1)  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        return r, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Encoder - Combined Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoderModel(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_heads=12, attention_dim=128, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the News Encoder Model with word embeddings, multi-head self-attention, and additive word attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden embeddings (e.g., 768 for xlm-roberta-base).\n",
    "            num_heads (int): The number of attention heads.\n",
    "            attention_dim (int): The dimensionality of the attention space for additive attention.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(NewsEncoderModel, self).__init__()\n",
    "\n",
    "        # Initialize the first layer: XLM-RoBERTa Word Embedder\n",
    "        self.word_embedder = XLMRobertaWordEmbedder()\n",
    "\n",
    "        # Initialize the second layer: Word-Level Multi-Head Self-Attention\n",
    "        self.self_attention = WordLevelMultiHeadSelfAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Initialize the third layer: Additive Word Attention\n",
    "        self.additive_attention = AdditiveWordAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            attention_dim=attention_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, titles):\n",
    "        \"\"\"\n",
    "        Generates enhanced word embeddings and final news representations using XLM-RoBERTa, multi-head self-attention,\n",
    "        and additive word attention.\n",
    "\n",
    "        Args:\n",
    "            titles (List[str]): A list of input news titles.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Enhanced embeddings from self-attention with shape (batch_size, seq_length, hidden_size).\n",
    "            torch.Tensor: Self-attention weights with shape (batch_size, num_heads, seq_length, seq_length).\n",
    "            torch.Tensor: Final news representations with shape (batch_size, hidden_size).\n",
    "            torch.Tensor: Additive attention weights with shape (batch_size, seq_length).\n",
    "        \"\"\"\n",
    "        # Obtain word embeddings and attention masks from the first layer\n",
    "        token_embeddings, attention_mask = self.word_embedder(titles)  # (batch_size, seq_length, hidden_size), (batch_size, seq_length)\n",
    "\n",
    "        # Prepare the attention mask for the self-attention layer\n",
    "        # nn.MultiheadAttention expects 'key_padding_mask' where True indicates padding tokens\n",
    "        # The 'attention_mask' from the tokenizer has 1 for valid tokens and 0 for padding\n",
    "        # Therefore, we invert it to get True for padding\n",
    "        key_padding_mask = ~attention_mask.bool()  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        # Apply the multi-head self-attention layer\n",
    "        enhanced_embeddings, self_attn_weights = self.self_attention(token_embeddings, attention_mask=key_padding_mask)\n",
    "        # enhanced_embeddings: (batch_size, seq_length, hidden_size)\n",
    "        # self_attn_weights: (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        # Apply the additive word attention layer\n",
    "        # Prepare mask where True indicates valid tokens for additive attention\n",
    "        additive_mask = attention_mask.bool()  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        final_representations, additive_attn_weights = self.additive_attention(enhanced_embeddings, mask=additive_mask)\n",
    "        # final_representations: (batch_size, hidden_size)\n",
    "        # additive_attn_weights: (batch_size, seq_length)\n",
    "\n",
    "        return enhanced_embeddings, self_attn_weights, final_representations, additive_attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the news encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Embedding Shape: torch.Size([4, 30, 768])\n",
      "Self-Attention Weights Shape: torch.Size([4, 30, 30])\n",
      "Final Representations Shape: torch.Size([4, 768])\n",
      "Additive Attention Weights Shape: torch.Size([4, 30])\n",
      "\n",
      "Final Representations:\n",
      "tensor([[ 0.2187,  0.1671, -0.1930,  ..., -0.0246, -0.0005, -0.2953],\n",
      "        [ 0.1388,  0.2266, -0.1874,  ..., -0.1695, -0.0960, -0.3198],\n",
      "        [ 0.2769,  0.1504, -0.1835,  ..., -0.1307, -0.0296, -0.3529],\n",
      "        [ 0.1794,  0.1878, -0.1491,  ..., -0.0731, -0.0544, -0.3193]])\n",
      "\n",
      "Additive Attention Weights:\n",
      "tensor([[0.1124, 0.0930, 0.0936, 0.0897, 0.1002, 0.1001, 0.1054, 0.0973, 0.0974,\n",
      "         0.1110, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.2159, 0.1928, 0.1836, 0.1949, 0.2128, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.1263, 0.1037, 0.1050, 0.1161, 0.1008, 0.1015, 0.1066, 0.1165, 0.1234,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.1031, 0.0906, 0.0923, 0.0832, 0.0921, 0.0845, 0.0831, 0.0933, 0.0863,\n",
      "         0.0904, 0.1010, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "encoder = NewsEncoderModel()\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Example titles\n",
    "titles = [\n",
    "    \"Rockets Ends 2018 with a Win\",\n",
    "    \"Another News Title\",\n",
    "    \"Breaking News: NFL Championship Highlights\",\n",
    "    \"Today in Technology: New Innovations Released\"\n",
    "]\n",
    "\n",
    "# Generate embeddings and representations\n",
    "with torch.no_grad():\n",
    "    enhanced_embeddings, self_attn_weights, final_representations, additive_attn_weights = encoder(titles)\n",
    "\n",
    "# Print output shapes\n",
    "print(\"Enhanced Embedding Shape:\", enhanced_embeddings.shape)  # Expected: (batch_size, seq_length, hidden_size)\n",
    "print(\"Self-Attention Weights Shape:\", self_attn_weights.shape)  # Expected: (batch_size, num_heads, seq_length, seq_length)\n",
    "print(\"Final Representations Shape:\", final_representations.shape)  # Expected: (batch_size, hidden_size)\n",
    "print(\"Additive Attention Weights Shape:\", additive_attn_weights.shape)  # Expected: (batch_size, seq_length)\n",
    "\n",
    "# Optionally, print the final representations and attention weights\n",
    "print(\"\\nFinal Representations:\")\n",
    "print(final_representations)\n",
    "\n",
    "print(\"\\nAdditive Attention Weights:\")\n",
    "print(additive_attn_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
