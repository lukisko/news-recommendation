{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!run\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#import transformers\n",
    "#import tokenizers\n",
    "#import datasets\n",
    "#import zipfile\n",
    "#from huggingface_hub import hf_hub_download\n",
    "device = 'cpu'\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "dim_emb = 300 # this is just starting for embedding dimensionality of a word so that training and learning is quick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_behavior = pq.read_table('ebnerd_small/train/behaviors.parquet')\n",
    "table_history = pq.read_table('ebnerd_small/train/history.parquet')\n",
    "table_articles = pq.read_table('ebnerd_small/articles.parquet')\n",
    "df_behavior = table_behavior.to_pandas()\n",
    "df_history = table_history.to_pandas()\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = df_behavior[['article_ids_inview','article_ids_clicked','user_id']]\n",
    "joined_table = main_table.join(df_history[['user_id', 'article_id_fixed']].set_index('user_id'), on='user_id', validate='many_to_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify the data so that we get the format we need for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "def remove_clicked(row):\n",
    "    index_of_clicked_one = np.where(row['article_ids_inview'] == row['article_ids_clicked'][0])\n",
    "    indexes_of_not_clicked = np.delete(row['article_ids_inview'], index_of_clicked_one)\n",
    "    indexes_of_not_clicked_suffled = np.random.choice(indexes_of_not_clicked, size=(K), replace=False) # now we have list of K = 4 things\n",
    "    indexes_of_all = np.concatenate((indexes_of_not_clicked_suffled, [row['article_ids_clicked'][0]]), axis=0) # merge random no selected ones and the selected one\n",
    "    np.random.shuffle(indexes_of_all) # suffle them\n",
    "    correct_index = np.where(indexes_of_all == row['article_ids_clicked'][0]) # get the index - used as label\n",
    "    return [indexes_of_all, correct_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_table[['articles_input_ids', 'articles_correct_idx']] = joined_table.apply(remove_clicked, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change ids for title names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_map = df_articles.set_index('article_id') # this make a significant speedup in the following method\n",
    "\n",
    "def from_ids_arr_to_article_title_arr(ids_arr):\n",
    "    return article_map.loc[ids_arr]['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_shown = joined_table[['articles_input_ids']][:].map(from_ids_arr_to_article_title_arr) # shown articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_clicked = joined_table['articles_correct_idx'] # index of selected article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_history = joined_table[['article_id_fixed']][:].map(from_ids_arr_to_article_title_arr) # history of articles shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = article_history['article_id_fixed'].apply(len).max()\n",
    "def pad_list(row):\n",
    "    padded_row = np.append(row, [''] * (max_len - len(row)))\n",
    "    return np.array(padded_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_history['article_id_fixed_padded'] = article_history['article_id_fixed'].apply(pad_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_history.npy', 'wb') as f:\n",
    "    np.save(f,article_history['article_id_fixed_padded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles_shown.npy', 'wb') as f:\n",
    "    np.save(f,articles_shown['articles_input_ids'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles_clicked.npy', 'wb') as f:\n",
    "    np.save(f, articles_clicked.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run - just if you have npy already, if not, you need to run all code up to this point\n",
    "user_history_npy = np.load('user_history.npy', allow_pickle=True)\n",
    "articles_shown_npy = np.load('articles_shown.npy', allow_pickle=True)\n",
    "articles_clicked_npy = np.load('articles_clicked.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make dataset out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "history_limit = 20\n",
    "class BrowsedCandidateClickedDataset(Dataset):\n",
    "    def __init__(self, browsed, candidate, clicked):\n",
    "        self.browsed = browsed\n",
    "        self.candidate = candidate\n",
    "        self.clicked = clicked\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.browsed)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.browsed[index][-history_limit:], self.candidate[index], self.clicked[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "full_dataset = BrowsedCandidateClickedDataset(user_history_npy, articles_shown_npy, articles_clicked_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_dataset = BrowsedCandidateClickedDataset(article_history['article_id_fixed_padded'].values, articles_shown['articles_input_ids'].values, articles_clicked.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.__getitem__(5)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "def custom_collate_fn(batch): \n",
    "    browsed, candidate, clicked = zip(*batch)\n",
    "    return list(browsed), list(candidate), list(clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "train_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old data loading - need to run just if  running dummy embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): # turn numpy arrays to a torch dataset\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_train[index], self.y_train[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take input so we can have some dummy embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_articles['title'].values\n",
    "# get all the words into list\n",
    "split_words = lambda tit : tit.split(' ')\n",
    "all_words = list(map(split_words, titles))\n",
    "words = []\n",
    "for title in all_words:\n",
    "    for word in title:\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding we need to know just unique ones\n",
    "all_words = np.unique(np.sort(np.array(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27693,)\n"
     ]
    }
   ],
   "source": [
    "# for fast access we want to have dictionary of the words - the map gives us unique index for each word - needed for torch embedding\n",
    "print(all_words.shape)\n",
    "dummy_dictionary_embedding = {}\n",
    "for i in range(len(all_words)):\n",
    "    dummy_dictionary_embedding[all_words[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare x and y\n",
    "X_all = df_articles['title'].values\n",
    "Y_all = df_articles['category_str'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 27693 + 1 # adding one symbol so that we have a special marking for empty word\n",
    "MAX_WORDS = 30 # maximum number that is in the title\n",
    "\n",
    "class MyEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim, dummy_dictionary_embedding):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_torch = nn.Embedding(word_count, emb_dim)\n",
    "        self.dummy_emb = dummy_dictionary_embedding\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten() #flatten things so that we look just at the titles\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split(\" \")\n",
    "            for word in words:\n",
    "                output.append(torch.IntTensor([self.dummy_emb[word.lower()]]))\n",
    "                \n",
    "            # all titles need to have the same number of \"words\" so I just add \"empty\" words at the end\n",
    "            for i in range(MAX_WORDS - len(words)): \n",
    "                output.append(torch.IntTensor([word_count-1]))\n",
    "        \n",
    "        output = torch.stack(output).to(device)\n",
    "        #print(output.device)\n",
    "        output = self.emb_torch(output)\n",
    "        \n",
    "        # invert the action of flataning\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim)) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the new embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, AutoTokenizer, AutoModel\n",
    "\n",
    "class XLMRobertaWordEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the tokenizer and model from the specified pretrained XLM-RoBERTa model.\n",
    "        \"\"\"\n",
    "        super(XLMRobertaWordEmbedder, self).__init__()\n",
    "\n",
    "        # Initialize the tokenizer\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained(\"NbAiLab/nb-bert-base\")\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        #self.model = AutoModel.from_pretrained(\"NbAiLab/nb-bert-base\")\n",
    "        # Set the model to evaluation mode to deactivate dropout layers\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, titles):\n",
    "        \"\"\"\n",
    "        Generates word embeddings for the provided input list of titles.\n",
    "\n",
    "        Args:\n",
    "            titles (List[str]): A list of input titles.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor containing word embeddings with shape (batch_size, seq_length, hidden_size).\n",
    "        \"\"\"\n",
    "        # Tokenize the input titles\n",
    "        print('titles', titles)\n",
    "        print('titles type', type(titles))\n",
    "        encoded_input = self.tokenizer(\n",
    "            titles,                      # List of titles to encode\n",
    "            padding='max_length',        # Pad all sequences to the max_length\n",
    "            truncation=True,             # Truncate sentences longer than max_length\n",
    "            max_length=30,               # Define a fixed max_length\n",
    "            return_tensors='pt',         # Return PyTorch tensors\n",
    "            return_attention_mask=True,  # Return attention masks\n",
    "            return_token_type_ids=False  # XLM-RoBERTa doesn't use token type IDs\n",
    "        )\n",
    "\n",
    "        # Move tensors to the same device as the model\n",
    "        device = next(self.model.parameters()).device\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            outputs = self.model(**encoded_input)\n",
    "\n",
    "        # Extract the last hidden states (token embeddings)\n",
    "        token_embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n",
    "        attention_mask = encoded_input['attention_mask']  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        return token_embeddings, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the light embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run \n",
    "import fasttext\n",
    "\n",
    "# DO NOT LOAD THE MODEL MORE THEN ONCE, IT TAKES A LOT OF RAM\n",
    "fasttext_model = fasttext.load_model('cc.da.300.bin')\n",
    "\n",
    "MAX_WORDS = 30\n",
    "class FastTextEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        #self.emb_torch = nn.Embedding(word_count, emb_dim)\n",
    "        self.embedding_fasttext = fasttext_model # this is to avoid loading the model every time we create a model, as it is very RAM hungry\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten() #flatten things so that we look just at the titles\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split()\n",
    "            for word in words:\n",
    "                output.append(torch.from_numpy(self.embedding_fasttext.get_word_vector(word)))\n",
    "                \n",
    "            # all titles need to have the same number of \"words\" so I just add \"empty\" words at the end\n",
    "            for i in range(MAX_WORDS - len(words)): \n",
    "                output.append(torch.zeros(300)) # this is vector for string with space\n",
    "        \n",
    "        output = torch.stack(output).to(device)\n",
    "        #print(output.device)\n",
    "        #output = self.emb_torch(output)\n",
    "        \n",
    "        # invert the action of flataning\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim)) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make self attention head - single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_out):\n",
    "        super().__init__()\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=-1) # TODO do I go for the correct dimension?\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=head_out, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        qe = self.lin_qk(x) # = Q_k^w e_j\n",
    "        et_qt = x @ qe.transpose(-2,-1) # = e_i^T Q_k^w e_j\n",
    "        ak = self.softmax_dim1(et_qt) # = exp(...)/ SUM exp(...)\n",
    "        # ak @ x = SUM a_i,j^k e_j \n",
    "        hk = self.lin_vk(ak @ x) # =  V_k^w (...)\n",
    "        return hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make multiple heads and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self,embedding_dimension, head_count=16, head_vector_size=16):\n",
    "        super().__init__()\n",
    "        self.head_out = head_vector_size #embedding_dimension // head_count # TODO this will be later more specific\n",
    "        self.selfAtt = nn.ModuleList([SelfAttHead(embedding_dimension, self.head_out) for _ in range(head_count)])\n",
    "        \n",
    "    def forward(self, e_s):\n",
    "        hk = []\n",
    "        for head in self.selfAtt:\n",
    "            att = head(e_s)\n",
    "            hk.append(att)\n",
    "        h = torch.cat(hk, -1) # simply concatinaiton as mentioned in paper\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch multiheadSelfAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run - ACTUALLY ALL THE THINGS FROM HERE AND BELLOW\n",
    "class PytorchMultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the 2nd layer with the Word-Level Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden embeddings (e.g., 768 for xlm-roberta-base).\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dropout (float): Dropout probability for attention weights.\n",
    "        \"\"\"\n",
    "        super(PytorchMultiHeadSelfAttHead, self).__init__()\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures input/output tensors are (batch, seq, feature)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head self-attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, hidden_size).\n",
    "            attention_mask (torch.Tensor, optional): Attention mask of shape (batch_size, seq_length),\n",
    "                                                     where elements with value `True` are masked.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after self-attention and residual connection,\n",
    "                          shape (batch_size, seq_length, hidden_size).\n",
    "            torch.Tensor: Attention weights of shape (batch_size, num_heads, seq_length, seq_length).\n",
    "        \"\"\"\n",
    "        # Apply multi-head self-attention\n",
    "        # Note: nn.MultiheadAttention expects inputs of shape (batch, seq, feature) with batch_first=True\n",
    "        #print(x.shape)\n",
    "        input_shape = x.shape\n",
    "        \n",
    "        merged_batch_and_titles = x.reshape((-1,) + (input_shape[-2], input_shape[-1]))\n",
    "        \n",
    "        attn_output, attn_weights = self.multihead_attn(\n",
    "            query=merged_batch_and_titles,\n",
    "            key=merged_batch_and_titles,\n",
    "            value=merged_batch_and_titles,\n",
    "            key_padding_mask=attention_mask  # Masks padded tokens if provided\n",
    "        )\n",
    "        \n",
    "        #print('att out',attn_output.shape)\n",
    "        x = attn_output.reshape(input_shape)\n",
    "\n",
    "        # Apparently not used in the paper.\n",
    "        # TODO:\n",
    "        #   This be an idea to improve the model, maybe bring back with it the normalization.\n",
    "        # Add residual connections\n",
    "        # x = x + attn_output\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### additive self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveWordAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, additive_vector_dim=200):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.lin_vw = nn.Linear(in_features=embedding_dimension, out_features=additive_vector_dim)\n",
    "        self.lin_q = nn.Linear(in_features=additive_vector_dim, out_features=1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        # lin_vw(h) = V_w × h_i^w + v_w\n",
    "        # lin_q(act_fn(...)) = q_w^T tanh(...)\n",
    "        tmp = self.activation_fn(self.lin_vw(h))\n",
    "        aw = self.lin_q(tmp)\n",
    "        aw = self.softmax(aw) # exp(...) / SUM exp(...)\n",
    "        r = aw.transpose(-2,-1) @ h # SUM a_i^w h_i^w\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_count=10, head_vector_size=30, embedding_dropout=0.0):\n",
    "        super().__init__()\n",
    "        #assert embedding_dimension % head_count == 0, \"embeding must be divisible by heads\"\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb, dummy_dictionary_embedding)\n",
    "        #self.embedding = XLMRobertaWordEmbedder()\n",
    "        #self.embedding = FastTextEmbeddingLayer(embedding_dimension)\n",
    "        self.embedding_drop = nn.Dropout(embedding_dropout)\n",
    "        #self.mult_head_att = MultiHeadSelfAttHead(embedding_dimension, head_count, head_vector_size)\n",
    "        #print(embedding_dimension, head_count)\n",
    "        self.mult_head_att = PytorchMultiHeadSelfAttHead(embedding_dimension, head_count)\n",
    "        #print('in word add', head_count, head_vector_size)\n",
    "        self.add_word_att = AdditiveWordAttention(head_count * head_vector_size)# 16 heads and 16 dimensions each # TODO later change the vector dim to 200\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        \n",
    "        #input_shape = x.shape\n",
    "        #print('0_0', input_shape)\n",
    "        #flatten_titles = x.flatten()\n",
    "        #print('0_1', flatten_titles.shape)\n",
    "        #titles_list = flatten_titles.tolist()\n",
    "        #token_embeddings, attention_mask = self.embedding(titles_list)\n",
    "        #e_s = token_embeddings.reshape(input_shape + (30, self.embedding_dimension))\n",
    "        e_s = self.embedding(x)\n",
    "        e_s = self.embedding_drop(e_s)\n",
    "        #print('1',e_s.shape)\n",
    "        \n",
    "        h, ignore = self.mult_head_att(e_s)\n",
    "        #print('1_1',h.shape)\n",
    "        \n",
    "        r = self.add_word_att(h)\n",
    "        #print('1_2',r.shape)\n",
    "        return r.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, emb_dimension, user_head_count=10, news_head_count=10, head_vector_size=30):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.news_encoder = MyNewsEncoder(emb_dimension, news_head_count, head_vector_size)\n",
    "        #self.multi_head_att = MultiHeadSelfAttHead(news_head_count*head_vector_size, user_head_count)\n",
    "        self.multi_head_att = PytorchMultiHeadSelfAttHead(news_head_count*head_vector_size, user_head_count)\n",
    "        self.add_news_att = AdditiveWordAttention(user_head_count*head_vector_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        r = self.news_encoder(x)\n",
    "        #print('2',r.shape)\n",
    "        \n",
    "        h, ignore = self.multi_head_att(r)\n",
    "        #print('2_1',h.shape)\n",
    "        \n",
    "        u = self.add_news_att(h)\n",
    "        #print('2_2',u.shape)\n",
    "        \n",
    "        return u.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### click predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickPredictor(nn.Module):\n",
    "    #def __init__(self, emb_dimension, user_head_count=16, news_head_count=16, head_vector_size=16):\n",
    "    def __init__(self, emb_dimension, user_head_count=10, news_head_count=10, head_vector_size=30):\n",
    "        super().__init__()\n",
    "        self.userEncoder = UserEncoder(emb_dimension, user_head_count, news_head_count, head_vector_size)\n",
    "        self.news_encoder = MyNewsEncoder(emb_dimension, news_head_count, head_vector_size)\n",
    "        \n",
    "    def forward(self, browsed_news, candidate_news):\n",
    "        \n",
    "        u = self.userEncoder(browsed_news)\n",
    "        u = u.unsqueeze(-2)\n",
    "        \n",
    "        r = self.news_encoder(candidate_news)\n",
    "        \n",
    "        ŷ = u @ r.transpose(-2, -1) # = u^T r^c\n",
    "        #ŷ = torch.tensor([torch.dot(u[i], r[i]) for i in range(u.shape[0])])\n",
    "        \n",
    "        return ŷ.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk_input = np.array([[\"Natascha var ikke den første\", \"Kun Star Wars tjente mere\", \"Luderne flytter på landet\"],['Cybersex: Hvornår er man utro?','Kniven for struben-vært får selv kniven','Willy Strube har begået selvmord']]) # fist samples to be used\n",
    "model_news = MyNewsEncoder(dim_emb)\n",
    "model_users = UserEncoder(dim_emb)\n",
    "model_click = ClickPredictor(dim_emb)\n",
    "loss_fn = nn.L1Loss()\n",
    "#optimizer = optim.Adam(model_news.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 300])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_news(dk_input)\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_users(dk_input)\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_click(dk_input, np.array([\n",
    "    ['Cybersex: Hvornår er man utro?', 'Natascha var ikke den første', 'Færdig: Bestyrelsesformand i Naviair stopper','Sælger færre billetter end sidste år: Nu bliver pladsen mindre og boderne færre', '14 døde i Italien'],\n",
    "    ['Vidner melder om fortsatte kampe i Sudan trods våbenhvile', 'Nye Schumacher-anklager: Har et problem med navnet', 'Dele af metrolinje M1 er nede', 'Strømafbrydelse: 1454 kunder ramt', 'Flere ministeriers hjemmesider ramt af nedbrud']]))\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_history_npy[:5].shape\n",
    "sample = next(iter(train_loader))\n",
    "test_click_input = sample[0]\n",
    "test_articles_shown = sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raise Exception('be carefull, this usually kills my VSCode')\n",
    "results = model_click(np.array(test_click_input), np.array(test_articles_shown))\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClickPredictor(dim_emb)\n",
    "model.to(device)\n",
    "full_dataset\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('trained_model.torch', weights_only=True, map_location=torch.device(device)))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20      training AUC: 0.5326328422057458, loss: 1.8872582912445068\n",
      "Step 40      training AUC: 0.5506139359762204, loss: 1.6240406036376953\n",
      "Step 60      training AUC: 0.5468263905697002, loss: 1.612134337425232\n",
      "Step 80      training AUC: 0.5761834386626223, loss: 1.593735694885254\n",
      "Step 100     training AUC: 0.5830150463489949, loss: 1.5885372161865234\n",
      "Step 120     training AUC: 0.570660931743976, loss: 1.5927468538284302\n",
      "Step 140     training AUC: 0.5748956602477996, loss: 1.5850683450698853\n",
      "Step 160     training AUC: 0.5827258336962473, loss: 1.5869311094284058\n",
      "Step 180     training AUC: 0.5588083897273326, loss: 1.5960593223571777\n",
      "Step 200     training AUC: 0.5760992422466834, loss: 1.5777111053466797\n",
      "Step 220     training AUC: 0.5768651505832784, loss: 1.5911909341812134\n",
      "Step 240     training AUC: 0.6002359439767624, loss: 1.5717297792434692\n",
      "Step 260     training AUC: 0.5737004324097296, loss: 1.5881212949752808\n",
      "Step 280     training AUC: 0.6006249870056093, loss: 1.5577714443206787\n",
      "Step 300     training AUC: 0.609029541141755, loss: 1.5608561038970947\n",
      "Step 320     training AUC: 0.612438095428965, loss: 1.5546810626983643\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "If you want to run more then 200 steps remove this check",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;66;03m#print(f\"             validation accuracy: {validation_accuracies[-1]}\")\u001b[39;00m\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m300\u001b[39m: \n\u001b[0;32m--> 104\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you want to run more then 200 steps remove this check\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: If you want to run more then 200 steps remove this check"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "validation_every_steps = 20\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "train_loss = []\n",
    "validation_accuracies = []\n",
    "validation_loss = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    train_loss_batches = []\n",
    "    \n",
    "    for browsed, candidate, clicked in train_loader:#[(tmp_dk_input, target)]:#train_loader:#[(dk_input, target)]:#train_loader:\n",
    "        \n",
    "        # Forward pass.\n",
    "        \n",
    "        output = model(np.array(browsed), np.array(candidate))\n",
    "        \n",
    "        \n",
    "        # Compute loss.\n",
    "        targ_ind = torch.tensor(clicked).to(device)\n",
    "        loss = loss_fn(output, targ_ind)\n",
    "        train_loss_batches.append(loss.cpu().data.numpy())\n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions =  torch.argmax(output, dim=-1)\n",
    "        y_true = targ_ind.cpu().data.numpy()\n",
    "        y_pred = F.softmax(output, dim=-1).cpu().data.numpy()\n",
    "        batch_classes = np.unique(y_true)\n",
    "        y_true_binarized = label_binarize(y_true, classes=batch_classes)\n",
    "        calculated_acc = roc_auc_score(y_true_binarized, y_pred[:, batch_classes], multi_class='ovr')\n",
    "        #print('AUC',calculated_acc)\n",
    "        train_accuracies_batches.append(calculated_acc)\n",
    "        \n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            train_loss.append(np.mean(train_loss_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "            train_loss_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            # validation_accuracies_batches = []\n",
    "            # with torch.no_grad():\n",
    "            #     model.eval()\n",
    "            #     for inputs, targets in validation_loader:\n",
    "            #         output = model(inputs)\n",
    "            #         loss = loss_fn(output, targets.float())\n",
    "\n",
    "            #         predictions = output.max(1)[1]\n",
    "            #         targ_ind = targets.max(1)[1]\n",
    "                    \n",
    "            #         # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "            #         validation_accuracies_batches.append(accuracy_score(targ_ind, predictions) * len(inputs))\n",
    "\n",
    "            #     model.train()\n",
    "                \n",
    "            # # Append average validation accuracy to list.\n",
    "            # validation_accuracies.append(np.sum(validation_accuracies_batches) / len(validation_dataset))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training AUC: {train_accuracies[-1]}, loss: {train_loss[-1]}\")\n",
    "            #print(f\"             validation accuracy: {validation_accuracies[-1]}\")\n",
    "            if step > 300: \n",
    "                raise Exception('If you want to run more then 300 steps remove this check')\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished training.\")\n",
    "\n",
    "folder = 'dummyX/'\n",
    "\n",
    "torch.save(model.state_dict(), folder + 'trained_model')\n",
    "\n",
    "with open(folder + 'train_acc.npy', 'wb') as f:\n",
    "    np.save(f,np.array(train_accuracies))\n",
    "\n",
    "with open(folder + 'train_loss.npy', 'wb') as f:\n",
    "    np.save(f, np.array(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for validation you can check file main_dummy-emb_3.py')\n",
    "\n",
    "print('\\n this file runs the training and afterwards it use the model for validation prints out f1 .. f6 files')\n",
    "\n",
    "pritn('\\n these files can be used to calculate ROC and also graphs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
