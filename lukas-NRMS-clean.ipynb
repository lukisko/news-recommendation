{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!run\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#import transformers\n",
    "#import tokenizers\n",
    "#import datasets\n",
    "#import zipfile\n",
    "#from huggingface_hub import hf_hub_download\n",
    "device = 'cpu'\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "dim_emb = 300 # this is just starting for embedding dimensionality of a word so that training and learning is quick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_behavior = pq.read_table('ebnerd_small/train/behaviors.parquet')\n",
    "table_history = pq.read_table('ebnerd_small/train/history.parquet')\n",
    "table_articles = pq.read_table('ebnerd_small/articles.parquet')\n",
    "df_behavior = table_behavior.to_pandas()\n",
    "df_history = table_history.to_pandas()\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = df_behavior[['article_ids_inview','article_ids_clicked','user_id']]\n",
    "joined_table = main_table.join(df_history[['user_id', 'article_id_fixed']].set_index('user_id'), on='user_id', validate='many_to_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify the data so that we get the format we need for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "def remove_clicked(row):\n",
    "    index_of_clicked_one = np.where(row['article_ids_inview'] == row['article_ids_clicked'][0])\n",
    "    indexes_of_not_clicked = np.delete(row['article_ids_inview'], index_of_clicked_one)\n",
    "    indexes_of_not_clicked_suffled = np.random.choice(indexes_of_not_clicked, size=(K), replace=False) # now we have list of K = 4 things\n",
    "    indexes_of_all = np.concatenate((indexes_of_not_clicked_suffled, [row['article_ids_clicked'][0]]), axis=0) # merge random no selected ones and the selected one\n",
    "    np.random.shuffle(indexes_of_all) # suffle them\n",
    "    correct_index = np.where(indexes_of_all == row['article_ids_clicked'][0]) # get the index - used as label\n",
    "    return [indexes_of_all, correct_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_table[['articles_input_ids', 'articles_correct_idx']] = joined_table.apply(remove_clicked, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change ids for title names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_map = df_articles.set_index('article_id') # this make a significant speedup in the following method\n",
    "\n",
    "def from_ids_arr_to_article_title_arr(ids_arr):\n",
    "    return article_map.loc[ids_arr]['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_shown = joined_table[['articles_input_ids']][:].map(from_ids_arr_to_article_title_arr) # shown articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_clicked = joined_table['articles_correct_idx'] # index of selected article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_history = joined_table[['article_id_fixed']][:].map(from_ids_arr_to_article_title_arr) # history of articles shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = article_history['article_id_fixed'].apply(len).max()\n",
    "def pad_list(row):\n",
    "    padded_row = np.append(row, [''] * (max_len - len(row)))\n",
    "    return np.array(padded_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_history['article_id_fixed_padded'] = article_history['article_id_fixed'].apply(pad_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_history.npy', 'wb') as f:\n",
    "    np.save(f,article_history['article_id_fixed_padded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles_shown.npy', 'wb') as f:\n",
    "    np.save(f,articles_shown['articles_input_ids'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles_clicked.npy', 'wb') as f:\n",
    "    np.save(f, articles_clicked.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run - just if you have npy already, if not, you need to run all code up to this point\n",
    "user_history_npy = np.load('user_history.npy', allow_pickle=True)\n",
    "articles_shown_npy = np.load('articles_shown.npy', allow_pickle=True)\n",
    "articles_clicked_npy = np.load('articles_clicked.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make dataset out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "history_limit = 20\n",
    "class BrowsedCandidateClickedDataset(Dataset):\n",
    "    def __init__(self, browsed, candidate, clicked):\n",
    "        self.browsed = browsed\n",
    "        self.candidate = candidate\n",
    "        self.clicked = clicked\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.browsed)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.browsed[index][-history_limit:], self.candidate[index], self.clicked[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "full_dataset = BrowsedCandidateClickedDataset(user_history_npy, articles_shown_npy, articles_clicked_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_dataset = BrowsedCandidateClickedDataset(article_history['article_id_fixed_padded'].values, articles_shown['articles_input_ids'].values, articles_clicked.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.__getitem__(5)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "def custom_collate_fn(batch): \n",
    "    browsed, candidate, clicked = zip(*batch)\n",
    "    return list(browsed), list(candidate), list(clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run\n",
    "train_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old data loading - you do not need to run anything in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): # turn numpy arrays to a torch dataset\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_train[index], self.y_train[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take input so we can have some dummy embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_articles['title'].values\n",
    "# get all the words into list\n",
    "split_words = lambda tit : tit.split(' ')\n",
    "all_words = list(map(split_words, titles))\n",
    "words = []\n",
    "for title in all_words:\n",
    "    for word in title:\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding we need to know just unique ones\n",
    "all_words = np.unique(np.sort(np.array(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27693,)\n"
     ]
    }
   ],
   "source": [
    "# for fast access we want to have dictionary of the words - the map gives us unique index for each word - needed for torch embedding\n",
    "print(all_words.shape)\n",
    "dummy_dictionary_embedding = {}\n",
    "for i in range(len(all_words)):\n",
    "    dummy_dictionary_embedding[all_words[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare x and y\n",
    "X_all = df_articles['title'].values\n",
    "Y_all = df_articles['category_str'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 27693 + 1 # adding one symbol so that we have a special marking for empty word\n",
    "MAX_WORDS = 30 # maximum number that is in the title\n",
    "\n",
    "class MyEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim, dummy_dictionary_embedding):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_torch = nn.Embedding(word_count, emb_dim)\n",
    "        self.dummy_emb = dummy_dictionary_embedding\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten() #flatten things so that we look just at the titles\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split(\" \")\n",
    "            for word in words:\n",
    "                output.append(torch.IntTensor([self.dummy_emb[word.lower()]]))\n",
    "                \n",
    "            # all titles need to have the same number of \"words\" so I just add \"empty\" words at the end\n",
    "            for i in range(MAX_WORDS - len(words)): \n",
    "                output.append(torch.IntTensor([word_count-1]))\n",
    "        \n",
    "        output = torch.stack(output).to(device)\n",
    "        #print(output.device)\n",
    "        output = self.emb_torch(output)\n",
    "        \n",
    "        # invert the action of flataning\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim)) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the new embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, AutoTokenizer, AutoModel\n",
    "\n",
    "class XLMRobertaWordEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the tokenizer and model from the specified pretrained XLM-RoBERTa model.\n",
    "        \"\"\"\n",
    "        super(XLMRobertaWordEmbedder, self).__init__()\n",
    "\n",
    "        # Initialize the tokenizer\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained(\"NbAiLab/nb-bert-base\")\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        #self.model = AutoModel.from_pretrained(\"NbAiLab/nb-bert-base\")\n",
    "        # Set the model to evaluation mode to deactivate dropout layers\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, titles):\n",
    "        \"\"\"\n",
    "        Generates word embeddings for the provided input list of titles.\n",
    "\n",
    "        Args:\n",
    "            titles (List[str]): A list of input titles.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor containing word embeddings with shape (batch_size, seq_length, hidden_size).\n",
    "        \"\"\"\n",
    "        # Tokenize the input titles\n",
    "        print('titles', titles)\n",
    "        print('titles type', type(titles))\n",
    "        encoded_input = self.tokenizer(\n",
    "            titles,                      # List of titles to encode\n",
    "            padding='max_length',        # Pad all sequences to the max_length\n",
    "            truncation=True,             # Truncate sentences longer than max_length\n",
    "            max_length=30,               # Define a fixed max_length\n",
    "            return_tensors='pt',         # Return PyTorch tensors\n",
    "            return_attention_mask=True,  # Return attention masks\n",
    "            return_token_type_ids=False  # XLM-RoBERTa doesn't use token type IDs\n",
    "        )\n",
    "\n",
    "        # Move tensors to the same device as the model\n",
    "        device = next(self.model.parameters()).device\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            outputs = self.model(**encoded_input)\n",
    "\n",
    "        # Extract the last hidden states (token embeddings)\n",
    "        token_embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n",
    "        attention_mask = encoded_input['attention_mask']  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        return token_embeddings, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the light embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run \n",
    "import fasttext\n",
    "\n",
    "# DO NOT LOAD THE MODEL MORE THEN ONCE, IT TAKES A LOT OF RAM\n",
    "fasttext_model = fasttext.load_model('cc.da.300.bin')\n",
    "\n",
    "MAX_WORDS = 30\n",
    "class FastTextEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        #self.emb_torch = nn.Embedding(word_count, emb_dim)\n",
    "        self.embedding_fasttext = fasttext_model # this is to avoid loading the model every time we create a model, as it is very RAM hungry\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten() #flatten things so that we look just at the titles\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split()\n",
    "            for word in words:\n",
    "                output.append(torch.from_numpy(self.embedding_fasttext.get_word_vector(word)))\n",
    "                \n",
    "            # all titles need to have the same number of \"words\" so I just add \"empty\" words at the end\n",
    "            for i in range(MAX_WORDS - len(words)): \n",
    "                output.append(torch.zeros(300)) # this is vector for string with space\n",
    "        \n",
    "        output = torch.stack(output).to(device)\n",
    "        #print(output.device)\n",
    "        #output = self.emb_torch(output)\n",
    "        \n",
    "        # invert the action of flataning\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim)) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make self attention head - single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_out):\n",
    "        super().__init__()\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=-1) # TODO do I go for the correct dimension?\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=head_out, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        qe = self.lin_qk(x) # = Q_k^w e_j\n",
    "        et_qt = x @ qe.transpose(-2,-1) # = e_i^T Q_k^w e_j\n",
    "        ak = self.softmax_dim1(et_qt) # = exp(...)/ SUM exp(...)\n",
    "        # ak @ x = SUM a_i,j^k e_j \n",
    "        hk = self.lin_vk(ak @ x) # =  V_k^w (...)\n",
    "        return hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make multiple heads and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self,embedding_dimension, head_count=16, head_vector_size=16):\n",
    "        super().__init__()\n",
    "        self.head_out = head_vector_size #embedding_dimension // head_count # TODO this will be later more specific\n",
    "        self.selfAtt = nn.ModuleList([SelfAttHead(embedding_dimension, self.head_out) for _ in range(head_count)])\n",
    "        \n",
    "    def forward(self, e_s):\n",
    "        hk = []\n",
    "        for head in self.selfAtt:\n",
    "            att = head(e_s)\n",
    "            hk.append(att)\n",
    "        h = torch.cat(hk, -1) # simply concatinaiton as mentioned in paper\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch multiheadSelfAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!run - ACTUALLY ALL THE THINGS FROM HERE AND BELLOW\n",
    "class PytorchMultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the 2nd layer with the Word-Level Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden embeddings (e.g., 768 for xlm-roberta-base).\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dropout (float): Dropout probability for attention weights.\n",
    "        \"\"\"\n",
    "        super(PytorchMultiHeadSelfAttHead, self).__init__()\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures input/output tensors are (batch, seq, feature)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head self-attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, hidden_size).\n",
    "            attention_mask (torch.Tensor, optional): Attention mask of shape (batch_size, seq_length),\n",
    "                                                     where elements with value `True` are masked.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after self-attention and residual connection,\n",
    "                          shape (batch_size, seq_length, hidden_size).\n",
    "            torch.Tensor: Attention weights of shape (batch_size, num_heads, seq_length, seq_length).\n",
    "        \"\"\"\n",
    "        # Apply multi-head self-attention\n",
    "        # Note: nn.MultiheadAttention expects inputs of shape (batch, seq, feature) with batch_first=True\n",
    "        #print(x.shape)\n",
    "        input_shape = x.shape\n",
    "        \n",
    "        merged_batch_and_titles = x.reshape((-1,) + (input_shape[-2], input_shape[-1]))\n",
    "        \n",
    "        attn_output, attn_weights = self.multihead_attn(\n",
    "            query=merged_batch_and_titles,\n",
    "            key=merged_batch_and_titles,\n",
    "            value=merged_batch_and_titles,\n",
    "            key_padding_mask=attention_mask  # Masks padded tokens if provided\n",
    "        )\n",
    "        \n",
    "        #print('att out',attn_output.shape)\n",
    "        x = attn_output.reshape(input_shape)\n",
    "\n",
    "        # Apparently not used in the paper.\n",
    "        # TODO:\n",
    "        #   This be an idea to improve the model, maybe bring back with it the normalization.\n",
    "        # Add residual connections\n",
    "        # x = x + attn_output\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### additive self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveWordAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, additive_vector_dim=200):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.lin_vw = nn.Linear(in_features=embedding_dimension, out_features=additive_vector_dim)\n",
    "        self.lin_q = nn.Linear(in_features=additive_vector_dim, out_features=1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        # lin_vw(h) = V_w × h_i^w + v_w\n",
    "        # lin_q(act_fn(...)) = q_w^T tanh(...)\n",
    "        tmp = self.activation_fn(self.lin_vw(h))\n",
    "        aw = self.lin_q(tmp)\n",
    "        aw = self.softmax(aw) # exp(...) / SUM exp(...)\n",
    "        r = aw.transpose(-2,-1) @ h # SUM a_i^w h_i^w\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_count=10, head_vector_size=30, embedding_dropout=0.0):\n",
    "        super().__init__()\n",
    "        #assert embedding_dimension % head_count == 0, \"embeding must be divisible by heads\"\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        #self.embedding = MyEmbeddingLayer(dim_emb, dummy_dictionary_embedding)\n",
    "        #self.embedding = XLMRobertaWordEmbedder()\n",
    "        self.embedding = FastTextEmbeddingLayer(embedding_dimension)\n",
    "        self.embedding_drop = nn.Dropout(embedding_dropout)\n",
    "        #self.mult_head_att = MultiHeadSelfAttHead(embedding_dimension, head_count, head_vector_size)\n",
    "        #print(embedding_dimension, head_count)\n",
    "        self.mult_head_att = PytorchMultiHeadSelfAttHead(embedding_dimension, head_count)\n",
    "        #print('in word add', head_count, head_vector_size)\n",
    "        self.add_word_att = AdditiveWordAttention(head_count * head_vector_size)# 16 heads and 16 dimensions each # TODO later change the vector dim to 200\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        \n",
    "        #input_shape = x.shape\n",
    "        #print('0_0', input_shape)\n",
    "        #flatten_titles = x.flatten()\n",
    "        #print('0_1', flatten_titles.shape)\n",
    "        #titles_list = flatten_titles.tolist()\n",
    "        #token_embeddings, attention_mask = self.embedding(titles_list)\n",
    "        #e_s = token_embeddings.reshape(input_shape + (30, self.embedding_dimension))\n",
    "        e_s = self.embedding(x)\n",
    "        e_s = self.embedding_drop(e_s)\n",
    "        #print('1',e_s.shape)\n",
    "        \n",
    "        h, ignore = self.mult_head_att(e_s)\n",
    "        #print('1_1',h.shape)\n",
    "        \n",
    "        r = self.add_word_att(h)\n",
    "        #print('1_2',r.shape)\n",
    "        return r.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, emb_dimension, user_head_count=10, news_head_count=10, head_vector_size=30):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.news_encoder = MyNewsEncoder(emb_dimension, news_head_count, head_vector_size)\n",
    "        #self.multi_head_att = MultiHeadSelfAttHead(news_head_count*head_vector_size, user_head_count)\n",
    "        self.multi_head_att = PytorchMultiHeadSelfAttHead(news_head_count*head_vector_size, user_head_count)\n",
    "        self.add_news_att = AdditiveWordAttention(user_head_count*head_vector_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        r = self.news_encoder(x)\n",
    "        #print('2',r.shape)\n",
    "        \n",
    "        h, ignore = self.multi_head_att(r)\n",
    "        #print('2_1',h.shape)\n",
    "        \n",
    "        u = self.add_news_att(h)\n",
    "        #print('2_2',u.shape)\n",
    "        \n",
    "        return u.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### click predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickPredictor(nn.Module):\n",
    "    #def __init__(self, emb_dimension, user_head_count=16, news_head_count=16, head_vector_size=16):\n",
    "    def __init__(self, emb_dimension, user_head_count=10, news_head_count=10, head_vector_size=30):\n",
    "        super().__init__()\n",
    "        self.userEncoder = UserEncoder(emb_dimension, user_head_count, news_head_count, head_vector_size)\n",
    "        self.news_encoder = MyNewsEncoder(emb_dimension, news_head_count, head_vector_size)\n",
    "        \n",
    "    def forward(self, browsed_news, candidate_news):\n",
    "        \n",
    "        u = self.userEncoder(browsed_news)\n",
    "        u = u.unsqueeze(-2)\n",
    "        \n",
    "        r = self.news_encoder(candidate_news)\n",
    "        \n",
    "        ŷ = u @ r.transpose(-2, -1) # = u^T r^c\n",
    "        #ŷ = torch.tensor([torch.dot(u[i], r[i]) for i in range(u.shape[0])])\n",
    "        \n",
    "        return ŷ.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk_input = np.array([[\"Natascha var ikke den første\", \"Kun Star Wars tjente mere\", \"Luderne flytter på landet\"],['Cybersex: Hvornår er man utro?','Kniven for struben-vært får selv kniven','Willy Strube har begået selvmord']]) # fist samples to be used\n",
    "model_news = MyNewsEncoder(dim_emb)\n",
    "model_users = UserEncoder(dim_emb)\n",
    "model_click = ClickPredictor(dim_emb)\n",
    "loss_fn = nn.L1Loss()\n",
    "#optimizer = optim.Adam(model_news.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 300])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_news(dk_input)\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_users(dk_input)\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_click(dk_input, np.array([\n",
    "    ['Cybersex: Hvornår er man utro?', 'Natascha var ikke den første', 'Færdig: Bestyrelsesformand i Naviair stopper','Sælger færre billetter end sidste år: Nu bliver pladsen mindre og boderne færre', '14 døde i Italien'],\n",
    "    ['Vidner melder om fortsatte kampe i Sudan trods våbenhvile', 'Nye Schumacher-anklager: Har et problem med navnet', 'Dele af metrolinje M1 er nede', 'Strømafbrydelse: 1454 kunder ramt', 'Flere ministeriers hjemmesider ramt af nedbrud']]))\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_history_npy[:5].shape\n",
    "sample = next(iter(train_loader))\n",
    "test_click_input = sample[0]\n",
    "test_articles_shown = sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raise Exception('be carefull, this usually kills my VSCode')\n",
    "results = model_click(np.array(test_click_input), np.array(test_articles_shown))\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClickPredictor(dim_emb)\n",
    "model.to(device)\n",
    "full_dataset\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClickPredictor(\n",
       "  (userEncoder): UserEncoder(\n",
       "    (news_encoder): MyNewsEncoder(\n",
       "      (embedding): FastTextEmbeddingLayer()\n",
       "      (embedding_drop): Dropout(p=0.0, inplace=False)\n",
       "      (mult_head_att): PytorchMultiHeadSelfAttHead(\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (add_word_att): AdditiveWordAttention(\n",
       "        (activation_fn): Tanh()\n",
       "        (lin_vw): Linear(in_features=300, out_features=200, bias=True)\n",
       "        (lin_q): Linear(in_features=200, out_features=1, bias=False)\n",
       "        (softmax): Softmax(dim=1)\n",
       "      )\n",
       "    )\n",
       "    (multi_head_att): PytorchMultiHeadSelfAttHead(\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (add_news_att): AdditiveWordAttention(\n",
       "      (activation_fn): Tanh()\n",
       "      (lin_vw): Linear(in_features=300, out_features=200, bias=True)\n",
       "      (lin_q): Linear(in_features=200, out_features=1, bias=False)\n",
       "      (softmax): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (news_encoder): MyNewsEncoder(\n",
       "    (embedding): FastTextEmbeddingLayer()\n",
       "    (embedding_drop): Dropout(p=0.0, inplace=False)\n",
       "    (mult_head_att): PytorchMultiHeadSelfAttHead(\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (add_word_att): AdditiveWordAttention(\n",
       "      (activation_fn): Tanh()\n",
       "      (lin_vw): Linear(in_features=300, out_features=200, bias=True)\n",
       "      (lin_q): Linear(in_features=200, out_features=1, bias=False)\n",
       "      (softmax): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_model.torch', weights_only=True, map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 4 2 1 0 1 2 1 4 0 2 1 2 3 2 4 4 2 3 0 2 3 4 4 4 1 4 3 0 0 2]\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]]\n",
      "[[0.3157148  0.346436   0.15229757 0.071282   0.11426964]\n",
      " [0.14100063 0.22316064 0.15765892 0.26216438 0.21601538]\n",
      " [0.23605086 0.07867795 0.18790688 0.132459   0.36490536]\n",
      " [0.06988451 0.14464402 0.36836988 0.13935086 0.2777507 ]\n",
      " [0.01924644 0.4319927  0.004344   0.27453706 0.26987982]\n",
      " [0.2597966  0.15127052 0.23338029 0.17780612 0.17774644]\n",
      " [0.1316449  0.37351894 0.12789981 0.15128945 0.2156469 ]\n",
      " [0.19560502 0.15841378 0.2885711  0.2222227  0.13518743]\n",
      " [0.03870738 0.29964668 0.03039884 0.3576411  0.273606  ]\n",
      " [0.22883314 0.13242173 0.18240541 0.24673003 0.20960964]\n",
      " [0.2688946  0.15914108 0.24394068 0.23229593 0.09572767]\n",
      " [0.24250522 0.24942602 0.10151088 0.15033823 0.25621963]\n",
      " [0.3208802  0.23383848 0.01154422 0.10602263 0.32771444]\n",
      " [0.28532928 0.1689135  0.28880695 0.0202738  0.23667648]\n",
      " [0.17483146 0.18052113 0.2562474  0.14183617 0.24656388]\n",
      " [0.09318491 0.25124907 0.20396565 0.21604778 0.23555264]\n",
      " [0.2080514  0.3380665  0.34953246 0.06736612 0.03698351]\n",
      " [0.16134536 0.00086659 0.16995811 0.20548652 0.46234336]\n",
      " [0.22552519 0.2619587  0.1442865  0.18258674 0.185643  ]\n",
      " [0.24748352 0.22220519 0.12667398 0.3367577  0.06687959]\n",
      " [0.26497352 0.32890007 0.02196033 0.17459433 0.20957182]\n",
      " [0.05958942 0.36302188 0.23824368 0.02029182 0.31885317]\n",
      " [0.07759975 0.18436444 0.14247069 0.323431   0.2721342 ]\n",
      " [0.01775203 0.4086091  0.17528673 0.10145809 0.29689404]\n",
      " [0.23754257 0.18262146 0.22286741 0.18991357 0.16705494]\n",
      " [0.07238866 0.07365815 0.42956653 0.24749957 0.17688707]\n",
      " [0.1858264  0.19175863 0.29051775 0.12074117 0.211156  ]\n",
      " [0.3623267  0.15722619 0.01682825 0.18275261 0.28086627]\n",
      " [0.22958057 0.12016852 0.00860122 0.31817237 0.32347733]\n",
      " [0.13541915 0.17149633 0.23993148 0.15904424 0.29410872]\n",
      " [0.197204   0.25077403 0.15773731 0.2131759  0.18110871]\n",
      " [0.09245543 0.23044111 0.2100815  0.22701648 0.24000551]]\n",
      "[[0.3157148  0.346436   0.15229757 0.071282   0.11426964]\n",
      " [0.14100063 0.22316064 0.15765892 0.26216438 0.21601538]\n",
      " [0.23605086 0.07867795 0.18790688 0.132459   0.36490536]\n",
      " [0.06988451 0.14464402 0.36836988 0.13935086 0.2777507 ]\n",
      " [0.01924644 0.4319927  0.004344   0.27453706 0.26987982]\n",
      " [0.2597966  0.15127052 0.23338029 0.17780612 0.17774644]\n",
      " [0.1316449  0.37351894 0.12789981 0.15128945 0.2156469 ]\n",
      " [0.19560502 0.15841378 0.2885711  0.2222227  0.13518743]\n",
      " [0.03870738 0.29964668 0.03039884 0.3576411  0.273606  ]\n",
      " [0.22883314 0.13242173 0.18240541 0.24673003 0.20960964]\n",
      " [0.2688946  0.15914108 0.24394068 0.23229593 0.09572767]\n",
      " [0.24250522 0.24942602 0.10151088 0.15033823 0.25621963]\n",
      " [0.3208802  0.23383848 0.01154422 0.10602263 0.32771444]\n",
      " [0.28532928 0.1689135  0.28880695 0.0202738  0.23667648]\n",
      " [0.17483146 0.18052113 0.2562474  0.14183617 0.24656388]\n",
      " [0.09318491 0.25124907 0.20396565 0.21604778 0.23555264]\n",
      " [0.2080514  0.3380665  0.34953246 0.06736612 0.03698351]\n",
      " [0.16134536 0.00086659 0.16995811 0.20548652 0.46234336]\n",
      " [0.22552519 0.2619587  0.1442865  0.18258674 0.185643  ]\n",
      " [0.24748352 0.22220519 0.12667398 0.3367577  0.06687959]\n",
      " [0.26497352 0.32890007 0.02196033 0.17459433 0.20957182]\n",
      " [0.05958942 0.36302188 0.23824368 0.02029182 0.31885317]\n",
      " [0.07759975 0.18436444 0.14247069 0.323431   0.2721342 ]\n",
      " [0.01775203 0.4086091  0.17528673 0.10145809 0.29689404]\n",
      " [0.23754257 0.18262146 0.22286741 0.18991357 0.16705494]\n",
      " [0.07238866 0.07365815 0.42956653 0.24749957 0.17688707]\n",
      " [0.1858264  0.19175863 0.29051775 0.12074117 0.211156  ]\n",
      " [0.3623267  0.15722619 0.01682825 0.18275261 0.28086627]\n",
      " [0.22958057 0.12016852 0.00860122 0.31817237 0.32347733]\n",
      " [0.13541915 0.17149633 0.23993148 0.15904424 0.29410872]\n",
      " [0.197204   0.25077403 0.15773731 0.2131759  0.18110871]\n",
      " [0.09245543 0.23044111 0.2100815  0.22701648 0.24000551]]\n",
      "[2 2 2 0 3 1 3 2 1 3 1 2 3 4 4 0 0 1 4 0 4 1 4 2 3 0 1 0 2 1 2 0]\n",
      "[[0 0 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [1 0 0 0 0]]\n",
      "[[1.38081580e-01 1.32961765e-01 1.94800913e-01 3.26129824e-01\n",
      "  2.08025873e-01]\n",
      " [3.07884574e-01 2.09690407e-02 2.92836696e-01 2.02166885e-01\n",
      "  1.76142827e-01]\n",
      " [1.72604203e-01 2.69383967e-01 2.32096747e-01 1.52093127e-01\n",
      "  1.73821911e-01]\n",
      " [2.25416899e-01 1.61077067e-01 1.73598215e-01 1.62603751e-01\n",
      "  2.77304024e-01]\n",
      " [2.79481053e-01 1.64589897e-01 2.36704960e-01 2.35899314e-01\n",
      "  8.33247527e-02]\n",
      " [1.96243882e-01 3.93436462e-01 6.20003678e-02 1.92023396e-01\n",
      "  1.56295970e-01]\n",
      " [2.05182865e-01 2.45048389e-01 2.10748002e-01 2.11263135e-01\n",
      "  1.27757668e-01]\n",
      " [3.63851964e-01 2.20626980e-01 1.85941547e-01 7.41710886e-03\n",
      "  2.22162262e-01]\n",
      " [1.56921163e-01 2.28653178e-01 3.02067995e-01 1.50840104e-01\n",
      "  1.61517590e-01]\n",
      " [1.21460893e-04 1.66716501e-01 6.18795812e-01 1.71721056e-01\n",
      "  4.26451266e-02]\n",
      " [2.46083617e-01 2.49897137e-01 2.34002814e-01 5.12306169e-02\n",
      "  2.18785837e-01]\n",
      " [1.87089041e-01 2.68778980e-01 2.29106843e-02 2.82344133e-01\n",
      "  2.38877162e-01]\n",
      " [9.49263871e-02 2.94491020e-03 3.86306942e-02 5.40753841e-01\n",
      "  3.22744191e-01]\n",
      " [2.30727062e-01 1.44085854e-01 1.77453399e-01 1.67122424e-01\n",
      "  2.80611306e-01]\n",
      " [2.41516247e-01 2.10442796e-01 2.46768042e-01 2.10557520e-01\n",
      "  9.07154232e-02]\n",
      " [1.58014342e-01 1.82262599e-01 2.41388723e-01 2.62713939e-01\n",
      "  1.55620426e-01]\n",
      " [1.34793460e-01 1.02119476e-01 2.81319499e-01 2.73122758e-01\n",
      "  2.08644807e-01]\n",
      " [3.04421008e-01 1.80214658e-01 2.12721765e-01 9.31793004e-02\n",
      "  2.09463269e-01]\n",
      " [1.82170466e-01 2.05517098e-01 1.12463236e-01 1.54879957e-01\n",
      "  3.44969302e-01]\n",
      " [1.81839004e-01 1.90661445e-01 2.66763479e-01 1.71147168e-01\n",
      "  1.89588934e-01]\n",
      " [1.55675426e-01 2.20693767e-01 1.51777998e-01 2.16172904e-01\n",
      "  2.55679876e-01]\n",
      " [2.55325973e-01 2.93618143e-01 4.04666290e-02 4.08765107e-01\n",
      "  1.82409817e-03]\n",
      " [4.62537725e-03 3.34960938e-01 2.31039196e-01 1.26818106e-01\n",
      "  3.02556306e-01]\n",
      " [1.93737879e-01 1.21892631e-01 1.97813243e-01 1.28191784e-01\n",
      "  3.58364522e-01]\n",
      " [6.52255863e-02 7.03710765e-02 2.54914194e-01 4.31453288e-01\n",
      "  1.78035855e-01]\n",
      " [1.92214072e-01 1.23248264e-01 1.53996587e-01 1.70019194e-01\n",
      "  3.60521883e-01]\n",
      " [1.13471381e-01 1.25847712e-01 1.69205680e-01 1.60844743e-01\n",
      "  4.30630505e-01]\n",
      " [2.44595930e-01 2.78784841e-01 3.85347456e-01 2.80656125e-02\n",
      "  6.32061660e-02]\n",
      " [1.67256996e-01 2.41000816e-01 2.71371931e-01 1.91753685e-01\n",
      "  1.28616586e-01]\n",
      " [1.59633517e-01 4.85167384e-01 1.46994889e-01 1.74605295e-01\n",
      "  3.35988998e-02]\n",
      " [4.23759818e-02 3.62303227e-01 3.10984761e-01 5.20035112e-03\n",
      "  2.79135704e-01]\n",
      " [2.41333738e-01 2.28218839e-01 1.66059181e-01 2.37763330e-01\n",
      "  1.26624987e-01]]\n",
      "[[1.38081580e-01 1.32961765e-01 1.94800913e-01 3.26129824e-01\n",
      "  2.08025873e-01]\n",
      " [3.07884574e-01 2.09690407e-02 2.92836696e-01 2.02166885e-01\n",
      "  1.76142827e-01]\n",
      " [1.72604203e-01 2.69383967e-01 2.32096747e-01 1.52093127e-01\n",
      "  1.73821911e-01]\n",
      " [2.25416899e-01 1.61077067e-01 1.73598215e-01 1.62603751e-01\n",
      "  2.77304024e-01]\n",
      " [2.79481053e-01 1.64589897e-01 2.36704960e-01 2.35899314e-01\n",
      "  8.33247527e-02]\n",
      " [1.96243882e-01 3.93436462e-01 6.20003678e-02 1.92023396e-01\n",
      "  1.56295970e-01]\n",
      " [2.05182865e-01 2.45048389e-01 2.10748002e-01 2.11263135e-01\n",
      "  1.27757668e-01]\n",
      " [3.63851964e-01 2.20626980e-01 1.85941547e-01 7.41710886e-03\n",
      "  2.22162262e-01]\n",
      " [1.56921163e-01 2.28653178e-01 3.02067995e-01 1.50840104e-01\n",
      "  1.61517590e-01]\n",
      " [1.21460893e-04 1.66716501e-01 6.18795812e-01 1.71721056e-01\n",
      "  4.26451266e-02]\n",
      " [2.46083617e-01 2.49897137e-01 2.34002814e-01 5.12306169e-02\n",
      "  2.18785837e-01]\n",
      " [1.87089041e-01 2.68778980e-01 2.29106843e-02 2.82344133e-01\n",
      "  2.38877162e-01]\n",
      " [9.49263871e-02 2.94491020e-03 3.86306942e-02 5.40753841e-01\n",
      "  3.22744191e-01]\n",
      " [2.30727062e-01 1.44085854e-01 1.77453399e-01 1.67122424e-01\n",
      "  2.80611306e-01]\n",
      " [2.41516247e-01 2.10442796e-01 2.46768042e-01 2.10557520e-01\n",
      "  9.07154232e-02]\n",
      " [1.58014342e-01 1.82262599e-01 2.41388723e-01 2.62713939e-01\n",
      "  1.55620426e-01]\n",
      " [1.34793460e-01 1.02119476e-01 2.81319499e-01 2.73122758e-01\n",
      "  2.08644807e-01]\n",
      " [3.04421008e-01 1.80214658e-01 2.12721765e-01 9.31793004e-02\n",
      "  2.09463269e-01]\n",
      " [1.82170466e-01 2.05517098e-01 1.12463236e-01 1.54879957e-01\n",
      "  3.44969302e-01]\n",
      " [1.81839004e-01 1.90661445e-01 2.66763479e-01 1.71147168e-01\n",
      "  1.89588934e-01]\n",
      " [1.55675426e-01 2.20693767e-01 1.51777998e-01 2.16172904e-01\n",
      "  2.55679876e-01]\n",
      " [2.55325973e-01 2.93618143e-01 4.04666290e-02 4.08765107e-01\n",
      "  1.82409817e-03]\n",
      " [4.62537725e-03 3.34960938e-01 2.31039196e-01 1.26818106e-01\n",
      "  3.02556306e-01]\n",
      " [1.93737879e-01 1.21892631e-01 1.97813243e-01 1.28191784e-01\n",
      "  3.58364522e-01]\n",
      " [6.52255863e-02 7.03710765e-02 2.54914194e-01 4.31453288e-01\n",
      "  1.78035855e-01]\n",
      " [1.92214072e-01 1.23248264e-01 1.53996587e-01 1.70019194e-01\n",
      "  3.60521883e-01]\n",
      " [1.13471381e-01 1.25847712e-01 1.69205680e-01 1.60844743e-01\n",
      "  4.30630505e-01]\n",
      " [2.44595930e-01 2.78784841e-01 3.85347456e-01 2.80656125e-02\n",
      "  6.32061660e-02]\n",
      " [1.67256996e-01 2.41000816e-01 2.71371931e-01 1.91753685e-01\n",
      "  1.28616586e-01]\n",
      " [1.59633517e-01 4.85167384e-01 1.46994889e-01 1.74605295e-01\n",
      "  3.35988998e-02]\n",
      " [4.23759818e-02 3.62303227e-01 3.10984761e-01 5.20035112e-03\n",
      "  2.79135704e-01]\n",
      " [2.41333738e-01 2.28218839e-01 1.66059181e-01 2.37763330e-01\n",
      "  1.26624987e-01]]\n",
      "[2 2 0 3 2 3 0 0 0 3 4 0 1 1 1 3 2 4 1 0 1 1 3 2 0 4 0 4 0 3 1 3]\n",
      "[[0 0 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]]\n",
      "[[0.06919584 0.23887575 0.2957922  0.20660615 0.18953009]\n",
      " [0.15581453 0.2597398  0.25118965 0.00168252 0.33157352]\n",
      " [0.31287307 0.02334603 0.01593146 0.41840577 0.22944374]\n",
      " [0.02582831 0.30904862 0.24350877 0.2814737  0.14014065]\n",
      " [0.20087919 0.18915232 0.22449297 0.21183498 0.17364056]\n",
      " [0.16125615 0.24635793 0.17797339 0.22245084 0.19196166]\n",
      " [0.14442669 0.20638685 0.330812   0.00352423 0.3148502 ]\n",
      " [0.34902003 0.11178801 0.17916957 0.13172124 0.22830114]\n",
      " [0.33965126 0.24322988 0.27726445 0.01433874 0.12551568]\n",
      " [0.40698677 0.11214202 0.01757241 0.15825193 0.30504683]\n",
      " [0.24082884 0.13747773 0.13185708 0.32148308 0.1683533 ]\n",
      " [0.58188874 0.01630247 0.00170448 0.14803271 0.25207165]\n",
      " [0.30380458 0.16348128 0.10170449 0.25200748 0.17900214]\n",
      " [0.11717672 0.17041813 0.26901957 0.1702224  0.27316314]\n",
      " [0.0016824  0.4789593  0.22973783 0.26487866 0.02474181]\n",
      " [0.39619878 0.28277215 0.01411942 0.28407416 0.0228355 ]\n",
      " [0.0053392  0.2228194  0.30565557 0.2027966  0.2633892 ]\n",
      " [0.23706694 0.12661421 0.22818212 0.18958546 0.21855131]\n",
      " [0.15018882 0.20715573 0.21295708 0.2500185  0.1796799 ]\n",
      " [0.33783278 0.16511072 0.20427272 0.11679655 0.17598723]\n",
      " [0.22403045 0.14527485 0.17089064 0.23730886 0.2224952 ]\n",
      " [0.09807453 0.36840168 0.15359887 0.1702199  0.20970497]\n",
      " [0.0059709  0.14342213 0.2714852  0.2760416  0.30308017]\n",
      " [0.32327917 0.1160771  0.2296769  0.30320925 0.02775763]\n",
      " [0.17071672 0.2519841  0.19481035 0.21747385 0.16501497]\n",
      " [0.21815704 0.03497825 0.32902643 0.22222358 0.19561467]\n",
      " [0.16720723 0.21725234 0.22006232 0.19832619 0.19715199]\n",
      " [0.15879552 0.2025114  0.21943739 0.09573915 0.3235165 ]\n",
      " [0.4284877  0.24659187 0.07575955 0.20970353 0.03945738]\n",
      " [0.16248558 0.39308187 0.09073415 0.22207369 0.13162468]\n",
      " [0.01704379 0.27273476 0.21646632 0.21412249 0.2796327 ]\n",
      " [0.00647018 0.21778432 0.35694492 0.18328151 0.23551907]]\n",
      "[[0.06919584 0.23887575 0.2957922  0.20660615 0.18953009]\n",
      " [0.15581453 0.2597398  0.25118965 0.00168252 0.33157352]\n",
      " [0.31287307 0.02334603 0.01593146 0.41840577 0.22944374]\n",
      " [0.02582831 0.30904862 0.24350877 0.2814737  0.14014065]\n",
      " [0.20087919 0.18915232 0.22449297 0.21183498 0.17364056]\n",
      " [0.16125615 0.24635793 0.17797339 0.22245084 0.19196166]\n",
      " [0.14442669 0.20638685 0.330812   0.00352423 0.3148502 ]\n",
      " [0.34902003 0.11178801 0.17916957 0.13172124 0.22830114]\n",
      " [0.33965126 0.24322988 0.27726445 0.01433874 0.12551568]\n",
      " [0.40698677 0.11214202 0.01757241 0.15825193 0.30504683]\n",
      " [0.24082884 0.13747773 0.13185708 0.32148308 0.1683533 ]\n",
      " [0.58188874 0.01630247 0.00170448 0.14803271 0.25207165]\n",
      " [0.30380458 0.16348128 0.10170449 0.25200748 0.17900214]\n",
      " [0.11717672 0.17041813 0.26901957 0.1702224  0.27316314]\n",
      " [0.0016824  0.4789593  0.22973783 0.26487866 0.02474181]\n",
      " [0.39619878 0.28277215 0.01411942 0.28407416 0.0228355 ]\n",
      " [0.0053392  0.2228194  0.30565557 0.2027966  0.2633892 ]\n",
      " [0.23706694 0.12661421 0.22818212 0.18958546 0.21855131]\n",
      " [0.15018882 0.20715573 0.21295708 0.2500185  0.1796799 ]\n",
      " [0.33783278 0.16511072 0.20427272 0.11679655 0.17598723]\n",
      " [0.22403045 0.14527485 0.17089064 0.23730886 0.2224952 ]\n",
      " [0.09807453 0.36840168 0.15359887 0.1702199  0.20970497]\n",
      " [0.0059709  0.14342213 0.2714852  0.2760416  0.30308017]\n",
      " [0.32327917 0.1160771  0.2296769  0.30320925 0.02775763]\n",
      " [0.17071672 0.2519841  0.19481035 0.21747385 0.16501497]\n",
      " [0.21815704 0.03497825 0.32902643 0.22222358 0.19561467]\n",
      " [0.16720723 0.21725234 0.22006232 0.19832619 0.19715199]\n",
      " [0.15879552 0.2025114  0.21943739 0.09573915 0.3235165 ]\n",
      " [0.4284877  0.24659187 0.07575955 0.20970353 0.03945738]\n",
      " [0.16248558 0.39308187 0.09073415 0.22207369 0.13162468]\n",
      " [0.01704379 0.27273476 0.21646632 0.21412249 0.2796327 ]\n",
      " [0.00647018 0.21778432 0.35694492 0.18328151 0.23551907]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 29\u001b[0m\n\u001b[1;32m     15\u001b[0m train_loss_batches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m browsed, candidate, clicked \u001b[38;5;129;01min\u001b[39;00m train_loader:\u001b[38;5;66;03m#[(tmp_dk_input, target)]:#train_loader:#[(dk_input, target)]:#train_loader:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print(targets)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Forward pass.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# print('in cand', np.array(candidate))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#print('in cand', np.array(candidate).shape)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrowsed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#model(np.array(tuple(dk_input)))#model(np.array(inputs))\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#output = model(np.array(browsed))\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Compute loss.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print(clicked)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     targ_ind \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(clicked)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m, in \u001b[0;36mClickPredictor.forward\u001b[0;34m(self, browsed_news, candidate_news)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, browsed_news, candidate_news):\n\u001b[0;32m---> 10\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muserEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrowsed_news\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_encoder(candidate_news)\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m, in \u001b[0;36mUserEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 12\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnews_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#print('2',r.shape)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     h, ignore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_att(r)\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mMyNewsEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m e_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_drop(e_s)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#print('1',e_s.shape)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m h, ignore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmult_head_att\u001b[49m\u001b[43m(\u001b[49m\u001b[43me_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#print('1_1',h.shape)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_word_att(h)\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m, in \u001b[0;36mPytorchMultiHeadSelfAttHead.forward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     41\u001b[0m merged_batch_and_titles \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m (input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 43\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_batch_and_titles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_batch_and_titles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_batch_and_titles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Masks padded tokens if provided\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#print('att out',attn_output.shape)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(input_shape)\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/functional.py:6251\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6246\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_output_weights, v)\n\u001b[1;32m   6248\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   6249\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(tgt_len \u001b[38;5;241m*\u001b[39m bsz, embed_dim)\n\u001b[1;32m   6250\u001b[0m )\n\u001b[0;32m-> 6251\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6252\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz, attn_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6254\u001b[0m \u001b[38;5;66;03m# optionally average attention weights over heads\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "validation_every_steps = 5\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "train_loss = []\n",
    "validation_accuracies = []\n",
    "validation_loss = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    train_loss_batches = []\n",
    "    \n",
    "    for browsed, candidate, clicked in train_loader:#[(tmp_dk_input, target)]:#train_loader:#[(dk_input, target)]:#train_loader:\n",
    "        #print(targets)\n",
    "        # Forward pass.\n",
    "        #print('broken',inputs)\n",
    "        # print('working',target)\n",
    "        # print('in brow', browsed)\n",
    "        # print('in brow', np.array(browsed))\n",
    "        # print('in brow', np.array(browsed).shape)\n",
    "        # print('in cand', candidate)\n",
    "        # print('in cand', np.array(candidate))\n",
    "        #print('in cand', np.array(candidate).shape)\n",
    "        \n",
    "        output = model(np.array(browsed), np.array(candidate))#model(np.array(tuple(dk_input)))#model(np.array(inputs))\n",
    "        #output = model(np.array(browsed))\n",
    "        \n",
    "        # Compute loss.\n",
    "        #print(clicked)\n",
    "        targ_ind = torch.tensor(clicked).to(device)\n",
    "        loss = loss_fn(output, targ_ind)\n",
    "        train_loss_batches.append(loss.cpu().data.numpy())#get_numpy(loss))#.detach().numpy())\n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        #print(output)\n",
    "        predictions =  torch.argmax(output, dim=-1)#.max(1)[1]\n",
    "        #print('out:', output)\n",
    "        #print('predictions:', predictions)\n",
    "        #print('targets:', targ_ind)\n",
    "        #print('targ_ind', targ_ind)\n",
    "        #print('predictions', predictions)\n",
    "        #print(targ_ind.device)\n",
    "        #print(predictions.device)\n",
    "        #print('1', targ_ind.cpu().data.numpy())\n",
    "        #print('2', F.softmax(output).cpu().data.numpy())\n",
    "        y_true = targ_ind.cpu().data.numpy()\n",
    "        y_pred = F.softmax(output, dim=-1).cpu().data.numpy()\n",
    "        batch_classes = np.unique(y_true)\n",
    "        y_true_binarized = label_binarize(y_true, classes=batch_classes)\n",
    "        print(y_true)\n",
    "        print(y_true_binarized)\n",
    "        print(y_pred)\n",
    "        print(y_pred[:, batch_classes])\n",
    "        calculated_acc = roc_auc_score(y_true_binarized, y_pred[:, batch_classes], multi_class='ovr')\n",
    "        #print('AUC',calculated_acc)\n",
    "        train_accuracies_batches.append(calculated_acc)\n",
    "        \n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            train_loss.append(np.mean(train_loss_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "            train_loss_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            # validation_accuracies_batches = []\n",
    "            # with torch.no_grad():\n",
    "            #     model.eval()\n",
    "            #     for inputs, targets in validation_loader:\n",
    "            #         output = model(inputs)\n",
    "            #         loss = loss_fn(output, targets.float())\n",
    "\n",
    "            #         predictions = output.max(1)[1]\n",
    "            #         targ_ind = targets.max(1)[1]\n",
    "                    \n",
    "            #         # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "            #         validation_accuracies_batches.append(accuracy_score(targ_ind, predictions) * len(inputs))\n",
    "\n",
    "            #     model.train()\n",
    "                \n",
    "            # # Append average validation accuracy to list.\n",
    "            # validation_accuracies.append(np.sum(validation_accuracies_batches) / len(validation_dataset))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}, loss: {train_loss[-1]}\")\n",
    "            #print(f\"             validation accuracy: {validation_accuracies[-1]}\")\n",
    "            break\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
