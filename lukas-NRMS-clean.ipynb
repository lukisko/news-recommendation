{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#import transformers\n",
    "#import tokenizers\n",
    "#import datasets\n",
    "#import zipfile\n",
    "#from huggingface_hub import hf_hub_download\n",
    "device = 'cpu'\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 50 # this is just starting for embedding dimensionality of a word so that training and learning is quick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_behavior = pq.read_table('ebnerd_small/train/behaviors.parquet')\n",
    "table_history = pq.read_table('ebnerd_small/train/history.parquet')\n",
    "table_articles = pq.read_table('ebnerd_small/articles.parquet')\n",
    "df_behavior = table_behavior.to_pandas()\n",
    "df_history = table_history.to_pandas()\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): # turn numpy arrays to a torch dataset\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_train[index], self.y_train[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take input so we can have some dummy embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_articles['title'].values\n",
    "# get all the words into list\n",
    "split_words = lambda tit : tit.split(' ')\n",
    "all_words = list(map(split_words, titles))\n",
    "words = []\n",
    "for title in all_words:\n",
    "    for word in title:\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding we need to know just unique ones\n",
    "all_words = np.unique(np.sort(np.array(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27693,)\n"
     ]
    }
   ],
   "source": [
    "# for fast access we want to have dictionary of the words - the map gives us unique index for each word - needed for torch embedding\n",
    "print(all_words.shape)\n",
    "dummy_dictionary_embedding = {}\n",
    "for i in range(len(all_words)):\n",
    "    dummy_dictionary_embedding[all_words[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare x and y\n",
    "X_all = df_articles['title'].values\n",
    "Y_all = df_articles['category_str'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 27693 + 1 # adding one symbol so that we have a special marking for empty word\n",
    "MAX_WORDS = 30 # maximum number that is in the title\n",
    "\n",
    "class MyEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_torch = nn.Embedding(word_count, emb_dim)\n",
    "        self.dummy_emb = dummy_dictionary_embedding\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten() #flatten things so that we look just at the titles\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split(\" \")\n",
    "            for word in words:\n",
    "                output.append(torch.IntTensor([self.dummy_emb[word.lower()]]))\n",
    "                \n",
    "            # all titles need to have the same number of \"words\" so I just add \"empty\" words at the end\n",
    "            for i in range(MAX_WORDS - len(words)): \n",
    "                output.append(torch.IntTensor([word_count-1]))\n",
    "        \n",
    "        output = torch.stack(output) \n",
    "        output = self.emb_torch(output)\n",
    "        \n",
    "        # invert the action of flataning\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim)) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make self attention head - single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_out):\n",
    "        super().__init__()\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1) # TODO do I go for the correct dimension?\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=head_out, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        qe = self.lin_qk(x) # = Q_k^w e_j\n",
    "        et_qt = x @ qe.transpose(-2,-1) # = e_i^T Q_k^w e_j\n",
    "        ak = self.softmax_dim1(et_qt) # = exp(...)/ SUM exp(...)\n",
    "        # ak @ x = SUM a_i,j^k e_j \n",
    "        hk = self.lin_vk(ak @ x) # =  V_k^w (...)\n",
    "        return hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make multiple heads and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self,embedding_dimension, head_count):\n",
    "        super().__init__()\n",
    "        self.head_out = embedding_dimension // head_count # TODO this will be later more specific\n",
    "        self.selfAtt = nn.ModuleList([SelfAttHead(dim_emb, self.head_out) for _ in range(head_count)])\n",
    "        \n",
    "    def forward(self, e_s):\n",
    "        hk = []\n",
    "        for head in self.selfAtt:\n",
    "            att = head(e_s)\n",
    "            hk.append(att)\n",
    "        h = torch.cat(hk, -1) # simply concatinaiton as mentioned in paper\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### additive self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveWordAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, additive_vector_dim):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.lin_vw = nn.Linear(in_features=embedding_dimension, out_features=additive_vector_dim)\n",
    "        self.lin_q = nn.Linear(in_features=additive_vector_dim, out_features=1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        # lin_vw(h) = V_w × h_i^w + v_w\n",
    "        # lin_q(act_fn(...)) = q_w^T tanh(...)\n",
    "        aw = self.lin_q(self.activation_fn(self.lin_vw(h)))\n",
    "        aw = self.softmax(aw) # exp(...) / SUM exp(...)\n",
    "        r = aw.transpose(-2,-1) @ h # SUM a_i^w h_i^w\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_count=1, embedding_dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embedding_dimension % head_count == 0, \"embeding must be divisible by heads\"\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb)\n",
    "        self.embedding_drop = nn.Dropout(embedding_dropout)\n",
    "        self.mult_head_att = MultiHeadSelfAttHead(embedding_dimension, head_count)\n",
    "        self.add_word_att = AdditiveWordAttention(embedding_dimension, embedding_dimension) # TODO later change the vector dim to 200\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        \n",
    "        e_s = self.embedding(x)\n",
    "        e_s = self.embedding_drop(e_s)\n",
    "        print('1',e_s.shape)\n",
    "        \n",
    "        h = self.mult_head_att(e_s)\n",
    "        print('1_1',h.shape)\n",
    "        \n",
    "        r = self.add_word_att(h)\n",
    "        print('1_2',r.shape)\n",
    "        return r.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, emb_dimension, user_head_count=1, news_head_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.news_encoder = MyNewsEncoder(emb_dimension, news_head_count)\n",
    "        self.multi_head_att = MultiHeadSelfAttHead(emb_dimension, user_head_count)\n",
    "        self.add_news_att = AdditiveWordAttention(emb_dimension,emb_dimension)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        r = self.news_encoder(x)\n",
    "        print('2',r.shape)\n",
    "        \n",
    "        h = self.multi_head_att(r)\n",
    "        print('2_1',h.shape)\n",
    "        \n",
    "        u = self.add_news_att(h)\n",
    "        print('2_2',u.shape)\n",
    "        \n",
    "        return u.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### click predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickPredictor(nn.Module):\n",
    "    def __init__(self, emb_dimension, user_head_count=1, news_head_count=1):\n",
    "        super().__init__()\n",
    "        self.userEncoder = UserEncoder(emb_dimension, user_head_count, news_head_count)\n",
    "        self.news_encoder = MyNewsEncoder(emb_dimension, news_head_count)\n",
    "        \n",
    "    def forward(self, browsed_news, candidate_news):\n",
    "        \n",
    "        u = self.userEncoder(browsed_news)\n",
    "        \n",
    "        r = self.news_encoder(candidate_news)\n",
    "        \n",
    "        ŷ = torch.diag(u @ r.transpose(-2, -1)) # = u^T r^c\n",
    "        #ŷ = torch.tensor([torch.dot(u[i], r[i]) for i in range(u.shape[0])])\n",
    "        \n",
    "        return ŷ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dk_input = np.array([[\"Natascha var ikke den første\", \"Kun Star Wars tjente mere\", \"Luderne flytter på landet\"],['Cybersex: Hvornår er man utro?','Kniven for struben-vært får selv kniven','Willy Strube har begået selvmord']]) # fist samples to be used\n",
    "model_news = MyNewsEncoder(dim_emb, head_count=10)\n",
    "model_users = UserEncoder(dim_emb, news_head_count=10)\n",
    "model_click = ClickPredictor(dim_emb)\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([2, 3, 30, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [6, 50] but got: [6, 30].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdk_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m shape_tmp \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      3\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m loss_fn(results, torch\u001b[38;5;241m.\u001b[39mrandn(shape_tmp))\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[198], line 17\u001b[0m, in \u001b[0;36mMyNewsEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m e_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_drop(e_s)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m,e_s\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 17\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmult_head_att\u001b[49m\u001b[43m(\u001b[49m\u001b[43me_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_1\u001b[39m\u001b[38;5;124m'\u001b[39m,h\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_word_att(h)\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[196], line 10\u001b[0m, in \u001b[0;36mMultiHeadSelfAttHead.forward\u001b[0;34m(self, e_s)\u001b[0m\n\u001b[1;32m      8\u001b[0m hk \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselfAtt:\n\u001b[0;32m---> 10\u001b[0m     att \u001b[38;5;241m=\u001b[39m \u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43me_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     hk\u001b[38;5;241m.\u001b[39mappend(att)\n\u001b[1;32m     12\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(hk, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# simply concatinaiton as mentioned in paper\u001b[39;00m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[195], line 10\u001b[0m, in \u001b[0;36mSelfAttHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m      9\u001b[0m     qe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_qk(x) \u001b[38;5;66;03m# = Q_k^w e_j\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     et_qt \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqe\u001b[49m \u001b[38;5;66;03m#.transpose(-2,-1) # = e_i^T Q_k^w e_j\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     ak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_dim1(et_qt) \u001b[38;5;66;03m# = exp(...)/ SUM exp(...)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# ak @ x = SUM a_i,j^k e_j \u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [6, 50] but got: [6, 30]."
     ]
    }
   ],
   "source": [
    "results = model_news(dk_input)\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([2, 3, 30, 50])\n",
      "1_1 torch.Size([2, 3, 30, 50])\n",
      "1_2 torch.Size([2, 3, 1, 50])\n",
      "2 torch.Size([2, 3, 50])\n",
      "2_1 torch.Size([2, 3, 50])\n",
      "2_2 torch.Size([2, 1, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_users(dk_input)\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([2, 3, 30, 50])\n",
      "1_1 torch.Size([2, 3, 30, 50])\n",
      "1_2 torch.Size([2, 3, 1, 50])\n",
      "2 torch.Size([2, 3, 50])\n",
      "2_1 torch.Size([2, 3, 50])\n",
      "2_2 torch.Size([2, 1, 50])\n",
      "1 torch.Size([2, 30, 50])\n",
      "1_1 torch.Size([2, 30, 50])\n",
      "1_2 torch.Size([2, 1, 50])\n",
      "u torch.Size([2, 50])\n",
      "r torch.Size([2, 50])\n",
      "tensor([[ 55.9878, 104.1340],\n",
      "        [ 61.2347,  99.8069]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_click(dk_input, np.array(['Cybersex: Hvornår er man utro?', 'Natascha var ikke den første']))\n",
    "shape_tmp = results.shape\n",
    "test_loss = loss_fn(results, torch.randn(shape_tmp))\n",
    "test_loss.backward()\n",
    "shape_tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
