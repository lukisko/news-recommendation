{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with loading the glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#import transformers\n",
    "#import tokenizers\n",
    "#import datasets\n",
    "#import zipfile\n",
    "#from huggingface_hub import hf_hub_download\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400001/400001 [00:08<00:00, 48825.03it/s]\n"
     ]
    }
   ],
   "source": [
    "target_file = \"glove.6B.50d.txt\"\n",
    "vocabulary = []\n",
    "vectors = []\n",
    "emb_dic = {}\n",
    "with open(target_file, \"r\", encoding=\"utf8\") as f:\n",
    "    for l in tqdm(f.readlines() ):\n",
    "        word, *vector = l.split()\n",
    "        vocabulary.append(word)\n",
    "        vector_t = torch.tensor([float(v) for v in vector])\n",
    "        vectors.append(vector_t)\n",
    "        emb_dic[word] = vector_t\n",
    "vectors = torch.stack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somtehing from week 4\n",
    "dim_emb = 50\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.mul_head_att = nn.MultiheadAttention(dim_emb,1)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_k = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_v = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        \n",
    "        self.ff1 = nn.Linear(in_features=dim_emb, out_features=128)\n",
    "        self.ff2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "        #self.l_1 = nn.Linear(in_features=int(features_cat_size),\n",
    "        #                  out_features=l_1_hidden_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        words = x.split(\" \")\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            print(word)\n",
    "            vectors.append(torch.Tensor(emb_dic[word.lower()]))\n",
    "        #features_final = self.l_1(x)\n",
    "        e_s = torch.stack(vectors)\n",
    "        q = self.lin_q(e_s)\n",
    "        k = self.lin_k(e_s)\n",
    "        v = self.lin_v(e_s)\n",
    "        attn_out, attn_weight = self.mul_head_att(q,k,v)\n",
    "        #print(attn_out)\n",
    "        cls_tok_emb = attn_out[0,:]\n",
    "        print(cls_tok_emb)\n",
    "        \n",
    "        output = self.ff1(cls_tok_emb)\n",
    "        output = self.activation_fn(output)\n",
    "        output = self.ff2(output)\n",
    "        \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NewsEncoder(5) # TODO - I need to change this with a number that will correspond to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewsEncoder(\n",
       "  (activation_fn): ReLU()\n",
       "  (mul_head_att): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "  )\n",
       "  (lin_q): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (lin_k): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (lin_v): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (ff1): Linear(in_features=50, out_features=128, bias=True)\n",
       "  (ff2): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"Breaking news about this\"\n",
    "test2 = \"This has happened again\"\n",
    "#test3 = \"Natascha var ikke den foerste\"\n",
    "#test4 = \"Kun Star Wars tjente mere\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "has\n",
      "happened\n",
      "again\n",
      "tensor([ 6.4050e-02,  5.3003e-02,  3.0495e-01,  2.7242e-02, -7.4303e-02,\n",
      "        -1.2375e-01,  4.5723e-02, -1.6454e-02,  1.5557e-01,  2.3183e-01,\n",
      "        -3.6420e-04,  2.2021e-01, -2.4660e-02,  1.1702e-01,  1.3349e-01,\n",
      "         3.0678e-01,  1.6261e-01,  6.5852e-02, -1.4067e-01, -5.7109e-02,\n",
      "         6.3530e-02,  4.5563e-02,  3.8250e-02, -1.1695e-01, -9.1109e-02,\n",
      "        -7.5147e-02, -3.4102e-02, -1.7600e-02,  3.6804e-01,  3.5737e-02,\n",
      "        -7.7090e-02, -3.1313e-01, -2.3903e-02, -2.5665e-01,  3.9103e-01,\n",
      "        -2.1776e-01, -1.9104e-02, -2.2428e-01,  7.5744e-03,  9.9495e-02,\n",
      "         1.4469e-01, -2.1187e-01, -1.9310e-01, -2.9819e-01, -6.9273e-02,\n",
      "         2.2607e-01, -1.0778e-01,  1.5534e-01, -4.4466e-03, -2.1049e-01],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0263,  0.0323, -0.0106, -0.1667,  0.0241], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output = model.forward(test2)\n",
    "shape = attn_output.shape\n",
    "attn_output.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brand new encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim, emb_dic):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_dic = emb_dic\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten()\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split(\" \")\n",
    "            for word in words:\n",
    "                output.append(torch.Tensor(self.emb_dic[word.lower()]))\n",
    "            \n",
    "        output = torch.stack(output)\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim))\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.5307e-01, -1.4580e-02, -7.5133e-02,  1.0766e-01, -1.1661e-02,\n",
       "           4.1505e-01,  7.2856e-02,  4.9378e-01,  1.4302e-01,  2.7014e-01,\n",
       "          -2.9928e-01, -4.9729e-01, -1.1358e+00,  5.7973e-01,  5.5898e-01,\n",
       "          -3.1587e-01, -5.7323e-02, -7.2313e-01, -4.0444e-01, -2.3284e-01,\n",
       "           1.8744e-01,  3.2356e-01, -1.6800e-02, -3.3481e-01,  2.2256e-01,\n",
       "          -1.3342e+00,  4.9411e-01,  8.8150e-02,  8.0475e-01, -1.7204e-01,\n",
       "           2.7073e+00,  1.2706e-01,  9.3040e-02,  3.3197e-01, -7.0747e-01,\n",
       "           2.3589e-01,  3.0587e-01,  1.6879e-01, -1.8252e-01, -1.2134e-01,\n",
       "          -2.3045e-01, -1.5842e-01,  5.3029e-02,  4.9780e-01,  1.1690e-01,\n",
       "           4.4732e-02,  2.3904e-01, -4.5940e-01, -4.5749e-01, -6.2330e-01],\n",
       "         [-2.0825e-01,  4.7786e-01,  5.2196e-01,  1.0587e+00, -1.0045e-01,\n",
       "          -1.1269e+00, -1.2581e+00, -1.1041e-01, -7.4125e-02, -7.7976e-01,\n",
       "          -3.7942e-01, -2.4860e-01, -3.9224e-01,  4.2972e-01,  9.8060e-01,\n",
       "           1.2668e-01, -1.3772e+00, -2.2793e-01, -1.8497e-01,  4.1014e-01,\n",
       "           9.6781e-01,  8.9160e-01,  8.4685e-01,  5.7416e-01,  4.6455e-01,\n",
       "          -1.7287e+00, -6.3918e-01,  5.6256e-01, -1.2651e-01,  4.9711e-01,\n",
       "           3.3326e+00,  3.4399e-02,  4.6149e-01, -4.4826e-01, -1.1945e+00,\n",
       "          -4.7593e-01, -3.1927e-01, -6.4420e-01,  8.9735e-02,  7.3952e-02,\n",
       "           7.0755e-01,  5.2948e-01, -1.2034e-01, -4.6779e-01,  2.4722e-01,\n",
       "           2.8045e-01, -6.2632e-01,  1.4458e+00,  5.1045e-01,  7.4156e-01],\n",
       "         [ 8.9466e-01,  3.6604e-01,  3.7588e-01, -4.1818e-01,  5.8462e-01,\n",
       "           1.8594e-01, -4.1907e-01, -4.6621e-01, -5.4903e-01,  2.4770e-02,\n",
       "          -9.0816e-01, -4.8271e-01, -5.0742e-02, -7.4039e-01,  1.4377e+00,\n",
       "          -1.9740e-02, -2.3840e-01,  4.3154e-01, -6.6120e-01, -4.1275e-01,\n",
       "           2.5475e-01,  9.3498e-01,  8.1404e-01, -1.7296e-01,  6.1296e-01,\n",
       "          -1.8475e+00, -2.7616e-01,  2.7701e-01,  4.2347e-01, -1.1599e-01,\n",
       "           3.6243e+00,  1.2306e-01, -2.3526e-02, -2.4843e-01, -2.2376e-01,\n",
       "          -5.3941e-01, -6.2444e-01, -2.7711e-01,  4.9406e-01,  2.0234e-02,\n",
       "          -2.3460e-01,  4.4512e-01,  5.3397e-01,  6.6654e-01, -9.3662e-02,\n",
       "          -3.5203e-02, -6.4194e-02,  5.5998e-01, -6.6593e-01,  1.2177e-01],\n",
       "         [ 5.3074e-01,  4.0117e-01, -4.0785e-01,  1.5444e-01,  4.7782e-01,\n",
       "           2.0754e-01, -2.6951e-01, -3.4023e-01, -1.0879e-01,  1.0563e-01,\n",
       "          -1.0289e-01,  1.0849e-01, -4.9681e-01, -2.5128e-01,  8.4025e-01,\n",
       "           3.8949e-01,  3.2284e-01, -2.2797e-01, -4.4342e-01, -3.1649e-01,\n",
       "          -1.2406e-01, -2.8170e-01,  1.9467e-01,  5.5513e-02,  5.6705e-01,\n",
       "          -1.7419e+00, -9.1145e-01,  2.7036e-01,  4.1927e-01,  2.0279e-02,\n",
       "           4.0405e+00, -2.4943e-01, -2.0416e-01, -6.2762e-01, -5.4783e-02,\n",
       "          -2.6883e-01,  1.8444e-01,  1.8204e-01, -2.3536e-01, -1.6155e-01,\n",
       "          -2.7655e-01,  3.5506e-02, -3.8211e-01, -7.5134e-04, -2.4822e-01,\n",
       "           2.8164e-01,  1.2819e-01,  2.8762e-01,  1.4440e-01,  2.3611e-01]],\n",
       "\n",
       "        [[ 5.3074e-01,  4.0117e-01, -4.0785e-01,  1.5444e-01,  4.7782e-01,\n",
       "           2.0754e-01, -2.6951e-01, -3.4023e-01, -1.0879e-01,  1.0563e-01,\n",
       "          -1.0289e-01,  1.0849e-01, -4.9681e-01, -2.5128e-01,  8.4025e-01,\n",
       "           3.8949e-01,  3.2284e-01, -2.2797e-01, -4.4342e-01, -3.1649e-01,\n",
       "          -1.2406e-01, -2.8170e-01,  1.9467e-01,  5.5513e-02,  5.6705e-01,\n",
       "          -1.7419e+00, -9.1145e-01,  2.7036e-01,  4.1927e-01,  2.0279e-02,\n",
       "           4.0405e+00, -2.4943e-01, -2.0416e-01, -6.2762e-01, -5.4783e-02,\n",
       "          -2.6883e-01,  1.8444e-01,  1.8204e-01, -2.3536e-01, -1.6155e-01,\n",
       "          -2.7655e-01,  3.5506e-02, -3.8211e-01, -7.5134e-04, -2.4822e-01,\n",
       "           2.8164e-01,  1.2819e-01,  2.8762e-01,  1.4440e-01,  2.3611e-01],\n",
       "         [ 5.4822e-01,  3.8847e-02,  1.0127e-01,  3.1319e-01,  9.5487e-02,\n",
       "           4.1814e-01, -7.9493e-01, -5.8296e-01,  2.6643e-02,  1.2392e-01,\n",
       "           3.5194e-01, -2.1630e-02, -8.7018e-01, -2.7178e-01,  6.5449e-01,\n",
       "           4.2934e-01,  9.7544e-02,  3.1779e-01, -1.1921e-01, -9.7106e-02,\n",
       "          -4.7585e-01,  2.4907e-01,  1.2230e-01, -2.9079e-01, -1.6866e-01,\n",
       "          -2.1072e+00,  2.2174e-02,  4.5277e-01, -6.4485e-01,  1.3181e-01,\n",
       "           3.6594e+00, -1.7140e-01,  2.3919e-01, -4.2249e-01, -8.8331e-02,\n",
       "          -3.2925e-01, -1.2847e-01,  4.7055e-01, -7.5953e-02, -2.7747e-01,\n",
       "          -4.1905e-01,  6.0803e-01, -2.4261e-01,  1.4885e-02, -2.3204e-01,\n",
       "           2.0879e-02, -8.2175e-01,  2.6588e-01, -4.0267e-01, -1.7111e-01],\n",
       "         [ 7.4296e-01, -2.3418e-01, -3.0478e-02,  8.1639e-02,  5.3106e-01,\n",
       "          -3.2628e-01, -3.6518e-01,  9.5918e-01, -6.1010e-01, -3.6267e-01,\n",
       "          -4.2784e-01, -2.3545e-01, -1.0559e+00, -4.2271e-01,  1.7476e+00,\n",
       "           3.8830e-01, -4.3234e-02,  8.9555e-02, -8.8168e-01,  1.6945e-02,\n",
       "           1.8666e-02,  7.5625e-01,  3.5678e-01,  3.0278e-01,  1.3405e+00,\n",
       "          -1.7071e+00, -1.1773e+00,  4.8061e-01,  1.2130e+00, -3.7524e-01,\n",
       "           2.1490e+00, -3.5389e-02, -2.6194e-01, -8.4915e-01, -1.0237e-01,\n",
       "          -3.2459e-01,  3.4824e-01, -3.3020e-01,  4.4822e-01,  4.0217e-01,\n",
       "          -1.0529e+00, -1.3292e-01,  1.5945e-02,  1.8610e-02, -4.5737e-02,\n",
       "           2.3829e-01,  8.3602e-02,  1.4976e-01,  3.3331e-01,  3.6752e-01],\n",
       "         [-1.1008e-01, -4.2842e-01,  2.3023e-02, -6.6241e-01, -1.1844e-02,\n",
       "          -2.1112e-01, -7.7791e-01,  8.2479e-01, -7.0825e-01, -2.2962e-01,\n",
       "           1.4064e-01,  3.0398e-01, -1.1173e+00, -1.8641e-01,  8.2667e-01,\n",
       "           4.4993e-01, -1.1678e-01, -4.7856e-01, -9.4898e-01, -1.4977e-01,\n",
       "           1.9157e-01,  1.0528e-01,  6.6893e-01,  2.2070e-02,  5.2699e-01,\n",
       "          -1.9035e+00,  1.8775e-01,  2.6756e-01,  4.8347e-01, -2.8322e-01,\n",
       "           3.1586e+00,  4.4772e-01, -3.4549e-01, -3.1892e-01, -1.6631e-01,\n",
       "          -2.2289e-01,  2.6832e-01,  4.3670e-01, -2.0443e-01, -6.1761e-01,\n",
       "          -4.1918e-01, -2.2599e-01, -4.7885e-01, -4.0102e-01, -3.7106e-01,\n",
       "           2.4178e-01,  1.3510e-01, -8.1978e-01, -4.1543e-02, -3.0139e-01]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MyEmbeddingLayer(50, emb_dic)\n",
    "input = np.array([test1,test2])\n",
    "print(input.shape)\n",
    "#input = torch.from_numpy(input)\n",
    "test.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somtehing from week 4\n",
    "dim_emb = 50\n",
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, out_dimension, head_count=1):\n",
    "        super().__init__()\n",
    "        self.out_dimension = out_dimension\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb,emb_dic)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.v_net = nn.Linear(dim_emb, dim_emb)\n",
    "        #--------\n",
    "        self.mul_head_att = nn.MultiheadAttention(dim_emb,1)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_k = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_v = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        \n",
    "        self.ff1 = nn.Linear(in_features=dim_emb, out_features=128)\n",
    "        self.ff2 = nn.Linear(in_features=128, out_features=out_dimension)\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        #words = x.split(\" \")\n",
    "        #vectors = []\n",
    "        #for word in words:\n",
    "        #    print(word)\n",
    "        #    vectors.append(torch.Tensor(emb_dic[word.lower()]))\n",
    "        #features_final = self.l_1(x)\n",
    "        e_s = self.embedding(x)\n",
    "        q = self.lin_q(e_s)\n",
    "        k = self.lin_k(e_s)\n",
    "        v = self.lin_v(e_s)\n",
    "        a_k = self.get_A_Matrix(e_s, q)\n",
    "        h_ks = self.get_h_head_vectors(v,a_k,e_s)\n",
    "        h = self.combine_heads(h_ks)\n",
    "        \n",
    "        a_w = q.T @ self.activation_fn(self.v_net(h))\n",
    "        a_w = self.softmax(a_w)\n",
    "        \n",
    "        \n",
    "        attn_out, attn_weight = self.mul_head_att(q,k,v)\n",
    "        #print(attn_out)\n",
    "        cls_tok_emb = attn_out[0,:]\n",
    "        print(cls_tok_emb)\n",
    "        \n",
    "        output = self.ff1(cls_tok_emb)\n",
    "        output = self.activation_fn(output)\n",
    "        output = self.ff2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_A_Matrix(e,Q):\n",
    "        matr_mult = e.T @ Q.T @ e\n",
    "        a = softmax_dim1(matr_mult)\n",
    "        return a\n",
    "    \n",
    "    def get_h_head_vectors(V, a, e):\n",
    "        inner_dot = a @ e # TODO check if the dimensions really make dot product\n",
    "        h = V @ inner\n",
    "        return h\n",
    "    \n",
    "    def combine_heads(h_ks):\n",
    "        return h_ks # TODO change this when I will have multi head\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somtehing from week 4\n",
    "dim_emb = 50\n",
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, out_dimension, head_count=1):\n",
    "        super().__init__()\n",
    "        self.out_dimension = out_dimension\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb,emb_dic)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1) # TODO do I go for the correct dimension?\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #--------\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=1)\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=dim_emb, bias=False)\n",
    "        self.lin_vw = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        e_s = self.embedding(x)\n",
    "        \n",
    "        # starting single head\n",
    "        qe = self.lin_qk(e_s)\n",
    "        et_qt = qe @ e_s.transpose(-2,-1)\n",
    "        ak = self.softmax_dim1(et_qt)\n",
    "        print((ak @ e_s).shape)\n",
    "        hk = self.lin_vk(ak @ e_s)\n",
    "        h = hk # TODO make it multi head\n",
    "        print('h shape', h.shape)\n",
    "        # end of single head\n",
    "        \n",
    "        aw = self.lin_q(self.activation_fn(self.lin_vw(h)))\n",
    "        print('AW',aw.shape)\n",
    "        aw = self.softmax(aw)\n",
    "        print(aw.shape, h.shape)\n",
    "        r = aw.transpose(-2,-1) @ h # just confirmed by calculation that this seems correct\n",
    "        return r.reshape(-1,50)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 50\n",
    "\n",
    "class SelfAttHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_out):\n",
    "        super().__init__()\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1) # TODO do I go for the correct dimension?\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=head_out, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        qe = self.lin_qk(x)\n",
    "        et_qt = qe @ x.transpose(-2,-1)\n",
    "        ak = self.softmax_dim1(et_qt)\n",
    "        hk = self.lin_vk(ak @ x)\n",
    "        return hk\n",
    "    \n",
    "    \n",
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_count=1):\n",
    "        super().__init__()\n",
    "        assert embedding_dimension % head_count == 0, \"embeding must be divisible by heads\"\n",
    "        self.head_out = embedding_dimension // head_count\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb,emb_dic)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #--------\n",
    "        self.lin_vw = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=1, bias=False)\n",
    "        \n",
    "        self.selfAtt = nn.ModuleList([SelfAttHead(dim_emb, self.head_out) for i in range(head_count)])\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        e_s = self.embedding(x)\n",
    "        \n",
    "        hk = []\n",
    "        for head in self.selfAtt:\n",
    "            att = head(e_s)\n",
    "            hk.append(att)\n",
    "        h = torch.cat(hk, 2)\n",
    "        print(h.shape)\n",
    "        \n",
    "        aw = self.lin_q(self.activation_fn(self.lin_vw(h)))\n",
    "        aw = self.softmax(aw)\n",
    "        r = aw.transpose(-2,-1) @ h # just confirmed by calculation that this seems correct\n",
    "        return r.reshape(-1,50)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0717,  0.0292,  0.1425,  0.1842, -0.5162,  0.0396,  0.5044, -0.0452,\n",
       "          0.0755, -0.3381, -0.8201,  0.5778,  0.4399, -0.1889,  0.5134,  0.2353,\n",
       "          0.4658, -0.1613, -0.2020,  0.2898,  0.1932, -0.3018, -0.3265, -0.1332,\n",
       "         -0.3535, -0.3792, -0.2867, -0.3376, -0.2533,  0.3144,  0.0669, -0.3916,\n",
       "         -0.4766, -0.0251,  0.4266, -0.0782, -0.4342, -0.3937, -0.2354,  0.1222,\n",
       "         -0.2173,  0.1128,  0.3991,  0.0846,  0.6094, -0.1019,  0.1692,  0.0449,\n",
       "         -0.2952,  0.0126],\n",
       "        [-0.0104,  0.1340,  0.2028,  0.0213, -0.2697, -0.0581,  0.1869, -0.0831,\n",
       "         -0.0571, -0.3982, -0.8542,  0.3998,  0.4479, -0.0935,  0.4947,  0.0428,\n",
       "          0.4897,  0.1213, -0.4090,  0.1667,  0.2823, -0.4390, -0.3521, -0.0259,\n",
       "         -0.5940, -0.3555, -0.4330, -0.0341, -0.1675,  0.2652, -0.1117, -0.1740,\n",
       "         -0.6428, -0.1229,  0.5833,  0.1539, -0.1460, -0.4751, -0.0909, -0.0585,\n",
       "         -0.2951,  0.2540,  0.3135,  0.1013,  0.6803, -0.0254, -0.0519,  0.2482,\n",
       "         -0.1028, -0.1600]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNewsEncoder(dim_emb, head_count=10)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.L1Loss()\n",
    "inp = torch.randn(3,5, requires_grad=True)\n",
    "target = torch.randn(3,5)\n",
    "output = loss(inp, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.L1Loss()\n",
    "output_tmp = model(input)\n",
    "target = torch.randn(2,50)\n",
    "fake_loss = loss(output_tmp, target)\n",
    "\n",
    "fake_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 27693 + 1\n",
    "MAX_WORDS = 30\n",
    "class MyEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim, emb_dic):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_dic = emb_dic\n",
    "        self.emb_torch = nn.Embedding(word_count, emb_dim)\n",
    "        self.dummy_emb = dummy_dictionary_embedding\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten()\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split(\" \")\n",
    "            for word in words:\n",
    "                #print('inside: ', word.lower(), ' -> ', self.dummy_emb[word.lower()])\n",
    "                output.append(torch.IntTensor([self.dummy_emb[word.lower()]]))\n",
    "            for i in range(30 - len(words)):\n",
    "                output.append(torch.IntTensor([word_count-1]))\n",
    "        \n",
    "        #print(output[0].shape, output[1].shape)\n",
    "        output = torch.stack(output)\n",
    "        output = self.emb_torch(output)\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 50\n",
    "\n",
    "class SelfAttHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_out):\n",
    "        super().__init__()\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1) # TODO do I go for the correct dimension?\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=head_out, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        qe = self.lin_qk(x)\n",
    "        et_qt = qe @ x.transpose(-2,-1)\n",
    "        ak = self.softmax_dim1(et_qt)\n",
    "        hk = self.lin_vk(ak @ x)\n",
    "        return hk\n",
    "    \n",
    "class MultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self,embedding_dimension, head_count):\n",
    "        super().__init__()\n",
    "        self.head_out = embedding_dimension // head_count # TODO this will be later more specific\n",
    "        self.selfAtt = nn.ModuleList([SelfAttHead(dim_emb, self.head_out) for _ in range(head_count)])\n",
    "        \n",
    "    def forward(self, e_s):\n",
    "        hk = []\n",
    "        for head in self.selfAtt:\n",
    "            att = head(e_s)\n",
    "            hk.append(att)\n",
    "        h = torch.cat(hk, 2)\n",
    "        return h\n",
    "\n",
    "class AdditiveWordAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, additive_vector_dim):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.lin_vw = nn.Linear(in_features=embedding_dimension, out_features=additive_vector_dim)\n",
    "        self.lin_q = nn.Linear(in_features=additive_vector_dim, out_features=1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        aw = self.lin_q(self.activation_fn(self.lin_vw(h)))\n",
    "        aw = self.softmax(aw)\n",
    "        r = aw.transpose(-2,-1) @ h\n",
    "        return r\n",
    "    \n",
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_count=1):\n",
    "        super().__init__()\n",
    "        assert embedding_dimension % head_count == 0, \"embeding must be divisible by heads\"\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb,emb_dic)\n",
    "        self.mult_head_att = MultiHeadSelfAttHead(embedding_dimension, head_count)\n",
    "        self.add_word_att = AdditiveWordAttention(embedding_dimension, embedding_dimension) # TODO later change the vector dim to 200\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        e_s = self.embedding(x)\n",
    "        \n",
    "        h = self.mult_head_att(e_s)\n",
    "        \n",
    "        r = self.add_word_att(h)\n",
    "        return r.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vesrion 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, emb_dimension, user_head_count=1, news_head_count=1):\n",
    "        super().__init__()\n",
    "        self.news_encoder = MyNewsEncoder(emb_dimension, news_head_count)\n",
    "        self.multi_head_att = MultiHeadSelfAttHead(emb_dimension, head_count)\n",
    "        self.add_news_att = AdditiveWordAttention(emb_dimension,emb_dimension)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        r = self.news_encoder(x)\n",
    "        \n",
    "        h = self.multi_head_att(r)\n",
    "        \n",
    "        u = self.add_news_att(h)\n",
    "        \n",
    "        return u.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk_input = np.array([\"Natascha var ikke den første\", \"Kun Star Wars tjente mere\", \"Luderne flytter på landet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.L1Loss()\n",
    "target = torch.randn(3,50)\n",
    "model = MyNewsEncoder(dim_emb, head_count=10)\n",
    "output_tmp = model(dk_input)\n",
    "tmp_test_loss = loss(output_tmp, target)\n",
    "tmp_test_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5886, 0.1133, 0.6954],\n",
       "         [0.7556, 0.0775, 0.5810]]),\n",
       " tensor([1.3973, 1.4141]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplemat = torch.rand(2,3)\n",
    "simplemat, simplemat.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### laoding input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_behavior = pq.read_table('ebnerd_small/train/behaviors.parquet')\n",
    "table_history = pq.read_table('ebnerd_small/train/history.parquet')\n",
    "table_articles = pq.read_table('ebnerd_small/articles.parquet')\n",
    "df_behavior = table_behavior.to_pandas()\n",
    "df_history = table_history.to_pandas()\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>last_modified_time</th>\n",
       "      <th>premium</th>\n",
       "      <th>body</th>\n",
       "      <th>published_time</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>article_type</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>entity_groups</th>\n",
       "      <th>topics</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>category_str</th>\n",
       "      <th>total_inviews</th>\n",
       "      <th>total_pageviews</th>\n",
       "      <th>total_read_time</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3001353</td>\n",
       "      <td>Natascha var ikke den første</td>\n",
       "      <td>Politiet frygter nu, at Nataschas bortfører ha...</td>\n",
       "      <td>2023-06-29 06:20:33</td>\n",
       "      <td>False</td>\n",
       "      <td>Sagen om den østriske Natascha og hendes bortf...</td>\n",
       "      <td>2006-08-31 08:06:45</td>\n",
       "      <td>[3150850]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/krimi/article3001353.ece</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kriminalitet, Personfarlig kriminalitet]</td>\n",
       "      <td>140</td>\n",
       "      <td>[]</td>\n",
       "      <td>krimi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3003065</td>\n",
       "      <td>Kun Star Wars tjente mere</td>\n",
       "      <td>Biografgængerne strømmer ind for at se 'Da Vin...</td>\n",
       "      <td>2023-06-29 06:20:35</td>\n",
       "      <td>False</td>\n",
       "      <td>Vatikanet har opfordret til at boykotte filmen...</td>\n",
       "      <td>2006-05-21 16:57:00</td>\n",
       "      <td>[3006712]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/underholdning/filmogtv...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Underholdning, Film og tv, Økonomi]</td>\n",
       "      <td>414</td>\n",
       "      <td>[433, 434]</td>\n",
       "      <td>underholdning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8460</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3012771</td>\n",
       "      <td>Morten Bruun fyret i SønderjyskE</td>\n",
       "      <td>FODBOLD: Morten Bruun fyret med øjeblikkelig v...</td>\n",
       "      <td>2023-06-29 06:20:39</td>\n",
       "      <td>False</td>\n",
       "      <td>Kemien mellem spillerne i Superligaklubben Søn...</td>\n",
       "      <td>2006-05-01 14:28:40</td>\n",
       "      <td>[3177953]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/sport/fodbold/dansk_fo...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Erhverv, Kendt, Sport, Fodbold, Ansættelsesfo...</td>\n",
       "      <td>142</td>\n",
       "      <td>[196, 199]</td>\n",
       "      <td>sport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3023463</td>\n",
       "      <td>Luderne flytter på landet</td>\n",
       "      <td>I landets tyndest befolkede områder skyder bor...</td>\n",
       "      <td>2023-06-29 06:20:43</td>\n",
       "      <td>False</td>\n",
       "      <td>Det frække erhverv rykker på landet. I den tyn...</td>\n",
       "      <td>2007-03-24 08:27:59</td>\n",
       "      <td>[3184029]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/nyheder/samfund/articl...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Livsstil, Erotik]</td>\n",
       "      <td>118</td>\n",
       "      <td>[133]</td>\n",
       "      <td>nyheder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032577</td>\n",
       "      <td>Cybersex: Hvornår er man utro?</td>\n",
       "      <td>En flirtende sms til den flotte fyr i regnskab...</td>\n",
       "      <td>2023-06-29 06:20:46</td>\n",
       "      <td>False</td>\n",
       "      <td>De fleste af os mener, at et tungekys er utros...</td>\n",
       "      <td>2007-01-18 10:30:37</td>\n",
       "      <td>[3030463]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/sex_og_samliv/article3...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Livsstil, Partnerskab]</td>\n",
       "      <td>565</td>\n",
       "      <td>[]</td>\n",
       "      <td>sex_og_samliv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3033563</td>\n",
       "      <td>Kniven for struben-vært får selv kniven</td>\n",
       "      <td>I aftenens udgave af 'Med kniven for struben' ...</td>\n",
       "      <td>2023-06-29 06:20:47</td>\n",
       "      <td>False</td>\n",
       "      <td>Når man ser fjerde program i TV3s nye programs...</td>\n",
       "      <td>2007-03-27 10:22:08</td>\n",
       "      <td>[3005524, 3005525]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/underholdning/filmogtv...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Livsstil, Underholdning, Film og tv, Kultur, ...</td>\n",
       "      <td>414</td>\n",
       "      <td>[433, 436]</td>\n",
       "      <td>underholdning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3034608</td>\n",
       "      <td>Willy Strube har begået selvmord</td>\n",
       "      <td>Den tidligere SiD-chef tog sit eget liv, efter...</td>\n",
       "      <td>2023-06-29 06:20:49</td>\n",
       "      <td>False</td>\n",
       "      <td>Den tidligere formand for Industrigruppen i Si...</td>\n",
       "      <td>2001-10-19 12:30:00</td>\n",
       "      <td>[3204848]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/nyheder/politik/articl...</td>\n",
       "      <td>...</td>\n",
       "      <td>[PER, PER, PER]</td>\n",
       "      <td>[Kriminalitet, Erhverv, Privat virksomhed, Ans...</td>\n",
       "      <td>118</td>\n",
       "      <td>[130]</td>\n",
       "      <td>nyheder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3034849</td>\n",
       "      <td>Venner for livet</td>\n",
       "      <td>VK-REGERINGEN</td>\n",
       "      <td>2023-06-29 06:20:50</td>\n",
       "      <td>False</td>\n",
       "      <td>VK-REGERINGEN\\nhåndplukkede Bjørn Lomborg som ...</td>\n",
       "      <td>2003-01-09 06:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/incoming/article303484...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kendt, Politik, National politik]</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>incoming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3035648</td>\n",
       "      <td>Dronning af escort-branchen</td>\n",
       "      <td>Trine Michelsen hjælper københavnsk bagmand, s...</td>\n",
       "      <td>2023-06-29 06:20:52</td>\n",
       "      <td>False</td>\n",
       "      <td>En af escortbranchens største bagmandvirksomhe...</td>\n",
       "      <td>2003-06-17 07:10:00</td>\n",
       "      <td>[3082573]</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/krimi/article3035648.ece</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Erhverv, Livsstil, Transportmiddel, Erotik]</td>\n",
       "      <td>140</td>\n",
       "      <td>[]</td>\n",
       "      <td>krimi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3036444</td>\n",
       "      <td>Mia kendte sandsynligvis sin morder</td>\n",
       "      <td>Hun var ikke den type, der søgte kontakt til f...</td>\n",
       "      <td>2023-06-29 06:20:54</td>\n",
       "      <td>False</td>\n",
       "      <td>Den 12-årige Mia Teglgaard Sprotte fra Benløse...</td>\n",
       "      <td>2003-07-13 19:50:00</td>\n",
       "      <td>None</td>\n",
       "      <td>article_default</td>\n",
       "      <td>https://ekstrabladet.dk/krimi/article3036444.ece</td>\n",
       "      <td>...</td>\n",
       "      <td>[PER, PER, PER, PER]</td>\n",
       "      <td>[Kriminalitet, Personfarlig kriminalitet]</td>\n",
       "      <td>140</td>\n",
       "      <td>[]</td>\n",
       "      <td>krimi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                    title  \\\n",
       "0     3001353             Natascha var ikke den første   \n",
       "1     3003065                Kun Star Wars tjente mere   \n",
       "2     3012771         Morten Bruun fyret i SønderjyskE   \n",
       "3     3023463                Luderne flytter på landet   \n",
       "4     3032577           Cybersex: Hvornår er man utro?   \n",
       "5     3033563  Kniven for struben-vært får selv kniven   \n",
       "6     3034608         Willy Strube har begået selvmord   \n",
       "7     3034849                         Venner for livet   \n",
       "8     3035648              Dronning af escort-branchen   \n",
       "9     3036444      Mia kendte sandsynligvis sin morder   \n",
       "\n",
       "                                            subtitle  last_modified_time  \\\n",
       "0  Politiet frygter nu, at Nataschas bortfører ha... 2023-06-29 06:20:33   \n",
       "1  Biografgængerne strømmer ind for at se 'Da Vin... 2023-06-29 06:20:35   \n",
       "2  FODBOLD: Morten Bruun fyret med øjeblikkelig v... 2023-06-29 06:20:39   \n",
       "3  I landets tyndest befolkede områder skyder bor... 2023-06-29 06:20:43   \n",
       "4  En flirtende sms til den flotte fyr i regnskab... 2023-06-29 06:20:46   \n",
       "5  I aftenens udgave af 'Med kniven for struben' ... 2023-06-29 06:20:47   \n",
       "6  Den tidligere SiD-chef tog sit eget liv, efter... 2023-06-29 06:20:49   \n",
       "7                                      VK-REGERINGEN 2023-06-29 06:20:50   \n",
       "8  Trine Michelsen hjælper københavnsk bagmand, s... 2023-06-29 06:20:52   \n",
       "9  Hun var ikke den type, der søgte kontakt til f... 2023-06-29 06:20:54   \n",
       "\n",
       "   premium                                               body  \\\n",
       "0    False  Sagen om den østriske Natascha og hendes bortf...   \n",
       "1    False  Vatikanet har opfordret til at boykotte filmen...   \n",
       "2    False  Kemien mellem spillerne i Superligaklubben Søn...   \n",
       "3    False  Det frække erhverv rykker på landet. I den tyn...   \n",
       "4    False  De fleste af os mener, at et tungekys er utros...   \n",
       "5    False  Når man ser fjerde program i TV3s nye programs...   \n",
       "6    False  Den tidligere formand for Industrigruppen i Si...   \n",
       "7    False  VK-REGERINGEN\\nhåndplukkede Bjørn Lomborg som ...   \n",
       "8    False  En af escortbranchens største bagmandvirksomhe...   \n",
       "9    False  Den 12-årige Mia Teglgaard Sprotte fra Benløse...   \n",
       "\n",
       "       published_time           image_ids     article_type  \\\n",
       "0 2006-08-31 08:06:45           [3150850]  article_default   \n",
       "1 2006-05-21 16:57:00           [3006712]  article_default   \n",
       "2 2006-05-01 14:28:40           [3177953]  article_default   \n",
       "3 2007-03-24 08:27:59           [3184029]  article_default   \n",
       "4 2007-01-18 10:30:37           [3030463]  article_default   \n",
       "5 2007-03-27 10:22:08  [3005524, 3005525]  article_default   \n",
       "6 2001-10-19 12:30:00           [3204848]  article_default   \n",
       "7 2003-01-09 06:00:00                None  article_default   \n",
       "8 2003-06-17 07:10:00           [3082573]  article_default   \n",
       "9 2003-07-13 19:50:00                None  article_default   \n",
       "\n",
       "                                                 url  ...  \\\n",
       "0   https://ekstrabladet.dk/krimi/article3001353.ece  ...   \n",
       "1  https://ekstrabladet.dk/underholdning/filmogtv...  ...   \n",
       "2  https://ekstrabladet.dk/sport/fodbold/dansk_fo...  ...   \n",
       "3  https://ekstrabladet.dk/nyheder/samfund/articl...  ...   \n",
       "4  https://ekstrabladet.dk/sex_og_samliv/article3...  ...   \n",
       "5  https://ekstrabladet.dk/underholdning/filmogtv...  ...   \n",
       "6  https://ekstrabladet.dk/nyheder/politik/articl...  ...   \n",
       "7  https://ekstrabladet.dk/incoming/article303484...  ...   \n",
       "8   https://ekstrabladet.dk/krimi/article3035648.ece  ...   \n",
       "9   https://ekstrabladet.dk/krimi/article3036444.ece  ...   \n",
       "\n",
       "          entity_groups                                             topics  \\\n",
       "0                    []          [Kriminalitet, Personfarlig kriminalitet]   \n",
       "1                    []               [Underholdning, Film og tv, Økonomi]   \n",
       "2                    []  [Erhverv, Kendt, Sport, Fodbold, Ansættelsesfo...   \n",
       "3                    []                                 [Livsstil, Erotik]   \n",
       "4                    []                            [Livsstil, Partnerskab]   \n",
       "5                    []  [Livsstil, Underholdning, Film og tv, Kultur, ...   \n",
       "6       [PER, PER, PER]  [Kriminalitet, Erhverv, Privat virksomhed, Ans...   \n",
       "7                    []                 [Kendt, Politik, National politik]   \n",
       "8                    []       [Erhverv, Livsstil, Transportmiddel, Erotik]   \n",
       "9  [PER, PER, PER, PER]          [Kriminalitet, Personfarlig kriminalitet]   \n",
       "\n",
       "  category  subcategory   category_str total_inviews  total_pageviews  \\\n",
       "0      140           []          krimi           NaN              NaN   \n",
       "1      414   [433, 434]  underholdning           NaN              NaN   \n",
       "2      142   [196, 199]          sport           NaN              NaN   \n",
       "3      118        [133]        nyheder           NaN              NaN   \n",
       "4      565           []  sex_og_samliv           NaN              NaN   \n",
       "5      414   [433, 436]  underholdning           NaN              NaN   \n",
       "6      118        [130]        nyheder           NaN              NaN   \n",
       "7        2           []       incoming           NaN              NaN   \n",
       "8      140           []          krimi           NaN              NaN   \n",
       "9      140           []          krimi           NaN              NaN   \n",
       "\n",
       "   total_read_time  sentiment_score  sentiment_label  \n",
       "0              NaN           0.9955         Negative  \n",
       "1              NaN           0.8460         Positive  \n",
       "2              NaN           0.8241         Negative  \n",
       "3              NaN           0.7053          Neutral  \n",
       "4              NaN           0.9307          Neutral  \n",
       "5              NaN           0.9371          Neutral  \n",
       "6              NaN           0.9971         Negative  \n",
       "7              NaN           0.8454          Neutral  \n",
       "8              NaN           0.8814          Neutral  \n",
       "9              NaN           0.9752         Negative  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Natascha var ikke den første', 'Kun Star Wars tjente mere',\n",
       "       'Morten Bruun fyret i SønderjyskE', ...,\n",
       "       'Dansk skuespiller: - Jeg nægtede, at jeg var syg',\n",
       "       'Så slemt er det: 14.000 huse er oversvømmet',\n",
       "       'Aktion mod svindlere: Seks personer anholdt'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = df_articles['title'].values\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words = lambda tit : tit.split(' ')\n",
    "all_words = list(map(split_words, titles))\n",
    "words = []\n",
    "for title in all_words:\n",
    "    for word in title:\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = np.unique(np.sort(np.array(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27693,)\n"
     ]
    }
   ],
   "source": [
    "print(all_words.shape)\n",
    "dummy_dictionary_embedding = {}\n",
    "for i in range(len(all_words)):\n",
    "    dummy_dictionary_embedding[all_words[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df_articles['title'].values\n",
    "Y_all = df_articles['category_str'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auto': 0, 'bibliotek': 1, 'biler': 2, 'dagsorden': 3, 'ferie': 4, 'forbrug': 5, 'haandvaerkeren': 6, 'horoskoper': 7, 'incoming': 8, 'krimi': 9, 'musik': 10, 'nationen': 11, 'nyheder': 12, 'om_ekstra_bladet': 13, 'opinionen': 14, 'penge': 15, 'play': 16, 'plus': 17, 'podcast': 18, 'services': 19, 'sex_og_samliv': 20, 'side9': 21, 'sport': 22, 'underholdning': 23, 'video': 24, 'vin': 25}\n"
     ]
    }
   ],
   "source": [
    "categories = np.unique(df_articles['category_str'].values)\n",
    "cat_map = {}\n",
    "for i in range(len(categories)):\n",
    "    cat_map[categories[i]] = i\n",
    "print(cat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp_embd = nn.Embedding(26,dim_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_index = lambda a: cat_map[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all_index = torch.IntTensor(list(map(cat_to_index, Y_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all_vec = y_temp_embd(Y_all_index).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20738,), torch.Size([20738, 50]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape, Y_all_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(self.X_train[index], self.y_train[index])\n",
    "        return [self.X_train[index], self.y_train[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20738"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dk_input = np.array(['Natascha var ikke den første','Kun Star Wars tjente mere','Luderne flytter på landet'])\n",
    "target = torch.randn(3,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Natascha var ikke den første', np.str_('Natascha var ikke den første'))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_all), type(tmp_dk_input)\n",
    "X_all.size, tmp_dk_input.size\n",
    "X_all[0], tmp_dk_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.8113, -0.5601,  0.0688,  0.0433,  0.6322,  1.7011, -0.6115,  1.5072,\n",
       "         -0.5583, -1.2399,  0.4653, -2.5312, -0.8267,  0.1064,  0.8190,  0.8980,\n",
       "         -0.7079,  0.8811, -1.6164, -0.6474, -0.4017, -0.1428, -0.3780, -0.4416,\n",
       "          0.3583, -0.9101,  0.6233,  0.0609, -0.9289,  0.2133,  1.6290,  0.4116,\n",
       "          2.2968,  0.1557, -0.3821,  1.9859,  0.3232, -0.6153, -0.8077, -0.0276,\n",
       "          0.1847,  0.5487,  2.3812,  0.1613,  1.5597,  1.5464,  0.1718, -0.2927,\n",
       "          0.4616,  1.3618]),\n",
       " tensor([-0.8797, -0.2395, -0.9081, -2.0901, -0.1674,  0.0502,  1.0764,  0.5941,\n",
       "          0.4634,  1.4409, -0.6228, -1.3523,  1.4419,  1.3898,  1.1106, -0.0901,\n",
       "         -0.7134,  0.1914,  0.2988, -1.5603, -2.6037, -0.6166,  0.1844, -0.7520,\n",
       "          2.0305, -0.0928,  2.7589,  0.3643, -2.0165,  1.2678,  1.1598,  0.3436,\n",
       "          1.8406, -1.3440, -0.1751, -1.1208, -1.1207,  0.4806,  0.8751, -0.7550,\n",
       "          0.4259, -0.4042,  0.2590, -1.1447, -1.2505,  0.7114,  1.8315,  1.0575,\n",
       "         -1.1097,  1.5282]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_all_vec), type(target)\n",
    "Y_all_vec.size(), target.size()\n",
    "Y_all_vec[0], target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natascha var ikke den første\n"
     ]
    }
   ],
   "source": [
    "model = MyNewsEncoder(50)\n",
    "train_dataset = CustomDataset(X_all, Y_all_vec)#CustomDataset(tmp_dk_input,target)#CustomDataset(X_all, Y_all_vec)\n",
    "print(X_all[0])\n",
    "loss_fn = nn.L1Loss() #nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dk_input = ('Natascha var ikke den første','Kun Star Wars tjente mere','Luderne flytter på landet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40      training accuracy: 0.0, loss: 0.7235375642776489\n",
      "Step 80      training accuracy: 0.0, loss: 0.7236472368240356\n",
      "Step 120     training accuracy: 0.0, loss: 0.723263144493103\n",
      "Step 160     training accuracy: 0.0, loss: 0.7244390249252319\n",
      "Step 200     training accuracy: 0.0, loss: 0.7234910130500793\n",
      "Step 240     training accuracy: 0.0, loss: 0.7228919267654419\n",
      "Step 280     training accuracy: 0.0, loss: 0.7216987609863281\n",
      "Step 320     training accuracy: 0.0, loss: 0.7200679183006287\n",
      "Step 360     training accuracy: 0.0, loss: 0.7220596671104431\n",
      "Step 400     training accuracy: 0.0, loss: 0.7213143110275269\n",
      "Step 440     training accuracy: 0.0, loss: 0.720506489276886\n",
      "Step 480     training accuracy: 0.0, loss: 0.7168058753013611\n",
      "Step 520     training accuracy: 0.0, loss: 0.7228422164916992\n",
      "Step 560     training accuracy: 0.0, loss: 0.7214546203613281\n",
      "Step 600     training accuracy: 0.0, loss: 0.7274776697158813\n",
      "Step 640     training accuracy: 0.0, loss: 0.7192741632461548\n",
      "Step 680     training accuracy: 0.0, loss: 0.7201901078224182\n",
      "Step 720     training accuracy: 0.0, loss: 0.7206810712814331\n",
      "Step 760     training accuracy: 0.0, loss: 0.7201488614082336\n",
      "Step 800     training accuracy: 0.0, loss: 0.7215224504470825\n",
      "Step 840     training accuracy: 0.0, loss: 0.7184439897537231\n",
      "Step 880     training accuracy: 0.0, loss: 0.7210267782211304\n",
      "Step 920     training accuracy: 0.0, loss: 0.7197328805923462\n",
      "Step 960     training accuracy: 0.0, loss: 0.7184923887252808\n",
      "Step 1000    training accuracy: 0.0, loss: 0.7211952805519104\n",
      "Step 1040    training accuracy: 0.0, loss: 0.7203774452209473\n",
      "Step 1080    training accuracy: 0.0, loss: 0.7214381098747253\n",
      "Step 1120    training accuracy: 0.0, loss: 0.7207198143005371\n",
      "Step 1160    training accuracy: 0.0, loss: 0.7204465866088867\n",
      "Step 1200    training accuracy: 0.0, loss: 0.7185659408569336\n",
      "Step 1240    training accuracy: 0.0, loss: 0.713857889175415\n",
      "Step 1280    training accuracy: 0.0, loss: 0.7116648554801941\n",
      "Step 1320    training accuracy: 0.0, loss: 0.7125093340873718\n",
      "Step 1360    training accuracy: 0.0, loss: 0.7190819978713989\n",
      "Step 1400    training accuracy: 0.0, loss: 0.717048704624176\n",
      "Step 1440    training accuracy: 0.0, loss: 0.7139748334884644\n",
      "Step 1480    training accuracy: 0.0, loss: 0.7133141756057739\n",
      "Step 1520    training accuracy: 0.0, loss: 0.715736985206604\n",
      "Step 1560    training accuracy: 0.0, loss: 0.7114728689193726\n",
      "Step 1600    training accuracy: 0.0, loss: 0.7101146578788757\n",
      "Step 1640    training accuracy: 0.0, loss: 0.7127936482429504\n",
      "Step 1680    training accuracy: 0.0, loss: 0.7046146392822266\n",
      "Step 1720    training accuracy: 0.0, loss: 0.7035819292068481\n",
      "Step 1760    training accuracy: 0.0, loss: 0.7058426141738892\n",
      "Step 1800    training accuracy: 0.0, loss: 0.6980223655700684\n",
      "Step 1840    training accuracy: 0.0, loss: 0.6995596885681152\n",
      "Step 1880    training accuracy: 0.0, loss: 0.6941153407096863\n",
      "Step 1920    training accuracy: 0.000390625, loss: 0.6995005011558533\n",
      "Step 1960    training accuracy: 0.0015625, loss: 0.69294273853302\n",
      "Step 2000    training accuracy: 0.00390625, loss: 0.6923187971115112\n",
      "Step 2040    training accuracy: 0.005859375, loss: 0.686358630657196\n",
      "Step 2080    training accuracy: 0.0078125, loss: 0.6865497827529907\n",
      "Step 2120    training accuracy: 0.01171875, loss: 0.6877692341804504\n",
      "Step 2160    training accuracy: 0.009375, loss: 0.6753047108650208\n",
      "Step 2200    training accuracy: 0.0125, loss: 0.6788670420646667\n",
      "Step 2240    training accuracy: 0.008984375, loss: 0.6813293695449829\n",
      "Step 2280    training accuracy: 0.01875, loss: 0.6751538515090942\n",
      "Step 2320    training accuracy: 0.01640625, loss: 0.6720610857009888\n",
      "Step 2360    training accuracy: 0.0171875, loss: 0.6703464388847351\n",
      "Step 2400    training accuracy: 0.015625, loss: 0.6756550073623657\n",
      "Step 2440    training accuracy: 0.019140625, loss: 0.6671907305717468\n",
      "Step 2480    training accuracy: 0.01875, loss: 0.6719348430633545\n",
      "Step 2520    training accuracy: 0.027734375, loss: 0.6630243062973022\n",
      "Step 2560    training accuracy: 0.0203125, loss: 0.6656708717346191\n",
      "Step 2600    training accuracy: 0.01953125, loss: 0.6602757573127747\n",
      "Step 2640    training accuracy: 0.037109375, loss: 0.6562225222587585\n",
      "Step 2680    training accuracy: 0.030859375, loss: 0.6618939638137817\n",
      "Step 2720    training accuracy: 0.031640625, loss: 0.6599811315536499\n",
      "Step 2760    training accuracy: 0.030078125, loss: 0.6573160290718079\n",
      "Step 2800    training accuracy: 0.046484375, loss: 0.6544517874717712\n",
      "Step 2840    training accuracy: 0.0671875, loss: 0.6569913625717163\n",
      "Step 2880    training accuracy: 0.0875, loss: 0.6527975797653198\n",
      "Step 2920    training accuracy: 0.085546875, loss: 0.6590420007705688\n",
      "Step 2960    training accuracy: 0.08705357142857142, loss: 0.650820255279541\n",
      "Step 3000    training accuracy: 0.09921875, loss: 0.646173894405365\n",
      "Step 3040    training accuracy: 0.08671875, loss: 0.6559334993362427\n",
      "Step 3080    training accuracy: 0.083984375, loss: 0.65190190076828\n",
      "Step 3120    training accuracy: 0.09609375, loss: 0.6463634371757507\n",
      "Step 3160    training accuracy: 0.08984375, loss: 0.6523236036300659\n",
      "Step 3200    training accuracy: 0.098046875, loss: 0.648650050163269\n",
      "Step 3240    training accuracy: 0.08515625, loss: 0.652000904083252\n",
      "Step 3280    training accuracy: 0.08229166666666667, loss: 0.6502674221992493\n",
      "Step 3320    training accuracy: 0.094140625, loss: 0.6441460251808167\n",
      "Step 3360    training accuracy: 0.09140625, loss: 0.6489849090576172\n",
      "Step 3400    training accuracy: 0.09453125, loss: 0.6451056599617004\n",
      "Step 3440    training accuracy: 0.090234375, loss: 0.6470127105712891\n",
      "Step 3480    training accuracy: 0.0921875, loss: 0.6457022428512573\n",
      "Step 3520    training accuracy: 0.08984375, loss: 0.6440057754516602\n",
      "Step 3560    training accuracy: 0.1, loss: 0.6413151025772095\n",
      "Step 3600    training accuracy: 0.1, loss: 0.645220935344696\n",
      "Step 3640    training accuracy: 0.090625, loss: 0.6461571455001831\n",
      "Step 3680    training accuracy: 0.105859375, loss: 0.6381946802139282\n",
      "Step 3720    training accuracy: 0.09140625, loss: 0.6486024260520935\n",
      "Step 3760    training accuracy: 0.0859375, loss: 0.6492998003959656\n",
      "Step 3800    training accuracy: 0.10078125, loss: 0.6410146951675415\n",
      "Step 3840    training accuracy: 0.102734375, loss: 0.6377782225608826\n",
      "Step 3880    training accuracy: 0.102734375, loss: 0.6409251093864441\n",
      "Step 3920    training accuracy: 0.10234375, loss: 0.6326196193695068\n",
      "Step 3960    training accuracy: 0.099609375, loss: 0.6407888531684875\n",
      "Step 4000    training accuracy: 0.101953125, loss: 0.6364291906356812\n",
      "Step 4040    training accuracy: 0.096875, loss: 0.6420507431030273\n",
      "Step 4080    training accuracy: 0.104296875, loss: 0.6368971467018127\n",
      "Step 4120    training accuracy: 0.10078125, loss: 0.6400712728500366\n",
      "Step 4160    training accuracy: 0.095703125, loss: 0.6458374261856079\n",
      "Step 4200    training accuracy: 0.091796875, loss: 0.650148868560791\n",
      "Step 4240    training accuracy: 0.09166666666666666, loss: 0.6439865231513977\n",
      "Step 4280    training accuracy: 0.11015625, loss: 0.6370812654495239\n",
      "Step 4320    training accuracy: 0.100390625, loss: 0.6379188895225525\n",
      "Step 4360    training accuracy: 0.09921875, loss: 0.6385658383369446\n",
      "Step 4400    training accuracy: 0.10234375, loss: 0.6387494802474976\n",
      "Step 4440    training accuracy: 0.09765625, loss: 0.6448583602905273\n",
      "Step 4480    training accuracy: 0.096484375, loss: 0.6404020190238953\n",
      "Step 4520    training accuracy: 0.096875, loss: 0.6398338079452515\n",
      "Step 4560    training accuracy: 0.10625, loss: 0.6349430084228516\n",
      "Step 4600    training accuracy: 0.105078125, loss: 0.6349763870239258\n",
      "Step 4640    training accuracy: 0.100390625, loss: 0.6398480534553528\n",
      "Step 4680    training accuracy: 0.109375, loss: 0.6282682418823242\n",
      "Step 4720    training accuracy: 0.109765625, loss: 0.6329506635665894\n",
      "Step 4760    training accuracy: 0.095703125, loss: 0.6398569941520691\n",
      "Step 4800    training accuracy: 0.09296875, loss: 0.6462522745132446\n",
      "Step 4840    training accuracy: 0.08828125, loss: 0.649278998374939\n",
      "Step 4880    training accuracy: 0.065625, loss: 0.6639525294303894\n",
      "Step 4920    training accuracy: 0.10390625, loss: 0.635780930519104\n",
      "Step 4960    training accuracy: 0.10859375, loss: 0.6311063766479492\n",
      "Step 5000    training accuracy: 0.096875, loss: 0.6398360133171082\n",
      "Step 5040    training accuracy: 0.100390625, loss: 0.641090989112854\n",
      "Step 5080    training accuracy: 0.0953125, loss: 0.6392938494682312\n",
      "Step 5120    training accuracy: 0.109765625, loss: 0.6284767985343933\n",
      "Step 5160    training accuracy: 0.09921875, loss: 0.638516902923584\n",
      "Step 5200    training accuracy: 0.116015625, loss: 0.6300627589225769\n",
      "Step 5240    training accuracy: 0.090625, loss: 0.6457841396331787\n",
      "Step 5280    training accuracy: 0.100390625, loss: 0.6395766139030457\n",
      "Step 5320    training accuracy: 0.107421875, loss: 0.6326694488525391\n",
      "Step 5360    training accuracy: 0.1078125, loss: 0.6276026964187622\n",
      "Step 5400    training accuracy: 0.111328125, loss: 0.6351900100708008\n",
      "Step 5440    training accuracy: 0.097265625, loss: 0.6339635848999023\n",
      "Step 5480    training accuracy: 0.0984375, loss: 0.6392183303833008\n",
      "Step 5520    training accuracy: 0.112890625, loss: 0.6269634366035461\n",
      "Step 5560    training accuracy: 0.09196428571428572, loss: 0.6432477235794067\n",
      "Step 5600    training accuracy: 0.103125, loss: 0.6367006301879883\n",
      "Step 5640    training accuracy: 0.109375, loss: 0.6328290700912476\n",
      "Step 5680    training accuracy: 0.108984375, loss: 0.6327675580978394\n",
      "Step 5720    training accuracy: 0.10625, loss: 0.6315757036209106\n",
      "Step 5760    training accuracy: 0.09765625, loss: 0.6353840827941895\n",
      "Step 5800    training accuracy: 0.108984375, loss: 0.6282600164413452\n",
      "Step 5840    training accuracy: 0.105078125, loss: 0.6307485103607178\n",
      "Step 5880    training accuracy: 0.10364583333333334, loss: 0.6340574026107788\n",
      "Step 5920    training accuracy: 0.1171875, loss: 0.6298726201057434\n",
      "Step 5960    training accuracy: 0.10546875, loss: 0.6348649859428406\n",
      "Step 6000    training accuracy: 0.09921875, loss: 0.6374229788780212\n",
      "Step 6040    training accuracy: 0.103125, loss: 0.6334196925163269\n",
      "Step 6080    training accuracy: 0.11171875, loss: 0.6195338368415833\n",
      "Step 6120    training accuracy: 0.109765625, loss: 0.6291810274124146\n",
      "Step 6160    training accuracy: 0.0921875, loss: 0.6410500407218933\n",
      "Step 6200    training accuracy: 0.1025, loss: 0.6351802349090576\n",
      "Step 6240    training accuracy: 0.1109375, loss: 0.6272953748703003\n",
      "Step 6280    training accuracy: 0.098046875, loss: 0.6330785751342773\n",
      "Step 6320    training accuracy: 0.11484375, loss: 0.6287150979042053\n",
      "Step 6360    training accuracy: 0.10546875, loss: 0.629810094833374\n",
      "Step 6400    training accuracy: 0.102734375, loss: 0.6327897906303406\n",
      "Step 6440    training accuracy: 0.109375, loss: 0.6286101937294006\n",
      "Step 6480    training accuracy: 0.109375, loss: 0.6318882703781128\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "validation_every_steps = 40\n",
    "\n",
    "# Make data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "##validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "train_loss = []\n",
    "validation_accuracies = []\n",
    "validation_loss = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    train_loss_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:#[(tmp_dk_input, target)]:#train_loader:#[(dk_input, target)]:#train_loader:\n",
    "        #print(targets)\n",
    "        # Forward pass.\n",
    "        #print('broken',inputs)\n",
    "        #print('working',target)\n",
    "        output = model(np.array(inputs))#model(np.array(tuple(dk_input)))#model(np.array(inputs))\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_fn(output, targets)\n",
    "        train_loss_batches.append(loss.detach().numpy())\n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        targ_ind = targets.max(1)[1]\n",
    "        #print('out:', output)\n",
    "        #print('predictions:', predictions)\n",
    "        #print('targets:', targ_ind)\n",
    "        train_accuracies_batches.append(accuracy_score(targ_ind, predictions))\n",
    "        \n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            train_loss.append(np.mean(train_loss_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "            train_loss_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            # validation_accuracies_batches = []\n",
    "            # with torch.no_grad():\n",
    "            #     model.eval()\n",
    "            #     for inputs, targets in validation_loader:\n",
    "            #         output = model(inputs)\n",
    "            #         loss = loss_fn(output, targets.float())\n",
    "\n",
    "            #         predictions = output.max(1)[1]\n",
    "            #         targ_ind = targets.max(1)[1]\n",
    "                    \n",
    "            #         # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "            #         validation_accuracies_batches.append(accuracy_score(targ_ind, predictions) * len(inputs))\n",
    "\n",
    "            #     model.train()\n",
    "                \n",
    "            # # Append average validation accuracy to list.\n",
    "            # validation_accuracies.append(np.sum(validation_accuracies_batches) / len(validation_dataset))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}, loss: {train_loss[-1]}\")\n",
    "            #print(f\"             validation accuracy: {validation_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoing_this = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_this_input, testing_target = testoing_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_this_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Accumulate the loss\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()  \u001b[38;5;66;03m# Backpropagate the accumulated loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update model weights\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "model = MyNewsEncoder(50)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()  # Clean up gradients from the model\n",
    "        \n",
    "        output = model(np.array(inputs))  # Forward pass\n",
    "        loss = loss_fn(output, target)  # Compute the loss\n",
    "        print(loss.shape)\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    total_loss.backward()  # Backpropagate the accumulated loss\n",
    "    optimizer.step()  # Update model weights\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Total Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(target,labels)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate gradients \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Update Weights\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    data = X_all \n",
    "    labels = Y_all_vec\n",
    "    for i in range(1):        \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        target = model(data)\n",
    "        # Find the Loss\n",
    "        loss = loss_fn(target,labels)\n",
    "        # Calculate gradients \n",
    "        loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        # Calculate Loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(trainloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import transformers\n",
    "import tokenizers\n",
    "import datasets\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "task_name = f\"translation_da_to_en\"\n",
    "model_name = f\"Helsinki-NLP/opus-mt-da-en\"\n",
    "translator  = transformers.pipeline(task_name, model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Natascha wasn't the first.\"},\n",
       " {'translation_text': 'Only Star Wars made more'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(['Natascha var ikke den første','Kun Star Wars tjente mere'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_articles = pq.read_table('ebnerd_small/articles.parquet')\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3792,  0.6185,  0.9593,  0.9040,  0.3681,  0.0230,  0.1680, -1.5309,\n",
       "        -0.0605, -0.2500,  0.1503,  0.3197, -0.6891, -0.7863, -0.0158,  0.5053,\n",
       "        -0.8473, -0.1235,  0.0785, -0.9602, -0.5431, -0.3352,  0.3893,  0.1946,\n",
       "        -1.1688, -0.8661, -0.3918,  0.2418,  0.3286, -0.7875,  2.4884,  0.7102,\n",
       "        -0.5311,  0.8959, -0.2305, -0.8202,  0.3442, -0.9687, -0.1514, -0.4491,\n",
       "         0.8951, -0.0166, -0.2749,  0.2795,  0.7794, -0.3194,  0.1676, -0.6252,\n",
       "         0.0533,  0.6202])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dic['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20738, 21)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# links\n",
    "https://sanjayasubedi.com.np/deeplearning/multihead-attention-from-scratch/\n",
    "\n",
    "https://github.com/ebanalyse/ebnerd-benchmark/blob/67a352b73306bd03aeff824f2451867314a26414/examples/00_quick_start/nrms_ebnerd.ipynb#L238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\u001b[38;5;66;03m#, weight_decay=0.3)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)#, weight_decay=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data\n",
    "out = model([])\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")\n",
    "print(f\"Output probabilities:\\n{out.softmax(1).detach().cpu().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
