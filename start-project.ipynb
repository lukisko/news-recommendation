{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with loading the glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#import transformers\n",
    "#import tokenizers\n",
    "#import datasets\n",
    "#import zipfile\n",
    "#from huggingface_hub import hf_hub_download\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400001/400001 [00:08<00:00, 44765.08it/s]\n"
     ]
    }
   ],
   "source": [
    "target_file = \"glove.6B.50d.txt\"\n",
    "vocabulary = []\n",
    "vectors = []\n",
    "emb_dic = {}\n",
    "with open(target_file, \"r\", encoding=\"utf8\") as f:\n",
    "    for l in tqdm(f.readlines() ):\n",
    "        word, *vector = l.split()\n",
    "        vocabulary.append(word)\n",
    "        vector_t = torch.tensor([float(v) for v in vector])\n",
    "        vectors.append(vector_t)\n",
    "        emb_dic[word] = vector_t\n",
    "vectors = torch.stack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somtehing from week 4\n",
    "dim_emb = 50\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.mul_head_att = nn.MultiheadAttention(dim_emb,1)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_k = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_v = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        \n",
    "        self.ff1 = nn.Linear(in_features=dim_emb, out_features=128)\n",
    "        self.ff2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "        #self.l_1 = nn.Linear(in_features=int(features_cat_size),\n",
    "        #                  out_features=l_1_hidden_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        words = x.split(\" \")\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            print(word)\n",
    "            vectors.append(torch.Tensor(emb_dic[word.lower()]))\n",
    "        #features_final = self.l_1(x)\n",
    "        e_s = torch.stack(vectors)\n",
    "        q = self.lin_q(e_s)\n",
    "        k = self.lin_k(e_s)\n",
    "        v = self.lin_v(e_s)\n",
    "        attn_out, attn_weight = self.mul_head_att(q,k,v)\n",
    "        #print(attn_out)\n",
    "        cls_tok_emb = attn_out[0,:]\n",
    "        print(cls_tok_emb)\n",
    "        \n",
    "        output = self.ff1(cls_tok_emb)\n",
    "        output = self.activation_fn(output)\n",
    "        output = self.ff2(output)\n",
    "        \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NewsEncoder(5) # TODO - I need to change this with a number that will correspond to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewsEncoder(\n",
       "  (activation_fn): ReLU()\n",
       "  (mul_head_att): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "  )\n",
       "  (lin_q): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (lin_k): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (lin_v): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (ff1): Linear(in_features=50, out_features=128, bias=True)\n",
       "  (ff2): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"Breaking news about this\"\n",
    "test2 = \"This has happened again\"\n",
    "#test3 = \"Natascha var ikke den foerste\"\n",
    "#test4 = \"Kun Star Wars tjente mere\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "has\n",
      "happened\n",
      "again\n",
      "tensor([ 0.0623, -0.1269,  0.1517,  0.0182, -0.0764, -0.1888,  0.0502,  0.1932,\n",
      "        -0.0752, -0.2148,  0.0839, -0.0535,  0.0456, -0.0375,  0.2603, -0.1023,\n",
      "        -0.4326, -0.0162, -0.2151,  0.2617, -0.4282,  0.0268,  0.0388, -0.2200,\n",
      "        -0.0063,  0.1313,  0.3021, -0.2832,  0.1869,  0.2080, -0.0155, -0.3546,\n",
      "         0.0543,  0.0897, -0.0034,  0.3228, -0.1696, -0.0180,  0.0544,  0.0395,\n",
      "        -0.2224,  0.0550, -0.1932,  0.0877,  0.2250, -0.0268, -0.0923, -0.1682,\n",
      "        -0.1431, -0.4634], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0823,  0.0722,  0.0352,  0.0860, -0.0197], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output = model.forward(test2)\n",
    "shape = attn_output.shape\n",
    "attn_output.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brand new encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,emb_dim, emb_dic):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_dic = emb_dic\n",
    "        \n",
    "    def forward(self,text):\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten()\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split(\" \")\n",
    "            for word in words:\n",
    "                output.append(torch.Tensor(self.emb_dic[word.lower()]))\n",
    "            \n",
    "        output = torch.stack(output)\n",
    "        output = output.reshape(input_shape + (-1,self.emb_dim))\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.5307e-01, -1.4580e-02, -7.5133e-02,  1.0766e-01, -1.1661e-02,\n",
       "           4.1505e-01,  7.2856e-02,  4.9378e-01,  1.4302e-01,  2.7014e-01,\n",
       "          -2.9928e-01, -4.9729e-01, -1.1358e+00,  5.7973e-01,  5.5898e-01,\n",
       "          -3.1587e-01, -5.7323e-02, -7.2313e-01, -4.0444e-01, -2.3284e-01,\n",
       "           1.8744e-01,  3.2356e-01, -1.6800e-02, -3.3481e-01,  2.2256e-01,\n",
       "          -1.3342e+00,  4.9411e-01,  8.8150e-02,  8.0475e-01, -1.7204e-01,\n",
       "           2.7073e+00,  1.2706e-01,  9.3040e-02,  3.3197e-01, -7.0747e-01,\n",
       "           2.3589e-01,  3.0587e-01,  1.6879e-01, -1.8252e-01, -1.2134e-01,\n",
       "          -2.3045e-01, -1.5842e-01,  5.3029e-02,  4.9780e-01,  1.1690e-01,\n",
       "           4.4732e-02,  2.3904e-01, -4.5940e-01, -4.5749e-01, -6.2330e-01],\n",
       "         [-2.0825e-01,  4.7786e-01,  5.2196e-01,  1.0587e+00, -1.0045e-01,\n",
       "          -1.1269e+00, -1.2581e+00, -1.1041e-01, -7.4125e-02, -7.7976e-01,\n",
       "          -3.7942e-01, -2.4860e-01, -3.9224e-01,  4.2972e-01,  9.8060e-01,\n",
       "           1.2668e-01, -1.3772e+00, -2.2793e-01, -1.8497e-01,  4.1014e-01,\n",
       "           9.6781e-01,  8.9160e-01,  8.4685e-01,  5.7416e-01,  4.6455e-01,\n",
       "          -1.7287e+00, -6.3918e-01,  5.6256e-01, -1.2651e-01,  4.9711e-01,\n",
       "           3.3326e+00,  3.4399e-02,  4.6149e-01, -4.4826e-01, -1.1945e+00,\n",
       "          -4.7593e-01, -3.1927e-01, -6.4420e-01,  8.9735e-02,  7.3952e-02,\n",
       "           7.0755e-01,  5.2948e-01, -1.2034e-01, -4.6779e-01,  2.4722e-01,\n",
       "           2.8045e-01, -6.2632e-01,  1.4458e+00,  5.1045e-01,  7.4156e-01],\n",
       "         [ 8.9466e-01,  3.6604e-01,  3.7588e-01, -4.1818e-01,  5.8462e-01,\n",
       "           1.8594e-01, -4.1907e-01, -4.6621e-01, -5.4903e-01,  2.4770e-02,\n",
       "          -9.0816e-01, -4.8271e-01, -5.0742e-02, -7.4039e-01,  1.4377e+00,\n",
       "          -1.9740e-02, -2.3840e-01,  4.3154e-01, -6.6120e-01, -4.1275e-01,\n",
       "           2.5475e-01,  9.3498e-01,  8.1404e-01, -1.7296e-01,  6.1296e-01,\n",
       "          -1.8475e+00, -2.7616e-01,  2.7701e-01,  4.2347e-01, -1.1599e-01,\n",
       "           3.6243e+00,  1.2306e-01, -2.3526e-02, -2.4843e-01, -2.2376e-01,\n",
       "          -5.3941e-01, -6.2444e-01, -2.7711e-01,  4.9406e-01,  2.0234e-02,\n",
       "          -2.3460e-01,  4.4512e-01,  5.3397e-01,  6.6654e-01, -9.3662e-02,\n",
       "          -3.5203e-02, -6.4194e-02,  5.5998e-01, -6.6593e-01,  1.2177e-01],\n",
       "         [ 5.3074e-01,  4.0117e-01, -4.0785e-01,  1.5444e-01,  4.7782e-01,\n",
       "           2.0754e-01, -2.6951e-01, -3.4023e-01, -1.0879e-01,  1.0563e-01,\n",
       "          -1.0289e-01,  1.0849e-01, -4.9681e-01, -2.5128e-01,  8.4025e-01,\n",
       "           3.8949e-01,  3.2284e-01, -2.2797e-01, -4.4342e-01, -3.1649e-01,\n",
       "          -1.2406e-01, -2.8170e-01,  1.9467e-01,  5.5513e-02,  5.6705e-01,\n",
       "          -1.7419e+00, -9.1145e-01,  2.7036e-01,  4.1927e-01,  2.0279e-02,\n",
       "           4.0405e+00, -2.4943e-01, -2.0416e-01, -6.2762e-01, -5.4783e-02,\n",
       "          -2.6883e-01,  1.8444e-01,  1.8204e-01, -2.3536e-01, -1.6155e-01,\n",
       "          -2.7655e-01,  3.5506e-02, -3.8211e-01, -7.5134e-04, -2.4822e-01,\n",
       "           2.8164e-01,  1.2819e-01,  2.8762e-01,  1.4440e-01,  2.3611e-01]],\n",
       "\n",
       "        [[ 5.3074e-01,  4.0117e-01, -4.0785e-01,  1.5444e-01,  4.7782e-01,\n",
       "           2.0754e-01, -2.6951e-01, -3.4023e-01, -1.0879e-01,  1.0563e-01,\n",
       "          -1.0289e-01,  1.0849e-01, -4.9681e-01, -2.5128e-01,  8.4025e-01,\n",
       "           3.8949e-01,  3.2284e-01, -2.2797e-01, -4.4342e-01, -3.1649e-01,\n",
       "          -1.2406e-01, -2.8170e-01,  1.9467e-01,  5.5513e-02,  5.6705e-01,\n",
       "          -1.7419e+00, -9.1145e-01,  2.7036e-01,  4.1927e-01,  2.0279e-02,\n",
       "           4.0405e+00, -2.4943e-01, -2.0416e-01, -6.2762e-01, -5.4783e-02,\n",
       "          -2.6883e-01,  1.8444e-01,  1.8204e-01, -2.3536e-01, -1.6155e-01,\n",
       "          -2.7655e-01,  3.5506e-02, -3.8211e-01, -7.5134e-04, -2.4822e-01,\n",
       "           2.8164e-01,  1.2819e-01,  2.8762e-01,  1.4440e-01,  2.3611e-01],\n",
       "         [ 5.4822e-01,  3.8847e-02,  1.0127e-01,  3.1319e-01,  9.5487e-02,\n",
       "           4.1814e-01, -7.9493e-01, -5.8296e-01,  2.6643e-02,  1.2392e-01,\n",
       "           3.5194e-01, -2.1630e-02, -8.7018e-01, -2.7178e-01,  6.5449e-01,\n",
       "           4.2934e-01,  9.7544e-02,  3.1779e-01, -1.1921e-01, -9.7106e-02,\n",
       "          -4.7585e-01,  2.4907e-01,  1.2230e-01, -2.9079e-01, -1.6866e-01,\n",
       "          -2.1072e+00,  2.2174e-02,  4.5277e-01, -6.4485e-01,  1.3181e-01,\n",
       "           3.6594e+00, -1.7140e-01,  2.3919e-01, -4.2249e-01, -8.8331e-02,\n",
       "          -3.2925e-01, -1.2847e-01,  4.7055e-01, -7.5953e-02, -2.7747e-01,\n",
       "          -4.1905e-01,  6.0803e-01, -2.4261e-01,  1.4885e-02, -2.3204e-01,\n",
       "           2.0879e-02, -8.2175e-01,  2.6588e-01, -4.0267e-01, -1.7111e-01],\n",
       "         [ 7.4296e-01, -2.3418e-01, -3.0478e-02,  8.1639e-02,  5.3106e-01,\n",
       "          -3.2628e-01, -3.6518e-01,  9.5918e-01, -6.1010e-01, -3.6267e-01,\n",
       "          -4.2784e-01, -2.3545e-01, -1.0559e+00, -4.2271e-01,  1.7476e+00,\n",
       "           3.8830e-01, -4.3234e-02,  8.9555e-02, -8.8168e-01,  1.6945e-02,\n",
       "           1.8666e-02,  7.5625e-01,  3.5678e-01,  3.0278e-01,  1.3405e+00,\n",
       "          -1.7071e+00, -1.1773e+00,  4.8061e-01,  1.2130e+00, -3.7524e-01,\n",
       "           2.1490e+00, -3.5389e-02, -2.6194e-01, -8.4915e-01, -1.0237e-01,\n",
       "          -3.2459e-01,  3.4824e-01, -3.3020e-01,  4.4822e-01,  4.0217e-01,\n",
       "          -1.0529e+00, -1.3292e-01,  1.5945e-02,  1.8610e-02, -4.5737e-02,\n",
       "           2.3829e-01,  8.3602e-02,  1.4976e-01,  3.3331e-01,  3.6752e-01],\n",
       "         [-1.1008e-01, -4.2842e-01,  2.3023e-02, -6.6241e-01, -1.1844e-02,\n",
       "          -2.1112e-01, -7.7791e-01,  8.2479e-01, -7.0825e-01, -2.2962e-01,\n",
       "           1.4064e-01,  3.0398e-01, -1.1173e+00, -1.8641e-01,  8.2667e-01,\n",
       "           4.4993e-01, -1.1678e-01, -4.7856e-01, -9.4898e-01, -1.4977e-01,\n",
       "           1.9157e-01,  1.0528e-01,  6.6893e-01,  2.2070e-02,  5.2699e-01,\n",
       "          -1.9035e+00,  1.8775e-01,  2.6756e-01,  4.8347e-01, -2.8322e-01,\n",
       "           3.1586e+00,  4.4772e-01, -3.4549e-01, -3.1892e-01, -1.6631e-01,\n",
       "          -2.2289e-01,  2.6832e-01,  4.3670e-01, -2.0443e-01, -6.1761e-01,\n",
       "          -4.1918e-01, -2.2599e-01, -4.7885e-01, -4.0102e-01, -3.7106e-01,\n",
       "           2.4178e-01,  1.3510e-01, -8.1978e-01, -4.1543e-02, -3.0139e-01]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MyEmbeddingLayer(50, emb_dic)\n",
    "input = np.array([test1,test2])\n",
    "print(input.shape)\n",
    "#input = torch.from_numpy(input)\n",
    "test.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somtehing from week 4\n",
    "dim_emb = 50\n",
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, out_dimension, head_count=1):\n",
    "        super().__init__()\n",
    "        self.out_dimension = out_dimension\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb,emb_dic)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.v_net = nn.Linear(dim_emb, dim_emb)\n",
    "        #--------\n",
    "        self.mul_head_att = nn.MultiheadAttention(dim_emb,1)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_k = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_v = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        \n",
    "        self.ff1 = nn.Linear(in_features=dim_emb, out_features=128)\n",
    "        self.ff2 = nn.Linear(in_features=128, out_features=out_dimension)\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        #words = x.split(\" \")\n",
    "        #vectors = []\n",
    "        #for word in words:\n",
    "        #    print(word)\n",
    "        #    vectors.append(torch.Tensor(emb_dic[word.lower()]))\n",
    "        #features_final = self.l_1(x)\n",
    "        e_s = self.embedding(x)\n",
    "        q = self.lin_q(e_s)\n",
    "        k = self.lin_k(e_s)\n",
    "        v = self.lin_v(e_s)\n",
    "        a_k = self.get_A_Matrix(e_s, q)\n",
    "        h_ks = self.get_h_head_vectors(v,a_k,e_s)\n",
    "        h = self.combine_heads(h_ks)\n",
    "        \n",
    "        a_w = q.T @ self.activation_fn(self.v_net(h))\n",
    "        a_w = self.softmax(a_w)\n",
    "        \n",
    "        \n",
    "        attn_out, attn_weight = self.mul_head_att(q,k,v)\n",
    "        #print(attn_out)\n",
    "        cls_tok_emb = attn_out[0,:]\n",
    "        print(cls_tok_emb)\n",
    "        \n",
    "        output = self.ff1(cls_tok_emb)\n",
    "        output = self.activation_fn(output)\n",
    "        output = self.ff2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_A_Matrix(e,Q):\n",
    "        matr_mult = e.T @ Q.T @ e\n",
    "        a = softmax_dim1(matr_mult)\n",
    "        return a\n",
    "    \n",
    "    def get_h_head_vectors(V, a, e):\n",
    "        inner_dot = a @ e # TODO check if the dimensions really make dot product\n",
    "        h = V @ inner\n",
    "        return h\n",
    "    \n",
    "    def combine_heads(h_ks):\n",
    "        return h_ks # TODO change this when I will have multi head\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somtehing from week 4\n",
    "dim_emb = 50\n",
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, out_dimension, head_count=1):\n",
    "        super().__init__()\n",
    "        self.out_dimension = out_dimension\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb,emb_dic)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1) # TODO do I go for the correct dimension?\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #--------\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=1)\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=dim_emb, bias=False)\n",
    "        self.lin_vw = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        e_s = self.embedding(x)\n",
    "        \n",
    "        # starting single head\n",
    "        qe = self.lin_qk(e_s)\n",
    "        et_qt = qe @ e_s.transpose(-2,-1)\n",
    "        ak = self.softmax_dim1(et_qt)\n",
    "        print((ak @ e_s).shape)\n",
    "        hk = self.lin_vk(ak @ e_s)\n",
    "        h = hk # TODO make it multi head\n",
    "        print('h shape', h.shape)\n",
    "        # end of single head\n",
    "        \n",
    "        aw = self.lin_q(self.activation_fn(self.lin_vw(h)))\n",
    "        print('AW',aw.shape)\n",
    "        aw = self.softmax(aw)\n",
    "        print(aw.shape, h.shape)\n",
    "        r = aw.transpose(-2,-1) @ h # just confirmed by calculation that this seems correct\n",
    "        return r.reshape(-1,50)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 50\n",
    "\n",
    "class SelfAttHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_out):\n",
    "        super().__init__()\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1) # TODO do I go for the correct dimension?\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=head_out, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        qe = self.lin_qk(x)\n",
    "        et_qt = qe @ x.transpose(-2,-1)\n",
    "        ak = self.softmax_dim1(et_qt)\n",
    "        hk = self.lin_vk(ak @ x)\n",
    "        return hk\n",
    "    \n",
    "    \n",
    "class MyNewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_count=1):\n",
    "        super().__init__()\n",
    "        assert embedding_dimension % head_count == 0, \"embeding must be divisible by heads\"\n",
    "        self.head_out = embedding_dimension // head_count\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.embedding_layer = nn.Embeding(200,dim_emb)\n",
    "        self.embedding = MyEmbeddingLayer(dim_emb,emb_dic)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #--------\n",
    "        self.lin_vw = nn.Linear(in_features=dim_emb,out_features=dim_emb)\n",
    "        self.lin_q = nn.Linear(in_features=dim_emb,out_features=1, bias=False)\n",
    "        \n",
    "        self.selfAtt = nn.ModuleList([SelfAttHead(dim_emb, self.head_out) for i in range(head_count)])\n",
    "\n",
    "    def forward(self, x): # x is a string of words - title\n",
    "        e_s = self.embedding(x)\n",
    "        \n",
    "        hk = []\n",
    "        for head in self.selfAtt:\n",
    "            att = head(e_s)\n",
    "            hk.append(att)\n",
    "        h = torch.cat(hk, 2)\n",
    "        print(h.shape)\n",
    "        \n",
    "        aw = self.lin_q(self.activation_fn(self.lin_vw(h)))\n",
    "        aw = self.softmax(aw)\n",
    "        r = aw.transpose(-2,-1) @ h # just confirmed by calculation that this seems correct\n",
    "        return r.reshape(-1,50)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1049, -0.4537, -0.1356, -0.1396,  0.0383, -0.3921,  0.3203, -0.4113,\n",
       "          0.2908, -0.4372,  0.3781,  0.6186,  0.2963,  0.4645,  0.1341,  0.1073,\n",
       "          0.1490,  0.2557,  0.0189,  0.0372,  0.0575,  0.0744,  0.1842,  0.4117,\n",
       "          0.3078,  0.1159,  0.3378,  0.2334,  0.1820, -0.0393, -0.3553, -0.0286,\n",
       "          0.1830, -0.0683,  0.7422, -0.4820, -0.2098,  0.1134,  0.2645,  0.6888,\n",
       "          0.3641,  0.0550, -0.0983,  0.2359, -0.3176,  0.3771,  0.4073,  0.1636,\n",
       "          0.0647,  0.2981],\n",
       "        [-0.0721, -0.3664, -0.2688, -0.1042, -0.0194, -0.2130,  0.2629, -0.1272,\n",
       "          0.0166, -0.1312,  0.3772,  0.5362,  0.0383,  0.3289,  0.1689,  0.0200,\n",
       "          0.1522,  0.2349, -0.0689, -0.2110,  0.2611, -0.1230,  0.0137,  0.5855,\n",
       "          0.3744,  0.2682,  0.3069,  0.2010,  0.0873, -0.1775, -0.3700, -0.0265,\n",
       "          0.0632,  0.1766,  0.7144, -0.3285, -0.1967,  0.0916,  0.2665,  0.6822,\n",
       "          0.6188,  0.0891,  0.1862,  0.1448, -0.3254,  0.3256,  0.1844,  0.2128,\n",
       "         -0.2101,  0.2799]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNewsEncoder(dim_emb, head_count=10)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0840, 0.0074, 0.7121],\n",
       "         [0.4423, 0.5169, 0.7201]]),\n",
       " tensor([0.8036, 1.6793]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplemat = torch.rand(2,3)\n",
    "simplemat, simplemat.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import transformers\n",
    "import tokenizers\n",
    "import datasets\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/lukisko/Windows-SSD/DTU/SEMESTER 2/deep-learning/project/env/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "task_name = f\"translation_da_to_en\"\n",
    "model_name = f\"Helsinki-NLP/opus-mt-da-en\"\n",
    "translator  = transformers.pipeline(task_name, model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Natascha wasn't the first.\"},\n",
       " {'translation_text': 'Only Star Wars made more'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(['Natascha var ikke den første','Kun Star Wars tjente mere'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_articles = pq.read_table('ebnerd_small/articles.parquet')\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3792,  0.6185,  0.9593,  0.9040,  0.3681,  0.0230,  0.1680, -1.5309,\n",
       "        -0.0605, -0.2500,  0.1503,  0.3197, -0.6891, -0.7863, -0.0158,  0.5053,\n",
       "        -0.8473, -0.1235,  0.0785, -0.9602, -0.5431, -0.3352,  0.3893,  0.1946,\n",
       "        -1.1688, -0.8661, -0.3918,  0.2418,  0.3286, -0.7875,  2.4884,  0.7102,\n",
       "        -0.5311,  0.8959, -0.2305, -0.8202,  0.3442, -0.9687, -0.1514, -0.4491,\n",
       "         0.8951, -0.0166, -0.2749,  0.2795,  0.7794, -0.3194,  0.1676, -0.6252,\n",
       "         0.0533,  0.6202])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dic['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20738, 21)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# links\n",
    "https://sanjayasubedi.com.np/deeplearning/multihead-attention-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\u001b[38;5;66;03m#, weight_decay=0.3)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)#, weight_decay=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data\n",
    "out = model([])\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")\n",
    "print(f\"Output probabilities:\\n{out.softmax(1).detach().cpu().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
