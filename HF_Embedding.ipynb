{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ba3e66-b87a-4d90-9fe5-4ff597862d26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq\n",
      "  Using cached fairseq-0.12.2.tar.gz (9.6 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting cffi (from fairseq)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting cython (from fairseq)\n",
      "  Using cached Cython-3.0.11-cp310-cp310-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
      "  Using cached hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting omegaconf<2.1 (from fairseq)\n",
      "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached omegaconf-2.0.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached omegaconf-2.0.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached omegaconf-2.0.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached omegaconf-2.0.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached omegaconf-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jhegu\\anaconda3\\envs\\myenv2\\lib\\site-packages (from fairseq) (2.1.3)\n",
      "Requirement already satisfied: regex in c:\\users\\jhegu\\anaconda3\\envs\\myenv2\\lib\\site-packages (from fairseq) (2024.11.6)\n",
      "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
      "  Using cached sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\jhegu\\anaconda3\\envs\\myenv2\\lib\\site-packages (from fairseq) (2.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jhegu\\anaconda3\\envs\\myenv2\\lib\\site-packages (from fairseq) (4.67.0)\n",
      "Collecting bitarray (from fairseq)\n",
      "  Using cached bitarray-3.0.0-cp310-cp310-win_amd64.whl.metadata (33 kB)\n",
      "Collecting torchaudio>=0.8.0 (from fairseq)\n",
      "  Using cached torchaudio-2.5.1-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting omegaconf<2.1 (from fairseq)\n",
      "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of hydra-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fairseq\n",
      "  Using cached fairseq-0.12.1.tar.gz (9.6 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "WARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "WARNING: Ignoring version 2.0.4 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/92/b1/4f3023143436f12c98bab53f0b3db617bd18a7d223627d5030e13a7b4fc2/omegaconf-2.0.4-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "WARNING: Ignoring version 2.0.3 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/29/08/a88210c2c1aa0a3f65f05d8a6c98939ccb84b6fb982aa6567dec4e6773f9/omegaconf-2.0.3-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "WARNING: Ignoring version 2.0.2 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/72/fe/f8d162aa059fb4f327fd75144dd69aa7e8acbb6d8d37013e4638c8490e0b/omegaconf-2.0.2-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "WARNING: Ignoring version 2.0.1 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/86/ec/605805e60abdb025b06664d107335031bb8ebdc52e0a90bdbad6a7130279/omegaconf-2.0.1-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "WARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "WARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [16 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\jhegu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\jhegu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "    File \"C:\\Users\\jhegu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "      return hook(config_settings)\n",
      "    File \"C:\\Users\\jhegu\\AppData\\Local\\Temp\\pip-build-env-zu79ran6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "      return self._get_build_requires(config_settings, requirements=[])\n",
      "    File \"C:\\Users\\jhegu\\AppData\\Local\\Temp\\pip-build-env-zu79ran6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\jhegu\\AppData\\Local\\Temp\\pip-build-env-zu79ran6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "      exec(code, locals())\n",
      "    File \"<string>\", line 27, in <module>\n",
      "    File \"<string>\", line 18, in write_version_py\n",
      "  FileNotFoundError: [Errno 2] No such file or directory: 'fairseq\\\\version.txt'\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ccfd22-7c44-4619-9aca-dd8030964529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig, XLMRobertaModel, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339423f-95f1-4947-b862-6e57f56ed4ea",
   "metadata": {},
   "source": [
    "## Test without module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb68c58-8401-4f41-a55f-d3e3c50bac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhegu\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model(**tokens)\n",
    "\n",
    "# Extract features from the last hidden layer\n",
    "last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758d1b8a-da21-4a7a-836f-c0a23f8cf8d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens\n",
    "outputs\n",
    "last_hidden_state\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2f831-f8df-429d-b3e7-ba1fae5f1802",
   "metadata": {},
   "source": [
    "## Class for XLM roberta Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d1bff6-41eb-4dfb-9caa-aec771175429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0887,  0.0626,  0.0622,  ..., -0.0749,  0.0795, -0.0274],\n",
      "         [-0.0762, -0.0833,  0.0425,  ..., -0.0128, -0.0034,  0.1306],\n",
      "         [-0.0448, -0.0070,  0.0241,  ...,  0.2206,  0.0091,  0.0963],\n",
      "         ...,\n",
      "         [-0.0385, -0.0817,  0.0115,  ..., -0.0211,  0.0403,  0.2267],\n",
      "         [-0.0579,  0.0076,  0.0340,  ..., -0.0997,  0.0186, -0.1023],\n",
      "         [ 0.0810,  0.0547,  0.0109,  ..., -0.1423,  0.0191,  0.0099]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "class XLM_Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: embedding dimension for the words, string to embed, max length of the titles\n",
    "\n",
    "    Output: Embedding of the string word for word with the attention mask if needed\"\"\"\n",
    "    def __init__(self, model = \"xlm-alberta-base\"):\n",
    "        super(XLM_Embedding, self).__init__()\n",
    "        \n",
    "        self.tokenizer  = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = AutoModel.from_pretrained(model)\n",
    "\n",
    "    def forward(self, text, max_len):\n",
    "        assert max(len(sentence) for sentence in text)<=max_len\n",
    "        tokens = self.tokenizer(text, return_tensors = \"pt\", padding= True, truncation = True)\n",
    "        #apply the model\n",
    "        res = self.model(**tokens)\n",
    "        last_hidden_state = res.last_hidden_state\n",
    "        #if you want the attention mask as output, you can uncomment the next line and add the attention mask in the return\n",
    "        # attention_mask = tokens[\"attention_mask\"]\n",
    "        #return only the last state which contains the dense representations of each token\n",
    "        return last_hidden_state\n",
    "        \n",
    "embedder = XLM_Embedding(\"xlm-roberta-base\")\n",
    "sentence_embedding = embedder(\"This is a sentence to try the model\",30)\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.shape)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
