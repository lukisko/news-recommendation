{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import fasttext\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_torch_device():\n",
    "    \"\"\"\n",
    "    Returns the optimal device available to use for pytorch models: CUDA, MPS (Apple Silicon), or CPU.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA device:\", torch.cuda.get_device_name(device))\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device for Apple Silicon\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for Apple Silicon\n"
     ]
    }
   ],
   "source": [
    "device = get_optimal_torch_device()\n",
    "\n",
    "# DO NOT LOAD THE MODEL MORE THEN ONCE, IT TAKES A LOT OF RAM\n",
    "fasttext_model = fasttext.load_model('cc.da.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_behavior = pq.read_table('ebnerd_small/train/behaviors.parquet')\n",
    "table_history = pq.read_table('ebnerd_small/train/history.parquet')\n",
    "table_articles = pq.read_table('ebnerd_small/articles.parquet')\n",
    "df_behavior = table_behavior.to_pandas()\n",
    "df_history = table_history.to_pandas()\n",
    "df_articles = table_articles.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = df_behavior[['article_ids_inview','article_ids_clicked','user_id']]\n",
    "joined_table = main_table.join(df_history[['user_id', 'article_id_fixed']].set_index('user_id'), on='user_id', validate='many_to_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify the data so that we get the format we need for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "def remove_clicked(row):\n",
    "    index_of_clicked_one = np.where(row['article_ids_inview'] == row['article_ids_clicked'][0])\n",
    "    indexes_of_not_clicked = np.delete(row['article_ids_inview'], index_of_clicked_one)\n",
    "    indexes_of_not_clicked_suffled = np.random.choice(indexes_of_not_clicked, size=(K), replace=False) # now we have list of K = 4 things\n",
    "    indexes_of_all = np.concatenate((indexes_of_not_clicked_suffled, [row['article_ids_clicked'][0]]), axis=0) # merge random no selected ones and the selected one\n",
    "    np.random.shuffle(indexes_of_all) # suffle them\n",
    "    correct_index = np.where(indexes_of_all == row['article_ids_clicked'][0]) # get the index - used as label\n",
    "    return [indexes_of_all, correct_index[0]]\n",
    "\n",
    "joined_table[['articles_input_ids', 'articles_correct_idx']] = joined_table.apply(remove_clicked, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ids for title names\n",
    "article_map = df_articles.set_index('article_id') # this make a significant speedup in the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_ids_arr_to_article_title_arr(ids_arr):\n",
    "    return article_map.loc[ids_arr]['title'].values\n",
    "\n",
    "articles_shown = joined_table[['articles_input_ids']][:].map(from_ids_arr_to_article_title_arr) # shown articles\n",
    "articles_clicked = joined_table['articles_correct_idx'] # index of selected article\n",
    "article_history = joined_table[['article_id_fixed']][:].map(from_ids_arr_to_article_title_arr) # history of articles shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = article_history['article_id_fixed'].apply(len).max()\n",
    "def pad_list(row):\n",
    "    padded_row = np.append(row, [''] * (max_len - len(row)))\n",
    "    return np.array(padded_row)\n",
    "\n",
    "article_history['article_id_fixed_padded'] = article_history['article_id_fixed'].apply(pad_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_history.npy', 'wb') as f:\n",
    "    np.save(f,article_history['article_id_fixed_padded'].values)\n",
    "\n",
    "with open('articles_shown.npy', 'wb') as f:\n",
    "    np.save(f,articles_shown['articles_input_ids'].values)\n",
    "\n",
    "with open('articles_clicked.npy', 'wb') as f:\n",
    "    np.save(f, articles_clicked.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just if you have npy already, if not, you need to run all code up to this point\n",
    "user_history_npy = np.load('user_history.npy', allow_pickle=True)\n",
    "articles_shown_npy = np.load('articles_shown.npy', allow_pickle=True)\n",
    "articles_clicked_npy = np.load('articles_clicked.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrowsedCandidateClickedDataset(Dataset):\n",
    "    def __init__(self, browsed, candidate, clicked):\n",
    "        self.browsed = browsed\n",
    "        self.candidate = candidate\n",
    "        self.clicked = clicked\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.browsed)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.browsed[index], self.candidate[index], self.clicked[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = BrowsedCandidateClickedDataset(user_history_npy, articles_shown_npy, articles_clicked_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    browsed, candidate, clicked = zip(*batch)\n",
    "    return list(browsed), list(candidate), list(clicked)\n",
    "\n",
    "train_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News Encoder - 1st Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 30  # Maximum number of tokens (words) per title\n",
    "\n",
    "class FastTextEmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch layer that converts a batch of text strings into fastText embeddings and adds positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedding_fasttext = fasttext_model  # Reference to a pre-loaded fastText model\n",
    "\n",
    "        # Create and store positional encoding (Augmentation #1)\n",
    "        self.register_buffer('positional_encoding', self._create_positional_encoding(MAX_WORDS, emb_dim))\n",
    "\n",
    "    def _create_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"\n",
    "        Creates a sinusoidal positional encoding matrix of shape (max_len, d_model).\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(-math.log(10000.0) * torch.arange(0, d_model, 2).float() / d_model)\n",
    "\n",
    "        # Apply sine to even indices and cosine to odd indices (Standard positional encoding)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Convert text to embeddings and add positional encoding.\n",
    "        \"\"\"\n",
    "        input_shape = text.shape\n",
    "        titles = text.flatten()\n",
    "\n",
    "        output = []\n",
    "        for title in titles:\n",
    "            words = title.split()\n",
    "            # Extract embeddings for each word\n",
    "            for word in words[:MAX_WORDS]:\n",
    "                word_vec = self.embedding_fasttext.get_word_vector(word)\n",
    "                output.append(torch.from_numpy(word_vec))\n",
    "\n",
    "            # Pad if fewer than MAX_WORDS words\n",
    "            words_count = len(words)\n",
    "            if words_count < MAX_WORDS:\n",
    "                num_padding = MAX_WORDS - words_count\n",
    "                for _ in range(num_padding):\n",
    "                    output.append(torch.zeros(self.emb_dim))\n",
    "\n",
    "        # Convert list to tensor and move to device\n",
    "        output = torch.stack(output).to(device)\n",
    "        # Reshape to [input_shape..., MAX_WORDS, emb_dim]\n",
    "        output = output.reshape(input_shape + (MAX_WORDS, self.emb_dim))\n",
    "\n",
    "        # Add positional encoding (Augmentation #1)\n",
    "        output = output + self.positional_encoding\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News Encoder - 2nd Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_out):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_qk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1) # TODO do I go for the correct dimension?\n",
    "        self.lin_vk = nn.Linear(in_features=dim_emb,out_features=head_out, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        qe = self.lin_qk(x) # = Q_k^w e_j\n",
    "        et_qt = x @ qe.transpose(-2,-1) # = e_i^T Q_k^w e_j\n",
    "        ak = self.softmax_dim1(et_qt) # = exp(...)/ SUM exp(...)\n",
    "        # ak @ x = SUM a_i,j^k e_j\n",
    "        hk = self.lin_vk(ak @ x) # =  V_k^w (...)\n",
    "        return hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self,embedding_dimension, head_count=16, head_vector_size=16):\n",
    "        super().__init__()\n",
    "        self.head_out = head_vector_size #embedding_dimension // head_count # TODO this will be later more specific\n",
    "        self.selfAtt = nn.ModuleList([SelfAttHead(embedding_dimension, self.head_out) for _ in range(head_count)])\n",
    "\n",
    "    def forward(self, e_s):\n",
    "        hk = []\n",
    "        for head in self.selfAtt:\n",
    "            att = head(e_s)\n",
    "            hk.append(att)\n",
    "        h = torch.cat(hk, -1) # simply concatinaiton as mentioned in paper\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchMultiHeadSelfAttHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the 2nd layer with Word-Level Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden embeddings.\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dropout (float): Dropout probability for attention weights.\n",
    "        \"\"\"\n",
    "        super(PytorchMultiHeadSelfAttHead, self).__init__()\n",
    "\n",
    "        # Multi-head attention module (original)\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # LayerNorm after attention + residual (Augmentation #1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Dropout on the attention output (Augmentation #2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Learnable scaling parameter to better control the residual magnitude (Augmentation #3)\n",
    "        self.output_scale = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        # (Optional) Initialize layer norms and scaling parameters for better convergence (Augmentation #4)\n",
    "        nn.init.ones_(self.layer_norm.weight)  # Keeps normalization initially neutral\n",
    "        nn.init.zeros_(self.layer_norm.bias)\n",
    "        # output_scale is already initialized to 1.0 above.\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head self-attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, hidden_size).\n",
    "            attention_mask (torch.Tensor, optional): Attention mask of shape (batch_size, seq_length),\n",
    "                                                     where elements with value `True` are masked.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after self-attention and residual connection,\n",
    "                          shape (batch_size, seq_length, hidden_size).\n",
    "            torch.Tensor: Attention weights of shape (batch_size, num_heads, seq_length, seq_length).\n",
    "        \"\"\"\n",
    "        # Save the original input for the residual connection\n",
    "        residual = x\n",
    "        input_shape = x.shape\n",
    "\n",
    "        # Reshape for multihead_attn (original)\n",
    "        merged_batch_and_titles = x.reshape((-1,) + (input_shape[-2], input_shape[-1]))\n",
    "\n",
    "        attn_output, attn_weights = self.multihead_attn(\n",
    "            query=merged_batch_and_titles,\n",
    "            key=merged_batch_and_titles,\n",
    "            value=merged_batch_and_titles,\n",
    "            key_padding_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # Reshape back to the original shape (original)\n",
    "        attn_output = attn_output.reshape(input_shape)\n",
    "\n",
    "        # Scale the attention output using the learnable parameter (Augmentation #3)\n",
    "        attn_output = self.output_scale * attn_output\n",
    "\n",
    "        # Add residual connection and dropout (original + Augmentation #2)\n",
    "        x = residual + self.dropout(attn_output)\n",
    "\n",
    "        # Apply layer normalization for stability (Augmentation #1)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News Encoder - 3rd Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveWordAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, additive_vector_dim=200):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.Tanh()\n",
    "        self.lin_vw = nn.Linear(in_features=embedding_dimension, out_features=additive_vector_dim)\n",
    "        self.lin_q = nn.Linear(in_features=additive_vector_dim, out_features=1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # lin_vw(h) = V_w × h_i^w + v_w\n",
    "        # lin_q(act_fn(...)) = q_w^T tanh(...)\n",
    "        tmp = self.activation_fn(self.lin_vw(h))\n",
    "        aw = self.lin_q(tmp)\n",
    "        aw = self.softmax(aw) # exp(...) / SUM exp(...)\n",
    "        r = aw.transpose(-2,-1) @ h # SUM a_i^w h_i^w\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News Encoder - Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_count=10, head_vector_size=30, embedding_dropout=0.0):\n",
    "        super().__init__()\n",
    "        #assert embedding_dimension % head_count == 0, \"embeding must be divisible by heads\"\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.embedding = FastTextEmbeddingLayer(embedding_dimension)\n",
    "        self.embedding_drop = nn.Dropout(embedding_dropout)\n",
    "        self.mult_head_att = PytorchMultiHeadSelfAttHead(embedding_dimension, head_count)\n",
    "        self.add_word_att = AdditiveWordAttention(head_count * head_vector_size)# 16 heads and 16 dimensions each # TODO later change the vector dim to 200\n",
    "\n",
    "    def forward(self, x):\n",
    "        e_s = self.embedding(x)\n",
    "        e_s = self.embedding_drop(e_s)\n",
    "        h, ignore = self.mult_head_att(e_s)\n",
    "        r = self.add_word_att(h)\n",
    "\n",
    "        return r.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Encoder - Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, emb_dimension, user_head_count=10, news_head_count=10, head_vector_size=30):\n",
    "        super().__init__()\n",
    "\n",
    "        self.news_encoder = NewsEncoder(emb_dimension, news_head_count, head_vector_size)\n",
    "        #self.multi_head_att = MultiHeadSelfAttHead(news_head_count*head_vector_size, user_head_count)\n",
    "        self.multi_head_att = PytorchMultiHeadSelfAttHead(news_head_count * head_vector_size, user_head_count)\n",
    "        self.add_news_att = AdditiveWordAttention(user_head_count*head_vector_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        r = self.news_encoder(x)\n",
    "        #print('2',r.shape)\n",
    "\n",
    "        h, ignore = self.multi_head_att(r)\n",
    "        #print('2_1',h.shape)\n",
    "\n",
    "        u = self.add_news_att(h)\n",
    "        #print('2_2',u.shape)\n",
    "\n",
    "        return u.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click Predictor - Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickPredictor(nn.Module):\n",
    "    #def __init__(self, emb_dimension, user_head_count=16, news_head_count=16, head_vector_size=16):\n",
    "    def __init__(self, emb_dimension, user_head_count=10, news_head_count=10, head_vector_size=30):\n",
    "        super().__init__()\n",
    "        self.userEncoder = UserEncoder(emb_dimension, user_head_count, news_head_count, head_vector_size)\n",
    "        self.news_encoder = NewsEncoder(emb_dimension, news_head_count, head_vector_size)\n",
    "\n",
    "    def forward(self, browsed_news, candidate_news):\n",
    "\n",
    "        u = self.userEncoder(browsed_news)\n",
    "        u = u.unsqueeze(-2)\n",
    "\n",
    "        r = self.news_encoder(candidate_news)\n",
    "\n",
    "        ŷ = u @ r.transpose(-2, -1) # = u^T r^c\n",
    "        #ŷ = torch.tensor([torch.dot(u[i], r[i]) for i in range(u.shape[0])])\n",
    "\n",
    "        return ŷ.squeeze(dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClickPredictor(dim_emb)\n",
    "model.to(device)\n",
    "full_dataset\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5       training accuracy: 0.1, loss: 24.232715606689453\n",
      "Step 10      training accuracy: 0.15, loss: 25.71176528930664\n",
      "Step 15      training accuracy: 0.15, loss: 25.317096710205078\n",
      "Step 20      training accuracy: 0.3, loss: 21.46005630493164\n",
      "Step 25      training accuracy: 0.15, loss: 18.664615631103516\n",
      "Step 30      training accuracy: 0.25, loss: 15.903146743774414\n",
      "Step 35      training accuracy: 0.1, loss: 16.975936889648438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 58\u001b[0m\n\u001b[1;32m     50\u001b[0m predictions \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.max(1)[1]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#print('out:', output)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#print('predictions:', predictions)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#print('targets:', targ_ind)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#print(targ_ind.device)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#print(predictions.device)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m calculated_acc \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43mtarg_ind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), predictions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     59\u001b[0m train_accuracies_batches\u001b[38;5;241m.\u001b[39mappend(calculated_acc)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m validation_every_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Append average training accuracy to list.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "validation_every_steps = 5\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "train_loss = []\n",
    "validation_accuracies = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_accuracies_batches = []\n",
    "    train_loss_batches = []\n",
    "\n",
    "    for browsed, candidate, clicked in train_loader:#[(tmp_dk_input, target)]:#train_loader:#[(dk_input, target)]:#train_loader:\n",
    "        #print(targets)\n",
    "        # Forward pass.\n",
    "        #print('broken',inputs)\n",
    "        # print('working',target)\n",
    "        # print('in brow', browsed)\n",
    "        # print('in brow', np.array(browsed))\n",
    "        # print('in brow', np.array(browsed).shape)\n",
    "        # print('in cand', candidate)\n",
    "        # print('in cand', np.array(candidate))\n",
    "        #print('in cand', np.array(candidate).shape)\n",
    "\n",
    "        output = model(np.array(browsed), np.array(candidate))#model(np.array(tuple(dk_input)))#model(np.array(inputs))\n",
    "        #output = model(np.array(browsed))\n",
    "\n",
    "        # Compute loss.\n",
    "        #print(clicked)\n",
    "        targ_ind = torch.tensor(clicked).to(device)\n",
    "        loss = loss_fn(output, targ_ind)\n",
    "        train_loss_batches.append(loss.cpu().data.numpy())#get_numpy(loss))#.detach().numpy())\n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # Compute accuracy.\n",
    "        #print(output)\n",
    "        predictions =  torch.argmax(output, dim=-1)#.max(1)[1]\n",
    "        #print('out:', output)\n",
    "        #print('predictions:', predictions)\n",
    "        #print('targets:', targ_ind)\n",
    "        #print('targ_ind', targ_ind)\n",
    "        #print('predictions', predictions)\n",
    "        #print(targ_ind.device)\n",
    "        #print(predictions.device)\n",
    "        calculated_acc = accuracy_score(targ_ind.cpu().data.numpy(), predictions.cpu().data.numpy())\n",
    "        train_accuracies_batches.append(calculated_acc)\n",
    "\n",
    "\n",
    "        if step % validation_every_steps == 0:\n",
    "\n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            train_loss.append(np.mean(train_loss_batches))\n",
    "\n",
    "            train_accuracies_batches = []\n",
    "            train_loss_batches = []\n",
    "\n",
    "            # Compute accuracies on validation set.\n",
    "            # validation_accuracies_batches = []\n",
    "            # with torch.no_grad():\n",
    "            #     model.eval()\n",
    "            #     for inputs, targets in validation_loader:\n",
    "            #         output = model(inputs)\n",
    "            #         loss = loss_fn(output, targets.float())\n",
    "\n",
    "            #         predictions = output.max(1)[1]\n",
    "            #         targ_ind = targets.max(1)[1]\n",
    "\n",
    "            #         # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "            #         validation_accuracies_batches.append(accuracy_score(targ_ind, predictions) * len(inputs))\n",
    "\n",
    "            #     model.train()\n",
    "\n",
    "            # # Append average validation accuracy to list.\n",
    "            # validation_accuracies.append(np.sum(validation_accuracies_batches) / len(validation_dataset))\n",
    "\n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}, loss: {train_loss[-1]}\")\n",
    "            #print(f\"             validation accuracy: {validation_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
